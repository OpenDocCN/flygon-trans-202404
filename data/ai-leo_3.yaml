- en: Artificial Intelligence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能
- en: Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In computer science, an ideal "intelligent" machine is a flexible rational agent
    that perceives its environment and takes actions that maximize its chance of success
    at some goal.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，一个理想的“智能”机器是一个灵活的理性代理，它感知环境并采取行动，以最大化在某个目标上取得成功的机会。
- en: What means rational
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是合理的
- en: 'Here we will try to explain what means acting rational:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将尝试解释什么是行为合理：
- en: Maximally achieve pre-defined goals. (Expected utility)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化实现预定义目标。（预期效用）
- en: Goals expressed in terms of utility (non dimensional scalar value that represent
    "happiness")
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以效用为目标表达的目标（代表“幸福”的无量纲标量值）
- en: Be rational means maximize your expected utility
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合理意味着最大化您的预期效用
- en: 'Have a rational (for us intelligent) decision, is only related to the quality
    of the decision(utility), not the process that lead to that decision, for instance:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个合理（对我们来说是智能的）决定，只与决策的质量（效用）有关，而不是导致该决定的过程，例如：
- en: You gave the right answer because you brute-force all possible outcomes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你给出了正确答案，因为你暴力搜索了所有可能的结果
- en: You gave the right answer because you used a fancy state of the art method
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你给出了正确答案，因为你使用了一种时髦的最先进方法
- en: Basically you don't care about the method just the decision.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上你不关心方法，只关心决定。
- en: As the AI field evolve what was considered intelligent, is not considered anymore
    after someone discover how to do it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能领域的发展，曾被认为是智能的东西，在有人发现如何做到之后就不再被认为是智能了。
- en: A system is rational if he does the right thing with the information available.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个系统是合理的，如果他根据可用信息做出正确的决定。
- en: Why we only care to be rational
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么我们只关心合理行为
- en: Basically we don't understand how our brain work, even with the current advances
    on neuro-science, and the advances on computation power. We don't know how the
    brain works and take decisions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们不了解我们的大脑是如何工作的，即使在神经科学和计算能力的当前进展下，我们也不知道大脑是如何工作和做出决定的。
- en: The only cool thing that we know about the brain, is that to do good decisions,
    **we need memory and simulation**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一知道关于大脑的酷事情是，为了做出正确的决定，**我们需要记忆和模拟**。
- en: 'Central AI problems:'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中心人工智能问题：
- en: Reasoning
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理
- en: Knowledge
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 知识
- en: 'Planning: Make prediction about their actions, and choose the one that'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 规划：对他们的行动进行预测，并选择其中一个
- en: Learning
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习
- en: Perception
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 感知
- en: Natural Language Processing (NLP)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: Agents
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理
- en: '* * *'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'It''s a system (ie: software program) which observes the world through sensors
    and acts upon an environment using actuators. It directs it''s activity towards
    achieving goals. Intelligent agents may also learn or use knowledge to achieve
    their goals.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个系统（即：软件程序），通过传感器观察世界，并使用执行器对环境进行操作。它将其活动引导到实现目标。智能代理也可以学习或使用知识来实现其目标。
- en: Type of agents
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理类型
- en: There are a lot of agents types, but here we're going to separate them in 2
    classes
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多类型的代理，但在这里我们将它们分为2类。
- en: 'Reflex Agents: Don''t care about future effects of it''s actions (Just use
    a if-table)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反射代理：不关心其行动的未来影响（只使用if表）
- en: 'Planning Agents: Simulate actions consequences before committing to them, using
    a model of the world.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划代理：在承诺之前模拟行动后果，使用世界模型。
- en: Both reflex and planning agents can be rational, again we just care about the
    result action, if the action maximize it's expected utility, then it is rational.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 反射和规划代理都可以是合理的，再次强调我们只关心结果行动，如果行动最大化其预期效用，则它是合理的。
- en: we need to identify which type of agent is needed to have a rational behavior.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确定需要哪种类型的代理才能表现出合理行为。
- en: A reflex or planning agent can be sub-optimal, but normally planning is a good
    idea to follow.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 反射或规划代理可能是次优的，但通常规划是一个好主意。
- en: On this book we're going to see a lot of tools used to address planning. For
    instance searching is a kind of tool for planning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将看到许多用于解决规划问题的工具。例如搜索是一种规划工具。
- en: Search problem
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索问题
- en: '* * *'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A search problem finds a solution which is a sequence of actions (a plan) that
    transform the start state to the goal state.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索问题找到一个解决方案，这是一系列行动（一个计划），将起始状态转换为目标状态。
- en: 'Types of search:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索类型：
- en: 'Uninformed Search: Keep searching everywhere until a solution is found'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无信息搜索：在任何地方继续搜索，直到找到解决方案
- en: 'Informed Search: Has kind of "information" saying if we''re close or not to
    the solution (Heuristic)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有信息搜索：具有“信息”的搜索，指示我们是否接近解决方案（启发式）
- en: 'A search problem consist on the following things:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索问题包括以下内容：
- en: 'A State space: Has the states needed to do planning'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个状态空间：具有进行规划所需的状态
- en: 'A successor function: For any state x, return a set of states reachable from
    x with one action'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后继函数：对于任何状态x，返回从x到达的一组状态以及一个动作
- en: 'A start state and goal test: Gives initial point and how to check when planning
    is over'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 起始状态和目标测试：提供初始点以及如何在规划结束时进行检查
- en: Example our objective is to travel from Arad, to Bucharest
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的目标是从阿拉德到布加勒斯特旅行
- en: '![](RomaniaMap.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](RomaniaMap.jpg)'
- en: Above you have the world state, we don't need so many details, we only need
    the cities, how the connect and the distances.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，您有世界状态，我们不需要那么多细节，我们只需要城市，它们是如何连接的以及距离。
- en: '![](travel_romania.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](travel_romania.png)'
- en: 'On this search problem we detect the following properties:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个搜索问题上，我们发现了以下属性：
- en: 'State space: The cities (The only variable pertinent to this search problem)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态空间：城市（与此搜索问题有关的唯一变量）
- en: 'Start state: Arad'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起始状态：阿拉德
- en: 'Successor Function: Go to adjacent city with cost as distance.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后继函数：以距离为成本前往相邻城市。
- en: Consider the map above as a graph, it contains nodes, that don't repeat and
    how they connect along with the costs of it's connection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将上面的地图视为图，它包含不重复的节点以及它们如何连接以及连接的成本。
- en: '![](StateSpaceGraph_vs_Tree.PNG)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](StateSpaceGraph_vs_Tree.PNG)'
- en: 'On way to do planning is convert the state space graph to a search Tree, then
    use some algorithms that search for a goal state on the tree. Here we observe
    the following things:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 规划的一种方式是将状态空间图转换为搜索树，然后使用一些搜索树上目标状态的算法。在这里我们观察到以下事情：
- en: The start state will be the tree root node
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起始状态将是树根节点
- en: Node children represent the possible outcomes for each state.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点子代代表每个状态的可能结果。
- en: The problem is that both Search Trees, or State space graphs can be to big to
    fit inside the computer, for instance the following state space graph has a infinite
    search tree.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于搜索树或状态空间图可能太大而无法放入计算机中，例如以下状态空间图具有无限搜索树。
- en: '![](infinite_tree.PNG)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](infinite_tree.PNG)'
- en: What to do on those cases, basically you don't keep on memory all the possible
    solutions of the tree or graph, you navigate the tree for a finite amount of depth.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下要做什么，基本上您不保留树或图的所有可能解决方案在内存中，您在树上导航一定深度。
- en: '![](Expanding_Tree.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](Expanding_Tree.png)'
- en: For instance, look to the state graph of the Romania, we start on Arad (Start
    state)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看一下罗马尼亚的状态图，我们从阿拉德（起始状态）开始
- en: Arad has 3 possible child nodes, Sibiu, Timisoara and Zerind
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿拉德有3个可能的子节点，锡比乌，蒂米什瓦拉和泽林德
- en: We choose the leftmost child node Sibiu
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择最左边的子节点锡比乌
- en: Then we choose the leftmost child node Arad, which is bad, so we try Fagaras,
    then Oradea, etc...
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们选择最左边的子节点阿拉德，这是不好的，所以我们尝试法加拉斯，然后奥拉迪亚等...
- en: The problem is that if one of the tree branches is infinite, or is to big and
    has no solution we will keep looking on it's branch until the end.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，如果树的一个分支是无限的，或者太大而没有解决方案，我们将继续在其分支上查找直到结束。
- en: At the point that we choose Sibiu on the first step, we need to keep the other
    possible nodes (Timisoara, and Zerind) this is called the tree fringe.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们第一步选择锡比乌时，我们需要保留其他可能的节点（蒂米什瓦拉和泽林德），这称为树边缘。
- en: 'Important ideas to keep in mind on tree search:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在树搜索中要牢记的重要思想：
- en: First of all tree search is a mechanism used for planning
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，树搜索是用于规划的机制
- en: Planning means that you have a model of the world, if the model is bad your
    solution will also be bad
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划意味着您有一个世界模型，如果模型不好，您的解决方案也会不好
- en: 'Fringe: Or cache of other possible solutions'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘：或其他可能解决方案的缓存
- en: How to explore the current branch
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何探索当前分支
- en: Branching Factor
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分支因子
- en: '* * *'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Branching factor is the number children on each node on a tree.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分支因子是树上每个节点的子节点数。
- en: '![](branching_factor.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](branching_factor.png)'
- en: Old problems like tic-tac-toe or other simple problems can be solved with a
    search tree or some sort of optimized tree algorithm. But games like chess or
    go has a huge branching factor, so you could not process them in real-time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 旧问题，如井字游戏或其他简单问题，可以使用搜索树或某种优化的树算法解决。但是国际象棋或围棋等游戏的分支因子非常大，因此您无法实时处理它们。
- en: On the animation bellow we compare the chess and go branching factor.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的动画中，我们比较了国际象棋和围棋的分支因子。
- en: '![](BranchingFactor.gif)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](BranchingFactor.gif)'
- en: Next Chapter
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: '* * *'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On the next chapter we will explore more about trees.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更多地探讨树。
- en: OpenAI Gym
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: OpenAI Gym
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym
- en: Introduction
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Tree Search
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树搜索
- en: Tree Search
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树搜索
- en: Introduction
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On this chapter we will learn about some ways to do tree search. Just to remember
    from the introduction tree search is one of the mechanisms to do planning. Planning
    means that the agent will simulate possible actions on a model of the word, and
    choose the one that will maximize it's utility.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一些进行树搜索的方法。只是从介绍中记住，树搜索是规划的一种机制之一。规划意味着代理将在一个模型中模拟可能的动作，并选择最大化其效用的动作。
- en: '![](types_tree_search.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](types_tree_search.png)'
- en: On this chapter we will learn the following techniques to tree searching
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将学习以下关于树搜索的技术
- en: Depth first search
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度优先搜索（Depth first search）
- en: Breadth-First search
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广度优先搜索（Breadth-First search）
- en: Uniform Cost search
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一成本搜索（Uniform Cost search）
- en: Greedy search
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪搜索（Greedy search）
- en: A-star search A*
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A*搜索（A-star search）
- en: As mentioned before we cannot hold the whole tree on memory, so what we do is
    to expand the tree only when you needed it and you keep track of the other options
    that you did not explored yet.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们无法将整个树保留在内存中，所以我们所做的是仅在需要时扩展树，并跟踪其他尚未探索的选项。
- en: '![](Tree_Search_Algo.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](Tree_Search_Algo.png)'
- en: To those parts that are still on memory but not expanded yet we call fringe.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些仍然在内存中但尚未展开的部分，我们称之为边缘（fringe）。
- en: '![](TreeAndFringe.PNG)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](TreeAndFringe.PNG)'
- en: Depth first search
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度优先搜索（Depth first search）
- en: Markov Decision process
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Markov Decision process
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Introduction
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Markov Decision process(MDP) is a framework used to help to make decisions on
    a stochastic environment. Our goal is to find a policy, which is a map that gives
    us all optimal actions on each state on our environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）是一个用于帮助在随机环境中做出决策的框架。我们的目标是找到一个策略，即一个将给出我们环境中每个状态的所有最优动作的映射。
- en: MDP is somehow more powerful than simple planning, because your policy will
    allow you to do optimal actions even if something went wrong along the way. Simple
    planning just follow the plan after you find the best strategy.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: MDP比简单的规划要强大一些，因为你的策略将允许你即使在路上出了问题也能做出最优的动作。简单的规划只是在找到最佳策略后按照计划执行。
- en: '![](policy.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](policy.png)'
- en: What is a State
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是状态
- en: '* * *'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Consider state as a summary (then called state-space) of all information needed
    to determine what happens next. There are 2 types of state space:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态视为一个摘要（然后称为状态空间），其中包含确定下一步会发生什么所需的所有信息。有两种类型的状态空间：
- en: 'World-State: Normally huge, and not available to the agent.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界-状态：通常很大，代理无法获得。
- en: 'Agent-State: Smaller, have all variables needed to make a decision related
    to the agent expected utility.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理-状态：更小，具有所有与代理期望效用相关的做出决策所需的变量。
- en: Markovian Property
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 马尔可夫性质
- en: '* * *'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Basically you don't need past states to do a optimal decision, all you need
    is the current state ![](15b18eec.png). This is because you could encode on your
    current state everything you need from the past to do a good decision. Still history
    matters...
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，你不需要过去的状态来做出最优的决策，你所需要的只是当前状态 ![](15b18eec.png)。这是因为你可以在当前状态中编码出你需要的过去来做出好的决策。然而历史是重要的...
- en: Environment
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境
- en: '* * *'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'To simplify our universe imagine the grid world, here your agent objective
    is to arrive on the green block, and avoid the red block. Your available actions
    are: ![](f1150405.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的宇宙，想象一下网格世界，在这里你的代理目标是到达绿色方块，并避免红色方块。你可以执行的操作是：![](f1150405.png)
- en: '![](Enviroment_MDP.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](Enviroment_MDP.png)'
- en: 'The problem is that we don''t live on a perfect deterministic world, so our
    actions could have have different outcomes:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于我们并不生活在一个完美确定性的世界，所以我们的行动可能会产生不同的结果：
- en: '![](perfect_vs_markov.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](perfect_vs_markov.png)'
- en: For instance when we choose the up action we have 80% probability of actually
    going up, and 10% of going left or right. Also if you choose to go left or right
    you have 80% chance of going left and 10% going up or down.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们选择向上行动时，我们有80%的概率实际向上行动，以及10%的概率向左或向右行动。此外，如果你选择向左或向右行动，你有80%的机会向左行动，10%的机会向上或向下行动。
- en: 'Here are the most important parts:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最重要的部分：
- en: 'States: A set of possible states ![](3e20d922.png)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态：一组可能的状态！[](3e20d922.png)
- en: 'Model: ![](1764bdbb.png) Probability to go to state ![](503136bb.png) when
    you do the action ![](3238ebd4.png) while you were on state ![](15b18eec.png),
    is also called transition model.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型：![](1764bdbb.png) 当你在状态 ![](15b18eec.png) 上执行动作 ![](3238ebd4.png) 时，前往状态
    ![](503136bb.png) 的概率，也称为转移模型。
- en: 'Action: ![](27bfe991.png), things that you can do on a particular state ![](15b18eec.png)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作：![](27bfe991.png)，你可以在特定状态下执行的操作！[](15b18eec.png)
- en: 'Reward: ![](e4b7b91.png), scalar value that you get for been on a state.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励：![](e4b7b91.png)，处于某个状态时获得的标量值。
- en: 'Policy: ![](b324d96.png), our goal, is a map that tells the optimal action
    for every state'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略：！[](b324d96.png)，我们的目标，是一个告诉每个状态最优动作的映射
- en: 'Optimal policy: ![](8199e226.png), is a policy that maximize your expected
    reward ![](e4b7b91.png)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优策略：！[](8199e226.png)，是最大化你的期望奖励！[](e4b7b91.png)
- en: In reinforcement learning we're going to learn a optimal policy by trial and
    error.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们将通过试错法学习最优策略。
- en: Reinforcement Learning
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement Learning
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Introduction
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On this chapter we will learn the basics for Reinforcement learning (Rl). Basically
    an RL agent differently of solving a MDP where a graph is given, does not know
    anything about the environment, it learns what to do by exploring the environment.
    It uses actions, and receive states and rewards. You can only change your environment
    through actions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习强化学习（Rl）的基础知识。基本上，一个 RL 代理与解决给定图形的 MDP 不同，它对环境一无所知，它通过探索环境学习该做什么。它使用动作，并接收状态和奖励。你只能通过动作改变你的环境。
- en: One of the big difficulties of Rl is that some actions take time to create a
    reward, and learning this dynamics can be challenging. Also the reward received
    by the environment is not related to the last action, but some action on the past.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个重大困难是一些动作需要时间才能产生奖励，学习这种动态可能具有挑战性。此外，环境接收到的奖励与上一动作无关，而是与过去的某些动作相关。
- en: 'Some concepts:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一些概念：
- en: Agents take actions in an environment and receive states and rewards
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理在环境中采取行动并接收状态和奖励
- en: Goal is to find a policy ![](91190153.png) that maximize it's utility function
    ![](2165b7f1.png)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是找到一个策略！[](91190153.png)，使其效用函数最大化！[](2165b7f1.png)
- en: Inspired by research on psychology and animal learning
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受心理学和动物学习研究启发
- en: '![](RL_1.PNG)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](RL_1.PNG)'
- en: Here we don't know which actions will produce rewards, also we don't know when
    an action will produce rewards, some times you do an action that will take time
    to produce rewards.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不知道哪些动作会产生奖励，同时我们也不知道何时会产生奖励，有时你会做一个需要时间才能产生奖励的动作。
- en: Basically all is learned with interactions with the environment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上所有内容都是通过与环境的交互学习的。
- en: 'Reinforcement learning components:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习组成部分：
- en: 'Agent: Our robot'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：我们的机器人
- en: 'Environment: The game, or where the agent lives.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境：游戏，或者代理所在的地方。
- en: A set of states ![](c93c5d9f.png)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组状态！[](c93c5d9f.png)
- en: 'Policy: Map between state to actions'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略：状态到动作的映射
- en: 'Reward Function ![](bb0b4258.png): Gives immediate reward for each state'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数！[](bb0b4258.png)：为每个状态提供即时奖励
- en: 'Value Function: Gives the total amount of reward the agent can expect from
    a particular state to all possible states from that state. With the value function
    you can find a policy.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值函数：给出代理从特定状态到所有可能状态的总奖励量。通过值函数可以找到一个策略。
- en: 'Model ![](d38145c9.png) (Optional): Used to do planning, instead of simple
    trial-and-error approach common to Reinforcement learning. Here ![](503136bb.png)
    means the possible state after we do an action ![](3238ebd4.png) on the state
    ![](15b18eec.png)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型！[](d38145c9.png)（可选）：用于规划，而不是强化学习常见的简单试错方法。这里的！[](503136bb.png)表示在状态！[](15b18eec.png)上执行动作！[](3238ebd4.png)后可能的状态
- en: 'In our minds we will still think that there is a Markov decision process (MDP),
    which have:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的想法中，我们仍然认为有一个马尔可夫决策过程（MDP），它有：
- en: '![](RL_2.PNG)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](RL_2.PNG)'
- en: We're looking for a policy ![](4b9d31fb.png), which means a map that give us
    optimal actions for every state
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找一个策略！[](4b9d31fb.png)，这意味着一个能为我们提供每个状态最优动作的映射
- en: The only problem is that we don't have now explicitly ![](d38145c9.png) or ![](bb0b4258.png),
    so we don't know which states are good or what the actions do. The only way to
    learn those things is to try them out and learn from our samples.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是我们现在没有明确的！[](d38145c9.png)或！[](bb0b4258.png)，所以我们不知道哪些状态是好的或者动作的作用是什么。学习这些东西的唯一方法是尝试并从样本中学习。
- en: On Reinforcement learning we know that we can move fast or slow (Actions) and
    if we're cool, warm or overheated (states). But we don't know what our actions
    do in terms of how they change states.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们知道我们可以快速或缓慢移动（动作），以及我们是凉爽的、温暖的还是过热的（状态）。但我们不知道我们的动作在改变状态方面的作用。
- en: '![](RL_3.PNG)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](RL_3.PNG)'
- en: Offline (MDPs) vs Online (RL)
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线（MDPs）与在线（RL）
- en: '* * *'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Another difference is that while a normal MDP planning agent, find the optimal
    solution, by means of searching and simulation (Planning). A Rl agent learns from
    trial and error, so it will do something bad before knowing that it should not
    do. Also to learn that something is real bad or good, the agent will repeat that
    a lot of times.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是，尽管正常的MDP规划代理人通过搜索和模拟（规划）找到最佳解决方案。Rl代理通过反复试验学习，因此在知道不应该做某事之前会做一些坏事。此外，要学习某事是真正糟糕还是好，代理人将重复很多次。
- en: '![](OnlineVsOffline.PNG)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](OnlineVsOffline.PNG)'
- en: How it works
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: '* * *'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We're going to learn an approximation of what the actions do and what rewards
    we get by experience. For instance we could randomly do actions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过经验学习行动的近似作用以及我们获得的奖励。例如，我们可以随机执行操作
- en: Late Reward
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 晚期奖励
- en: '* * *'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We force the MDP to have good rewards as soon as possible by giving some discount
    ![](c9b9fa39.png) reward over time. Basically you modulate how in a rush your
    agent by giving more negative values to ![](e4b7b91.png) over time.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过随着时间的推移给予一些负面奖励的折扣来尽快使MDP具有良好的奖励。基本上，您通过随着时间的推移给予更多负值来调节您的代理程序有多么急迫。
- en: Also you can change the behavior of your agent by giving the amount of time
    that your agent have.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过给出代理程序的时间量来改变代理程序的行为。
- en: '![](negative_reward.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](negative_reward.png)'
- en: Exploration and Exploitation
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索与开发
- en: '* * *'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: One of the problems of Reinforcement learning is the exploration vs exploitation
    dilemma.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习面临的问题之一是探索与开发的困境。
- en: 'Exploitation: Make the best decision with the knowledge that we already know
    (ex: Choose the action with biggest Q-value)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发：根据我们已经知道的知识做出最佳决策（例如：选择具有最大Q值的动作）
- en: 'Exploration: Gather more information by doing different (stochastic) actions
    from known states.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索：通过从已知状态进行不同（随机）操作来收集更多信息。
- en: 'Example: Restaurant'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：餐厅
- en: 'Exploitation: Go to favorite restaurant, when you are hungry (gives known reward)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发：在饥饿时去最喜欢的餐厅（给出已知奖励）
- en: 'Exploration: Try new restaurant (Could give more reward)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索：尝试新餐厅（可能会带来更多奖励）
- en: One technique to keep always exploring a bit is the usage of ![](f62cc344.png)
    exploration where before we take an action we add some random factor.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 保持始终进行一些探索的一种技术是使用 ![](f62cc344.png) 探索，在采取行动之前添加一些随机因素。
- en: Q_Learning_Simple
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q_Learning_Simple
- en: Q_Learning_Simple
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q_Learning_Simple
- en: Introduction
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Q_Learning is a model free reinforcement learning technique. Here we are interested
    on finding through experiences with the environment the action-value function
    Q. When the Q function is found we can achieve optimal policy just by selecting
    the action that gives the biggest expected utility(reward).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Q_Learning是一种无模型的强化学习技术。在这里，我们有兴趣通过与环境的经验找到动作值函数Q。当找到Q函数时，我们可以通过选择给出最大期望效用（奖励）的动作来实现最佳策略。
- en: Simple Example
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例
- en: '* * *'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Consider a robot that need to learn how to leave a house with the best path
    possible, on this example we have a house with 5 rooms, and one "exit" room.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个需要学习如何以最佳路径离开房子的机器人，例如，在这个例子中，我们有一个有5个房间和一个“出口”房间的房子。
- en: '![](Q_LearningSimpleExample.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](Q_LearningSimpleExample.png)'
- en: Above we show the house, plant and also a graph representing it. On this graph
    all rooms are nodes, and the arrows the actions that can be taken on each node.
    The arrow values, are the immediate rewards that the agent receive by taking some
    action on a specific room. We choose our reinforcement learning environment to
    give 0 reward for all rooms that are not the exit room. On our target room we
    give a 100 reward.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上面我们展示了房屋，植物以及代表它的图。在这个图上，所有房间都是节点，箭头是可以在每个节点上采取的行动。箭头值是代理人在特定房间采取某些行动时立即获得的即时奖励。我们选择我们的强化学习环境为不是出口房间的所有房间提供0奖励。在我们的目标房间上，我们给出100奖励。
- en: To summarize
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下
- en: 'Actions: 0,1,2,3,4,5'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作：0,1,2,3,4,5
- en: 'States: 0,1,2,3,4,5'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态：0,1,2,3,4,5
- en: Rewards:0,100
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励：0,100
- en: 'Goal state: 5'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标状态：5
- en: We can represent all this mechanics on a reward table.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在奖励表上表示所有这些机制。
- en: '![](RewardTable.gif)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](RewardTable.gif)'
- en: On this table the rows represent the rooms, and the columns the actions. The
    values on this matrix represent the rewards, the value (-1) indicate that some
    specific action is not available.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在此表中，行代表房间，列代表动作。此矩阵中的值代表奖励，值（-1）表示某些特定动作不可用。
- en: 'For example looking the row 4 on this matrix gives the following information:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看这个矩阵的第4行会得到以下信息：
- en: 'Available actions: Go to room 0,3,5'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用动作：去0,3,5号房间
- en: 'Possible rewards from room 4: 0 (room 4 to 0),0 (room 4 to 3),100 (room 4 to
    5)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从房间4可能获得的奖励：0（房间4到0）、0（房间4到3）、100（房间4到5）
- en: The whole point of Q learning is that the matrix R is available only to the
    environment, the agent need to learn R by himself through experience.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的整个重点在于矩阵R仅对环境可用，代理需要通过经验自己学习R。
- en: What the agent will have is a Q matrix that encodes, the state,action,rewards,
    but is initialized with zero, and through experience becomes like the matrix R.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将拥有一个编码状态、动作、奖励的Q矩阵，但是初始化为零，并通过经验变得像矩阵R一样。
- en: '![](Q_matrix.gif)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](Q_matrix.gif)'
- en: As seen on previous chapters, after we have found the Q matrix, we have an optimum
    policy, and we're done. Basically we just need to use the Q table to choose the
    action that gives best expected reward. You can also imagine the Q table as the
    memory of what the agent learned so far through it's experiences.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前几章中所看到的，当我们找到Q矩阵后，我们有了一个最佳策略，我们就完成了。基本上，我们只需要使用Q表来选择给出最佳预期奖励的动作。您还可以将Q表想象为代理通过经验学到的知识的记忆。
- en: Algorithm explanation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法解释
- en: '* * *'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Just as a quick reminder let's describe the steps on the Q-Learning algorithm
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个快速提醒，让我们描述一下Q学习算法的步骤
- en: Initialize the Q matrix with zeros
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用零初始化Q矩阵
- en: Select a random initial state
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个随机的初始状态
- en: For each episode (set of actions that starts on the initial state and ends on
    the goal state)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一集（从初始状态开始并以目标状态结束的一组动作）
- en: While state is not goal state
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当状态不是目标状态时
- en: Select a random possible action for the current state
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为当前状态选择一个随机可能的动作
- en: Using this possible action consider going to this next state
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个可能的动作考虑去到这个下一个状态
- en: Get maximum Q value for this next state (All actions from this next state)
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取下一个状态的最大Q值（来自该下一个状态的所有动作）
- en: '![](1935fbad.png)'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![](1935fbad.png)'
- en: 'After we have a good Q table we just need to follow it:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个良好的Q表时，我们只需要遵循它：
- en: Set current state = initial state.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置当前状态=初始状态。
- en: From current state, find the action with the highest Q value
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从当前状态中找到具有最高Q值的动作
- en: Set current state = next state(state from action chosen on 2).
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置当前状态=下一个状态（选择的动作的状态）。
- en: Repeat Steps 2 and 3 until current state = goal state.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3直到当前状态=目标状态。
- en: Q-Learning on manual
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q学习手册
- en: '* * *'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Let's exercise what we learned so far by doing some episodes by hand. Consider
    ![](68af480d.png) and our initial node(state) to be "room 1"
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过手工进行一些剧集来练习我们到目前为止所学到的知识。考虑![](68af480d.png)和我们的初始节点（状态）为“房间1”
- en: As we don't have any prior experience we start our Q matrix with zeros
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有任何先前的经验，我们从零开始构建我们的Q矩阵
- en: '![](Q_matrix.gif)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](Q_matrix.gif)'
- en: Now take a look on our reward table (Part of the environment)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看一下我们的奖励表（环境的一部分）
- en: '![](R_possible_states_1.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](R_possible_states_1.jpg)'
- en: Episode 1
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1集
- en: As we start from state "room 1" (second row) there are only the actions 3(reward
    0) or 5( reward 100) do be done, imagine that we choose randomly the action 5.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从状态“房间1”（第二行）开始，只有动作3（奖励0）或5（奖励100）可以执行，假设我们随机选择了动作5。
- en: '![](R_possible_states_5.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](R_possible_states_5.jpg)'
- en: 'On this new (next state 5) there are 3 possible actions: 1 , 4 or 5, with their
    rewards 0,0,100\. Basically is all positive values from row "5", and we''re just
    interested on the one with biggest value. We need to select the biggest Q value
    with those possible actions by selecting Q(5,1), Q(5,4), Q(5,5), then using a
    "max" function. But remember that at this state the Q table is still filled with
    zeros.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的（下一个状态5）上有3个可能的动作：1、4或5，它们的奖励分别为0、0、100。基本上是从行“5”中得到的所有正值，我们只对具有最大值的值感兴趣。我们需要通过选择Q(5,1)、Q(5,4)、Q(5,5)来选择具有这些可能动作的最大Q值，然后使用“max”函数。但请记住，在这个状态下，Q表仍然填充有零。
- en: '![](fc0a4ccc.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](fc0a4ccc.png)'
- en: 'As the new state is 5 and this state is the goal state, we finish our episode.
    Now at the end of this episode the Q matrix will be:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新状态为5且此状态为目标状态，我们结束了本集。现在在本集结束时，Q矩阵将为：
- en: '![](Q_end_episode1.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](Q_end_episode1.jpg)'
- en: Episode 2
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2集
- en: On this new state we randomly selected the state "room 3", by looking the R
    matrix we have 3 possible actions on this state, also now by chance we chose the
    action 1
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新状态上，我们随机选择了状态“房间3”，通过查看R矩阵，我们在这个状态上有3个可能的动作，现在我们偶然选择了动作1
- en: '![](R_possible_states_3.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](R_possible_states_3.jpg)'
- en: By selecting the action "1" as our next state will have now the following possible
    actions
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择动作“1”作为我们的下一个状态，现在将有以下可能的动作
- en: '![](R_possible_states_1.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](R_possible_states_1.jpg)'
- en: 'Now let''s update our Q table:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更新我们的Q表：
- en: '![](31713139.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](31713139.png)'
- en: '![](New_Q_episode_2.gif)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](New_Q_episode_2.gif)'
- en: 'As our new current state 1 is not the goal state, we continue the process.
    Now by chance from the possible actions of state 1, we choose the action 5\. From
    the action 5 we have the possible actions: 1,4,5 [Q(5,1), Q(5,4), Q(5,5)] unfortunately
    we did not computed yet this values and our Q matrix remain unchanged.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的新当前状态1不是目标状态，我们继续这个过程。现在，从状态1的可能动作中，我们偶然选择了动作5\. 从动作5开始，我���有可能的动作：1,4,5
    [Q(5,1), Q(5,4), Q(5,5)] 不幸的是，我们尚未计算这些值，我们的Q矩阵保持不变。
- en: Episode 100000
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第100000轮次
- en: 'After a lot of episodes our Q matrix can be considered to have convergence,
    on this case Q will be like this:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 经过许多轮次后，我们的Q矩阵可以被认为已经收敛，在这种情况下，Q将会是这样的：
- en: '![](Q_conv_not_normlize.gif)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](Q_conv_not_normlize.gif)'
- en: 'if we divide all of the nonzero elements by it''s greatest value (on this case
    500) we normalize the Q table (Optional):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有非零元素除以它的最大值（在这种情况下为500），我们就可以对Q表进行归一化（可选）：
- en: '![](Converged_Q.gif)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](Converged_Q.gif)'
- en: What to do with converged Q table.
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收敛的Q表应该怎么处理。
- en: '* * *'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now with the good Q table, imagine that we start from the state "room 2". If
    we keep choosing the action that gives maximum Q value, we're going from state
    2 to 3, then from 3 to 1, then from 1 to 5, and keeps at 5\. In other words we
    choose the actions [2,3,1,5].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了良好的Q表，想象一下我们从状态“房间2”开始。如果我们继续选择给出最大Q值的动作，我们将从状态2到3，然后从3到1，然后从1到5，并保持在5\.
    换句话说，我们选择动作[2,3,1,5]。
- en: Just one point to pay attention. On state 3 we have the options to choose action
    1 or 4, because both have the same max value, we choose the action 1\. But the
    other action would also give the same cumulative reward. [0+0+100]
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 只需注意一点。在状态3时，我们有选择动作1或4的选项，因为两者具有相同的最大值，我们选择动作1\. 但另一个动作也会给出相同的累积奖励。[0+0+100]
- en: '![](Using_Q_policy.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](Using_Q_policy.jpg)'
- en: '![](BestPolicy_from_Q.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](BestPolicy_from_Q.png)'
- en: Working out this example in Matlab
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Matlab中解决这个例子
- en: '* * *'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now we're ready to mix those things on the computer, we're going to develop
    2 functions, one for representing our agent, and the other for the enviroment.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备在计算机上将这些东西混合在一起，我们将开发两个函数，一个用于代表我们的代理，另一个用于环境。
- en: '![](Diagram_Simple_RL.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](Diagram_Simple_RL.png)'
- en: Enviroment
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境
- en: '* * *'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We start by modeling the environment. Which is the part that receives an action
    from the agent and give as feedback, a immediate reward and state information.
    This state is actually the state of the environment after some action is made.
    Just to remember, if our system is markovian all information necessary for choosing
    the best future action, is encoded on this state.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对环境进行建模。这部分接收代理的动作并反馈即时奖励和状态信息。这个状态实际上是在进行某些动作后环境的状态。只需记住，如果我们的系统是马尔可夫的，选择最佳未来动作所需的所有信息都编码在这个状态中。
- en: Observe that the environment has the matrix R and all models needed to completely
    implement the universe. For example the environment could be a game, our real
    world (Grid World) etc...
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意环境具有矩阵R和完全实现宇宙所需的所有模型。例如，环境可以是一个游戏，我们的真实世界（Grid World）等等...
- en: '[PRE0]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Agent
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理
- en: '* * *'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now on the agent side, we must train to gather experience from the environment.
    After this we use our learned Q table to act optimally, which means just follow
    the Q table looking for the actions that gives maximum expected reward. As you
    may expect the agent interface with the external world through the "simple_RL_enviroment"
    function.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在代理方面，我们必须训练以从环境中获取经验。之后，我们使用我们学到的Q表来进行最优行动，这意味着只需按照Q表寻找能够获得最大预期奖励的动作。正如您所期望的那样，代理通过“simple_RL_enviroment”函数与外部世界进行交互。
- en: '[PRE1]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Deep Q Learning
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: Deep Q Learning
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: '![](DQNBreakoutBlocks.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](DQNBreakoutBlocks.png)'
- en: Introduction
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On the previous chapter we learned about the "old school" Q learning, we used
    matrices to represent our Q tables. This somehow implies that you at least know
    how many states (rows) you have on your environment, the problem is that sometimes
    this is not true.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了关于“老派”Q学习，我们使用矩阵来表示我们的Q表。这在某种程度上意味着您至少知道环境中有多少个状态（行），问题是有时这并不成立。
- en: '![](Using_Q_policy.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](Using_Q_policy.jpg)'
- en: On this Q table we can say the the expectation of wining the game by taking
    the action 1 at state 3 is 80\. Bigger Q values means that we expect to win.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Q表中，我们可以说在状态3时采取动作1的期望获胜率为80\。较大的Q值意味着我们期望获胜。
- en: Also we learned that reinforcement learning is about learning how to behave
    on some environment where our only feedback is some sparse and time delayed "labels"
    called rewards. They are time delayed because there are cases where the environment
    will only tell if your action was good or bad some time after you actually moved.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到了强化学习是关于在某个环境中学习如何行为，我们唯一的反馈是一些稀疏的、时间延迟的“标签”，称为奖励。它们是时间延迟的，因为有些情况下环境只会告诉你，你的动作是好是坏，一段时间后你才会移动。
- en: Some Formalization before continue
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续之前的一些形式化
- en: '* * *'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Episode:'
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剧集：
- en: Considering our environment "Markovian" which means that our state encode everything
    needed to take a decision. We define an episode (game) as finite set of states,
    actions and rewards.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的环境是“马尔可夫”的，这意味着我们的状态编码了需要做出决策的一切。我们将一个剧集（游戏）定义为状态、动作和奖励的有限集。
- en: '![](88061f2f.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](88061f2f.png)'
- en: For instance here "![](dd6aae3a.png)" means the reward that we take by taking
    the action ![](271a23cb.png) on state ![](b923a530.png). An episode always finish
    on an end state (Game over).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在这里“![](dd6aae3a.png)”表示我们通过在状态![](b923a530.png)上采取动作![](271a23cb.png)而获得的奖励。一个剧集总是在一个结束状态（游戏结束）结束。
- en: Policy
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略
- en: Policy is defined as a "map" from states to actions, is the reinforcement learning
    objective to find the optimal policy. An optimal policy can be derived from a
    Q function.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 策略被定义为从状态到动作的“映射”，找到最优策略是强化学习的目标。最优策略可以从Q函数导出。
- en: Return or Future Total Reward
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回报或未来总奖励
- en: We define return as the sum of all immediate rewards on an episode. Which means
    the sum of rewards until the episode finish.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回报定义为剧集中所有即时奖励的总和。这意味着直到剧集结束的所有奖励的总和。
- en: '![](62432496.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](62432496.png)'
- en: Future Discounted Reward
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来折扣奖励
- en: To give some sort of flexibility we add to our total reward a parameter called
    gamma(![](c9b9fa39.png)). If gamma=0 all future rewards will be discarded, if
    gamma=1 all future rewards will be considered.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给予某种灵活性，我们在我们的总奖励中添加了一个称为gamma(![](c9b9fa39.png))的参数。如果gamma=0，所有未来奖励都将被丢弃，如果gamma=1，所有未来奖励都将被考虑。
- en: '![](75996783.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](75996783.png)'
- en: Q function
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q函数
- en: We define the function Q(s,a) as maximum expected "Future Discounted Reward"
    that we can get if we take an action "a" on state "s", and continue optimally
    from that point until the end of the episode. When we say "continue optimally"
    means that we continue choosing actions from the policy derived from Q.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将函数Q(s,a)定义为我们可以获得的最大预期“未来折扣奖励”，如果我们在状态“s”上采取一个动作“a”，并从那一点开始继续进行最佳操作，直到剧集结束。当我们说“继续最优”时，意味着我们继续从Q导出的策略中选择动作。
- en: '![](589f6cb5.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](589f6cb5.png)'
- en: 'Bellow we show how to derive a policy from the Q function, basically we want
    the action that gives the biggest Q value on state "s":'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示如何从Q函数中导出策略，基本上我们希望在状态"s"上给出最大Q值的动作：
- en: '![](62ffd5be.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](62ffd5be.png)'
- en: The function "![](b2461618.png)" will search for the action "a" that maximizes
    Q(s,a) You can think that the Q function is "the best possible score at the end
    of the game after performing action a in state s". So if you have the Q function
    you have everything needed to win all games!.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 函数"![](b2461618.png)"将搜索最大化Q(s,a)的动作"a"。您可以认为Q函数是“在状态s执行动作a后游戏结束时的最佳得分”。所以如果你有了Q函数，你就有了赢得所有游戏所需的一切！
- en: Greedy policy
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贪心策略
- en: If we choose an action that "ALWAYS" maximize the "Discounted Future Reward",
    you are acting greedy. This means that you are not exploring and you could miss
    some better actions. This is called exploration-exploitation problem. To solve
    this we use an algorithm called ![](f62cc344.png), where a small probability ![](9fe1930b.png)
    will choose a completely random action from time to time.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择一个“始终”最大化“折扣未来奖励”的动作，你就是在贪心行为。这意味着你不在探索，可能会错过一些更好的动作。这就是所谓的探索-利用问题。为了解决这个问题，我们使用一种称为![](f62cc344.png)的算法，其中一个小概率![](9fe1930b.png)会时不时地选择一个完全随机的动作。
- en: '![](eGreedyAlgo.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](eGreedyAlgo.png)'
- en: How to get the Q function
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何获得Q函数
- en: Basically we get the Q function by experience, using an iterative process called
    bellman equation.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们通过经验来获得Q函数，使用一个称为贝尔曼方程的迭代过程。
- en: '![](3870fb60.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](3870fb60.png)'
- en: In terms of algorithm. ![](Q_Learning_Algo.png) On the beginning the estimated
    Q(s,a) will be wrong but with time, the experiences with the environment, will
    give a true "r" and this reward will slowly shape our estimation to the true Q(s,a).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 就算法而言。![](Q_Learning_Algo.png)一开始估计的Q(s,a)将是错误的，但随着时间的推移，与环境的经验将给出一个真实的“r”，这个奖励将慢慢地塑造我们对真实Q(s,a)的估计。
- en: Deep Q Network
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: '* * *'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The Deep Q learning is about using deep learning techniques to represent the
    Q table. Is also a kind of recipe to use Q learning on games.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习是关于使用深度学习技术来表示 Q 表。它也是一种在游戏中使用 Q 学习的配方。
- en: Input
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入
- en: The first thing on this recipe is to get our input, as we may imagine we take
    information directly form the screen. To add some notion of time we actually get
    4 consecutive screens.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的第一步是获取我们的输入，正如我们可能想象的那样，我们直接从屏幕上获取信息。为了增加时间的概念，我们实际上获取了 4 个连续的屏幕。
- en: Get the images
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取图像
- en: Rescale to 84x84
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整大小为 84x84
- en: Convert to 8 bit grayscale.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换为 8 位灰度。
- en: Now we have 84x84x256x4 "stuff" as our environment state, this means 7 million
    states. To find structure on this data we will need a convolution neural network!.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有 84x84x256x4 的“东西”作为我们的环境状态，这意味着 700 万个状态。为了在这些数据上找到结构，我们将需要一个卷积神经网络！。
- en: Adding a Convolution Neural Network
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加一个卷积神经网络
- en: Now we will give those 84x84x4 tensor to an CNN, this model will have one output
    for each actions which will represent a Q-value for each possible action. So for
    each forward propagation we will have all possible Q values for a particular state
    encoded on the screen. It's valid to point out that this is not a classification
    problem but a regression, our Q-value output is a scalar number not a class.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把这个 84x84x4 张量传递给一个 CNN，这个模型将为每个动作输出一个值，这个值将表示每个可能动作的 Q 值。因此，对于每次前向传播，我们将在屏幕上编码一个特定状态的所有可能
    Q 值。值得指出的是，这不是一个分类问题，而是一个回归问题，我们的 Q 值输出是一个标量而不是一个类。
- en: '![](DQN_Struct_Model.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](DQN_Struct_Model.png)'
- en: 'On the Atari paper this CNN had the following structure:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在雅达利论文中，这个 CNN 的结构如下：
- en: '![](DeepmindCNN.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](DeepmindCNN.png)'
- en: Notice that there is not pooling layer, this is done because want to retain
    the spatial information.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里没有池化层，这是因为我们想保留空间信息。
- en: Loss function
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: As out CNN is a regression model our loss function will be a "squared error
    loss"
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的 CNN 是一个回归模型，我们的损失函数将是一个“平方误差损失”
- en: '![](7edc9fb9.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](7edc9fb9.png)'
- en: How to get the Deep Q function
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何得到深度 Q 函数
- en: 'Now to iterate and find the real Q value using deep learning we must follow:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要迭代并使用深度学习找到真正的 Q 值，我们必须遵循：
- en: Do a forward pass for the current state "![](15b18eec.png)" (screens) to get
    all possible Q values
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对当前状态“![](15b18eec.png)”(屏幕)进行前向传播，以获取所有可能的 Q 值
- en: Do a forward pass on the new state ![](9e821f3.png) and find the action ![](52cda3d8.png)
    (Action with biggest Q value)
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对新状态进行前向传播 ![](9e821f3.png) 并找到动作 ![](52cda3d8.png) (具有最大 Q 值的动作)
- en: Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated
    in step 2). For all other actions, set the Q-value target to the same as originally
    returned from step 1, making the error 0 for those outputs.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置动作的 Q 值目标为 r + γmax a’ Q(s’, a’) (使用步骤 2 中计算的最大值)。对于所有其他动作，将 Q 值目标设置为与步骤 1
    中最初返回的相同，使得这些输出的错误为 0。
- en: Use back-propagation and mini-batches stochastic gradient descent to update
    the network.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播和小批量随机梯度下降更新网络。
- en: Problems with this approach
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这种方法的问题
- en: Unfortunately, we need to solve some problems with this approach. But before
    talking about the possible solutions let's check which are the problems.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们需要解决这种方法的一些问题。但在讨论可能的解决方案之前，让我们检查一下问题是什么。
- en: 'Exploration-Exploitation issue: This is easy, we just add some random action
    along the way. (just use ![](5dcbc848.png))'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索-利用问题：这很容易，我们只需在途中添加一些随机动作。(只需使用 ![](5dcbc848.png))
- en: 'Local-Minima: During the training we''re going to have a lot of screens that
    are high correlated, this may guide your network to learn just a replay of the
    episode. To solve this we need somehow to shuffle the input mini-batch with some
    other playing data. (Of course of the same game but at different time)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部最小值：在训练过程中，我们将有许多高度相关的屏幕，这可能会导致您的网络只学习情节的重放。为了解决这个问题，我们需要以某种方式将输入小批量与一些其他播放数据混洗。(当然是同一场游戏但在不同的时间)
- en: Experience Replay
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经验重播
- en: As mentioned we need to break the similarities of continuous frames during our
    update step. To do this we will store all game experiences during the episode
    inside a "replay memory" then during training we will take random mini-batches
    of this memory. Also you can add some human-experience by adding on the replay
    memory some human episodes.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们需要在更新步骤中打破连续帧的相似性。为此，我们将在整个情节期间存储所有游戏经验到一个“重放记忆”中，然后在训练期间我们将随机取一些这个记忆的小批量。你还可以通过在重放记忆中添加一些人类经验来增加一些人类经验。
- en: Complete Recipe
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整配方
- en: '![](DeepQAlgorithm.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](DeepQAlgorithm.png)'
- en: A complete tensorflow example can be found [here](https://github.com/asrivat1/DeepLearningVideoGames).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 tensorflow 示例可以在[这里](https://github.com/asrivat1/DeepLearningVideoGames)找到。
- en: Deep Reinforcement Learning
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: Deep Reinforcement Learning
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: Introduction
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](conv_agent.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](conv_agent.png)'
- en: 'On this chapter we will learn the effects of merging Deep Neural Networks with
    Reinforcement learning. If you follow AI news you may heard about some stuff that
    AI is not capable to do without any specific programming:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习将深度神经网络与强化学习相结合的影响。如果您关注 AI 新闻，您可能听说过一些 AI 没有任何特定编程就无法完成的事情：
- en: Learn how to play atari from raw image pixels
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原始图像像素学习如何玩 Atari 游戏
- en: Learn how to beat Go champions (Huge branching factor)
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会如何击败围棋冠军（巨大的分支因子）
- en: Robots learning how to walk
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人学会如何行走
- en: 'All those achievements fall on the Reinforcement Learning umbrella, more specific
    Deep Reinforcement Learning. Basically all those achievements arrived not due
    to new algorithms, but due to more Data and more powerful resources (GPUs, FPGAs,
    ASICs). Examples:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些成就都属于强化学习的范畴，更具体地说是深度强化学习。基本上，所有这些成就都不是由于新算法，而是由于更多的数据和更强大的资源（GPU、FPGA、ASIC）。例如：
- en: 'Atari: Old Q-Learning algorithm but with a CNN as function aproximator (Since
    1988 people talk about standard RL with function approximators)'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atari：旧的 Q-Learning 算法，但使用 CNN 作为函数逼近器（自 1988 年以来人们一直在讨论带有函数逼近器的标准 RL）
- en: 'AlphaGo: Policy gradients that use Monte Carlo Tree Search (MCTS), which is
    pretty standard.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo：使用蒙特卡洛树搜索（MCTS）的策略梯度，这是相当标准的。
- en: Nowadays Policy Gradients it's the favorite choice for attacking Reinforcement
    learning(RL) problems. Previously it was DQN (Deep Q learning). One advantage
    of Policy Gradients is because it can be learned end-to-end.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，策略梯度是攻击强化学习（RL）问题的首选方法。之前是 DQN（深度 Q 学习）。策略梯度的一个优点是因为它可以端到端地学习。
- en: Policy Gradient
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度
- en: '* * *'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'We will start our study with policy gradient using the pong game:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从使用乒乓球游戏的策略梯度开始我们的研究：
- en: '![](pong.gif)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](pong.gif)'
- en: Here we have to possible actions (UP/DOWN) and our objective is make the ball
    pass the opponent paddle. Our inputs are going to be a 80x80x1 image pre-processed
    from a 210x160x3 image. Our game enviroment (openAI gym) will give a reward of
    +1 if you win the opponent, -1 if you lose or 0 otherwise.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有两个可能的动作（上/下），我们的目标是让球越过对手的球拍。我们的输入将是从 210x160x3 图像预处理而来的 80x80x1 图像。我们的游戏环境（openAI
    gym）将在你赢了对手时给出一个+1的奖励，输了时给出一个-1的奖励，否则为0。
- en: '![](DeepReinforcementLearning.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](DeepReinforcementLearning.png)'
- en: Here each node is a particular game state, and each edge is a possible transition,
    also each edge can gives a reward. Our goal is to find a policy (map from states
    to actions) that will give us the best action to do on each state. The whole point
    is that we don't know this graph, otherwise the whole thing would be just a reflex
    agent, and also sometimes we cannot fit all this on memory.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这里每个节点都是一个特定的游戏状态，每条边都是一个可能的转换，同时每条边都可以给出一个奖励。我们的目标是找到一个策略（将状态映射到动作的映射），这将给我们在每个状态上做出的最佳动作。整个问题的关键是我们不知道这个图，否则整个事情就会变成一个反射性代理，有时我们无法将所有这些内容都放入内存中。
- en: Preprocessing
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: '* * *'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On this pong example we do some operations on the original 210x160x3 (RGB) image
    to a simpler grayscale 80x80\. This is done to simplify the neural network size.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个乒乓球的例子中，我们对原始的 210x160x3（RGB）图像进行一些操作，转换成一个更简单的灰度图 80x80\. 这样做是为了简化神经网络的大小。
- en: Before
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: Before
- en: '![](pong_I_before.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](pong_I_before.png)'
- en: After
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: After
- en: '![](pong_I_after.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](pong_I_after.png)'
- en: Also to give some notion of time to the network a difference between the current
    image and the previous one is calculated. (On the DQN paper it was used a convolution
    neural network with 6 image samples)
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给网络一些时间概念，计算了当前图像和上一张图像之间的差异。（在 DQN 论文中使用了一个具有 6 个图像样本的卷积神经网络）
- en: Policy Network
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略网络
- en: '* * *'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The basic idea is to use a machine learning model that will learn a good policy
    from playing the game, and receiving rewards. So adding the machine learning part.
    We''re going to define a policy network (ex: 2 layer neural network). This simple
    neural network will receive the entire image and output the probability of going
    up.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是使用一个机器学习模型从游戏中学习一个良好的策略，并获得奖励。所以添加了机器学习部分。我们将定义一个策略网络（例如：2 层神经网络）。这个简单的神经网络将接收整个图像并输出向上移动的概率。
- en: '![](policy_newtork.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](policy_newtork.png)'
- en: After the probability of going up is calculated we sample an action (UP/DOWN)
    from a uniform distribution.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完上升概率后，我们从均匀分布中采样一个动作（UP/DOWN）。
- en: '![](policy_gradient_graph.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](policy_gradient_graph.png)'
- en: Policy Gradient
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略梯度
- en: '* * *'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'One difference between supervised learning and reinforcement learning is that
    we don''t have the correct labels to calculate the error to then back-propagate.
    Here is where the policy gradient idea comes. Imagine the following example:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和强化学习之间的一个区别是，我们没有正确的标签来计算错误，然后进行反向传播。这就是策略梯度思想的地方。想象以下例子：
- en: '![](policy_gradient_backprop.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](policy_gradient_backprop.png)'
- en: 'During the training our policy network gave a "UP" probability of 30% (0.3),
    therefor our "DOWN" probability will be 100-30=70% (0.7). During our action sampling
    from this distribution the "DOWN" action was selected. Now what we do is choose
    a unit gradient "1" and wait for the possible rewards (0,+1,-1). This will modulate
    our gradient to 3 possible values:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们的策略网络给出了“UP”的概率为30%（0.3），因此我们的“DOWN”概率将为100-30=70%（0.7）。在从这个分布中采样动作时，选择了“DOWN”动作。现在我们做的是选择一个单位梯度“1”，等待可能的奖励（0，+1，-1）。这将调节我们的梯度到3个可能的值：
- en: 'Zero: Our weights will remain the same'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zero: 我们的权重将保持不变'
- en: 'Positive: Make the network more likely to repeat this action on the future'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正：使网络更有可能在未来重复此操作
- en: 'Negative: Make the network less likely to repeat this action on the future'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负：使网络在未来更不可能重复此操作
- en: So this is the magic of policy gradient, we choose a unit gradient and modulate
    with our rewards. After a lot of good/bad actions been selected and properly rewarded
    the policy network map a "rational/optimum" policy.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是策略梯度的魔力，我们选择一个单位梯度，并用我们的奖励调节。经过大量良好/糟糕的动作选择并适当奖励后，策略网络映射出一个“合理/最优”的策略。
- en: Implementation notes
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现说明
- en: '* * *'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Here we show how to initialize our policy network as a 2 layer neural network
    with the first layer having 200 neurons and the second output layer 1 neuron.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了如何将我们的策略网络初始化为一个2层神经网络，第一层有200个神经元，第二输出层有1个神经元。
- en: '![](initializa_policy_network.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](initializa_policy_network.png)'
- en: On the code above, D is our input difference image after pre-processing and
    H is the number of neurons on the hidden layer.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，D是我们经过预处理的输入差异图像，H是隐藏层中的神经元数量。
- en: Bellow we have the python code for this network forward propagation (policy_forward),
    on this neural network we didn't use bias, but you could.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们有这个网络前向传播的Python代码（policy_forward），在这个神经网络上我们没有使用偏置，但你可以使用。
- en: '![](policy_network_python.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](policy_network_python.png)'
- en: From the code above we have 2 set of weights W1,W2. Where W1 or the weights
    of the hidden layer can detect states from the images (Ball is above/bellow paddle),
    and W2 can decide what to do (action up/down) on those states. So the only problem
    now is to find W1 and W2 that lead to expert (rational) policy.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码中，我们有两组权重W1，W2。其中W1或隐藏层的权重可以检测图像中的状态（球在球拍上方/下方），而W2可以决定在这些状态下做什么（向上/向下动作）。所以现在唯一的问题是找到W1和W2，使之导致专家（合理）策略。
- en: Actually we do a forward propagation to get the score for the "UP" action given
    an input image "x". Then from this "score" we "toss a biased coin" to actually
    do our atari joystick action (UP-2, Down-5).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们进行前向传播，得到了在输入图像“x”中执行“UP”动作的得分。然后，根据这个“得分”，我们“抛一枚偏倚的硬币”，来实际执行我们的Atari摇杆动作（UP-2，Down-5）。
- en: By doing this we hope that some times we're going to do a good action and if
    we do this a lot of times, our policy network will learn.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们希望有时我们会做出一个好的动作，如果我们这样做了很多次，我们的策略网络就会学习。
- en: '![](action_probability.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](action_probability.png)'
- en: Training the policy network
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练策略网络
- en: '* * *'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: First we will initialize randomly W1,W2.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将随机初始化W1，W2。
- en: Then we will play 20 games (one eposide).
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将进行20局游戏（一轮）。
- en: Keep track of all games and their result (win/loose)
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟踪所有游戏及其结果（胜利/失败）
- en: After a configured number of episodes we update our policy network
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在配置的一定轮次后，我们更新我们的策略网络
- en: Assuming that each game has 200 frames, and each episode has 20 games, we have
    to make 4000 decisions (up/down) per episode. Suppose that we run 100 games 12
    we win (+1) and 88 we loose(-1). We need to take all the 12x200=2400 decisions
    and do a positive(+1) update. (This will encourage do do those actions in the
    future for the same detected states). Now all the 88x200=17600 decisions that
    make us loose the game we do a negative(-1) update.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每个游戏有200帧，每个剧集有20个游戏，我们每剧集需要做4000个决定（上/下）。 假设我们运行100个游戏，我们赢了12个（+1），我们输了88个（-1）。
    我们需要做所有的12x200=2400个决定，并进行正(+1)更新。（这将鼓励将来在相同的检测到的状态下采取这些动作）。 现在，所有88x200=17600个使我们输掉游戏的决定，我们进行负(-1)更新。
- en: '![](episodes.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](episodes.png)'
- en: The network will now become slightly more likely to repeat actions that worked,
    and slightly less likely to repeat actions that didn't work.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 网络现在会稍微更有可能重复有效的动作，并且稍微不太可能重复无效的动作。
- en: Some words about Policy Gradient
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于策略梯度的一些话
- en: '* * *'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The policy gradient, are the current (2016) state of the art, but is still very
    far from human reasoning. For example, you don't need to crash your car hundred
    of times to start avoiding it. Policy gradient need a lot of samples to start
    to internalize correct actions, and it must have constant rewards of it. You can
    imagine that you are doing a kind of "brute-force" search where on the beginning
    we jitter some actions, and accidentally we do a good one. Also this good actions
    should be repeated hundred of times before the policy network start to repeat
    good actions. That's why the agent had to train for 3 days before start to play
    really good.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度，是当前（2016年）的前沿技术，但仍然与人类推理相去甚远。 例如，你不需要撞车数百次才开始避免它。 策略梯度需要大量样本才能开始内化正确的动作，并且它必须不断获得相应的奖励。
    你可以想象，你正在进行一种“蛮力”搜索，在开始时我们会随机尝试一些动作，偶然地我们做了一个好动作。 而且这些好的动作在策略网络开始重复好动作之前应该被重复数百次。
    这就是为什么代理需要在开始真正表现良好之前进行3天的训练。
- en: 'When policy gradient will shine:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 当策略梯度将发挥作用时：
- en: When the problem does not rely on very long-term planning
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当问题不依赖于非常长期的规划时
- en: When it receive frequent reward signals for good/bad actions.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当它接收到针对好/坏动作的频繁奖励信号时。
- en: Also if the action space start to be to complex (a lot of actions commands)
    there are other variants, like "deterministic policy gradients". This variant
    uses another network called "critic" that learns the score function.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作空间开始变得太复杂（有很多动作命令），还有其他变体，比如“确定性策略梯度”。 这个变体使用另一个称为“评论者”的网络来学习评分函数。
