- en: Chapter 5\. Apache Beam Implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. Apache Beam实现
- en: '[Beam](https://beam.apache.org/) is an open source, unified model for defining
    both batch and streaming processing pipelines. It is not a stream processing engine
    (SPE), but rather an SDK with which you can build a pipeline definition that can
    be executed by one of Beam’s supported distributed processing backends. Using
    Beam, you can do the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[Beam](https://beam.apache.org/)是一个开源的、统一的模型，用于定义批处理和流处理管道。它不是一个流处理引擎（SPE），而是一个SDK，您可以使用它构建一个管道定义，然后由Beam支持的分布式处理后端之一来���行。使用Beam，您可以做到以下几点：'
- en: Use a single programming model for both batch and streaming use cases.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单一的编程模型来处理批处理和流处理用例。
- en: Through separation of building and execution, you can use the same Beam pipelines
    on multiple execution environments.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构建和执行的分离，您可以在多个执行环境上使用相同的Beam流水线。
- en: Write and share new SDKs, IO connectors, and transformation libraries, regardless
    of the specific runner.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写和共享新的SDK、IO连接器和转换库，不受特定运行程序的限制。
- en: Let’s take a look at how you can use Beam’s semantics to implement our solution.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Beam的语义来实现我们的解决方案。
- en: Overall Architecture
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整体架构
- en: Beam provides very rich [execution semantics](http://bit.ly/2gvXOzd) for stream
    merging including [`CoGroupByKey`](http://bit.ly/2ychhwv) and [`Combine`](http://bit.ly/2zfOwPG).
    It also supports [side inputs](http://bit.ly/2kGzv6w) for bringing calculations
    on one stream as an input for processing in another stream. Unfortunately, all
    of these APIs are designed for [windowed streams](http://bit.ly/2xznvoc) and do
    not work for the global windows—this is the problem that I am trying to solve.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Beam为流合并提供了非常丰富的[执行语义](http://bit.ly/2gvXOzd)，包括[`CoGroupByKey`](http://bit.ly/2ychhwv)和[`Combine`](http://bit.ly/2zfOwPG)。它还支持[side
    inputs](http://bit.ly/2kGzv6w)，将一个流上的计算作为另一个流的处理输入。不幸的是，所有这些API都设计用于[窗口流](http://bit.ly/2xznvoc)，不适用于全局窗口——这是我正在尝试解决的问题。
- en: The only option that I have found is using [`Flatten`](http://bit.ly/2wPgkcg),
    which allows you to merge multiple streams into one. Combining this with the [state](http://bit.ly/2xza5ZG)
    feature used to store a model provides a reasonable approach for overall implementation,
    as shown in [Figure 5-1](#the_beam_implementation_approach).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我找到的唯一选项是使用[`Flatten`](http://bit.ly/2wPgkcg)，它允许您将多个流合并为一个。结合用于存储模型的[state](http://bit.ly/2xza5ZG)功能，提供了一个合理的整体实现方法，如[图5-1](#the_beam_implementation_approach)所示。
- en: '![smlt 0501](assets/smlt_0501.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![smlt 0501](assets/smlt_0501.png)'
- en: Figure 5-1\. The Beam implementation approach
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. Beam实现方法
- en: The caveat for using the `Flatten` operator is that all combined streams must
    have the same data definition. To satisfy this requirement, I have introduced
    the `DataWithModel` data structure (described momentarily), which can contain
    either data or a model definition.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Flatten`操作符的注意事项是所有合并的流必须具有相同的数据定义。为了满足这一要求，我引入了`DataWithModel`数据结构（稍后描述），它可以包含数据或模型定义。
- en: Implementing Model Serving Using Beam
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Beam实现模型服务
- en: '[Example 5-1](#beam_pipeline_for_model_serving) shows the overall pipeline
    for model serving using Beam (Beam provides only Java and Python APIs; as a result,
    code in this section is Java [[complete code available here](http://bit.ly/2hBvbAA)]):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-1](#beam_pipeline_for_model_serving)展示了使用Beam进行模型服务的整体流程（Beam仅提供Java和Python
    API；因此，本节中的代码为Java[[完整代码在此处可用](http://bit.ly/2hBvbAA)]）：'
- en: Example 5-1\. Beam pipeline for model serving
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-1\. 用于模型服务的Beam流水线
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code begins by creating the pipeline and setting up a Kafka coder. Because
    both streams contain only messages with no key, you must use `NullableCoder` for
    the key. Then, it defines two input streams, a data stream and a model stream,
    combining them together. Finally, the combined stream is used for model serving.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先创建流水线并设置Kafka编码器。因为两个流都只包含没有键的消息，所以必须为键使用`NullableCoder`。然后，它定义了两个输入流，一个数据流和一个模型流，将它们合并在一起。最后，合并的流用于模型服务。
- en: For reading from Kafka, I am using the new Beam support for Kafka, [Kafka.IO](http://bit.ly/2g0Mgnd),
    which reads byte arrays from Kafka and then applies transformation on this data
    to create a `PCollection` of `DataWithModel`, is definied in [Example 5-2](#the_datawithmodel_class)
    ([complete code available here](http://bit.ly/2yEX45V)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从Kafka中读取数据，我正在使用新的Beam对Kafka的支持，[Kafka.IO](http://bit.ly/2g0Mgnd)，它从Kafka读取字节数组，然后对这些数据进行转换以创建一个`PCollection`的`DataWithModel`，在[示例5-2](#the_datawithmodel_class)中定义（[完整代码在此处可用](http://bit.ly/2yEX45V)）。
- en: Example 5-2\. The DataWithModel class
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-2\. DataWithModel类
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here `ModelDescriptor` is a Java version of the previous `ModelToServe` class
    ([Example 4-2](ch04.html#the_modeltoserve_class)) and `WineRecord` is a representation
    of the data protobuf definition ([Example 3-4](ch03.html#model_factory_representation)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `ModelDescriptor` 是之前 `ModelToServe` 类的 Java 版本（[示例 4-2](ch04.html#the_modeltoserve_class)），而
    `WineRecord` 则是数据 protobuf 定义的表示（[示例 3-4](ch03.html#model_factory_representation)）。
- en: '[Example 5-3](#converting_data_stream_to_datawithmodel) shows you how to handle
    the transformation of the data input to `DataWithModel` ([complete code available
    here](http://bit.ly/2i62j7h)):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-3](#converting_data_stream_to_datawithmodel) 展示了如何处理数据输入到 `DataWithModel`
    的转换（[完整代码在此处可用](http://bit.ly/2i62j7h)）：'
- en: Example 5-3\. Converting data stream to DataWithModel
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 将数据流转换为 DataWithModel
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Transformation of the model input to `DataWithModel` is equall simple.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型输入转换为 `DataWithModel` 的转换同样简单。
- en: 'The actual model serving uses [state support](http://bit.ly/2xza5ZG) and is
    implemented by the class shown in [Example 5-4](#implementation_of_the_model_scoring)
    ([complete code available here](http://bit.ly/2yb0kF7)):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的模型服务使用了[状态支持](http://bit.ly/2xza5ZG)，并由 [示例 5-4](#implementation_of_the_model_scoring)
    中所示的类实现（[完整代码在此处可用](http://bit.ly/2yb0kF7)）：
- en: Example 5-4\. Implementation of the model scoring
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 模型评分的实现
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This class gets the next element from the stream and checks its type. If this
    is a model element, it updates the current model. If it is a data element, it
    is scored (assuming a model is present). This code is very similar to the `DataProcessor`
    class ([Example 4-1](ch04.html#the_dataprocessor_class)) from the Flink implementation,
    with a significant difference: Flink’s `DataProcessor` class provides two different
    methods that can be invoked on two different threads so that any extended processing
    time for model loading does not affect scoring significantly. In the case of Beam,
    it’s a single method, so any delay in model creation can affect scoring.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类从流中获取下一个元素并检查其类型。如果这是一个模型元素，它就会更新当前模型。如果是数据元素，则会得分（假设存在模型）。这段代码与 Flink 实现中的
    `DataProcessor` 类（[示例 4-1](ch04.html#the_dataprocessor_class)）非常相似，但有一个重要区别：Flink
    的 `DataProcessor` 类提供了两种不同的方法，可以在两个不同的线程上调用，以便模型加载的任何扩展处理时间都不会显著影响得分。而在 Beam 的情况下，它是一个单一方法，因此模型创建的任何延迟都可能影响得分。
- en: Beam programs require a runner, and Flink is a popular choice. To be able to
    run this implementation on the Flink runner, which supports checkpointing, it
    is also necessary to implement a coder for the model object, as shown in [Example 5-5](#model_coder)
    ([complete code available here](http://bit.ly/2i4T4Ey)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Beam 程序需要一个运行器，而 Flink 是一个常用选择。为了能够在 Flink 运行器上运行此实现，它还必须实现一个模型对象的编码器，如[示例 5-5](#model_coder)
    中所示（[完整代码在此处可用](http://bit.ly/2i4T4Ey)）。
- en: Example 5-5\. Model coder
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 模型编码器
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although this example uses a single model, you can expand it easily to support
    multiple models by using a state in the form of a map of models keyed on the data
    type.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此示例仅使用一个模型，但您可以轻松地扩展它以支持多个模型，方法是使用一个以数据类型为键的模型映射的状态。
- en: Beam allows you to build an execution pipeline implementing model serving, which
    can be executed using multiple runners, including [Apache Apex](http://apex.apache.org/),
    [Apache Flink](http://flink.apache.org/), [Apache Spark](http://spark.apache.org/),
    and [Google Cloud Dataflow](https://cloud.google.com/dataflow). In [Chapter 6](ch06.html#apache_spark_implementation),
    we look at how you can use Spark streaming to solve the same problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Beam 允许您构建实现模型服务的执行管道，可以使用多个运行器执行，包括 [Apache Apex](http://apex.apache.org/)、[Apache
    Flink](http://flink.apache.org/)、[Apache Spark](http://spark.apache.org/) 和 [Google
    Cloud Dataflow](https://cloud.google.com/dataflow)。在[第 6 章](ch06.html#apache_spark_implementation)中，我们将看看如何使用
    Spark 流处理解决相同的问题。
