- en: Memcache at Facebook
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Facebook 上的 Memcache
- en: '6.824 2015 Lecture 16: Memcache at Facebook'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.824 2015 年第 16 讲：Facebook 上的 Memcache
- en: '**Note:** These lecture notes were slightly modified from the ones posted on
    the 6.824 [course website](http://nil.csail.mit.edu/6.824/2015/schedule.html)
    from Spring 2015.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 这些讲义笔记是从 2015 年春季的 6.824 [课程网站](http://nil.csail.mit.edu/6.824/2015/schedule.html)
    上稍作修改的。'
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Facebook Memcached paper:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook Memcached 论文：
- en: an experience paper, not a research results paper
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个经验论文，而不是研究结果论文
- en: you can read it as a triumph paper
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将其视为一篇胜利的论文。
- en: 'you can read it as a caution paper: what happens when you don''t think about
    scalability'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将其视为一篇警示论文：当你不考虑可扩展性时会发生什么。
- en: you can read it as a trade-offs paper
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将其视为一篇权衡的论文。
- en: Scaling a webapp
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展一个 Web 应用程序。
- en: Initial design for any webapp is a single webserver machine with a DB server
    running on it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 Web 应用程序的初始设计都是一个单个 Web 服务器机器，其中运行一个数据库服务器。
- en: 'Diagram (single machine: webserver and DB server)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图表（单台机器：Web 服务器和数据库服务器）
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: eventually they find out they use 100% of the CPU on this machine
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终他们发现他们在这台机器上使用了 100% 的 CPU。
- en: '`top` will tell them the CPU time is going to the web-app'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top` 告诉他们 CPU 时间正流向 Web 应用程序'
- en: 'Diagram (multiple webserver machines, single DB machine):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图表（多个 Web 服务器机器，单个数据库机器）：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: next problem they will face is the database will be the bottleneck
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来他们将面临的下一个问题是数据库将成为瓶颈
- en: DB CPU is 100% or disk is 100%
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库 CPU 是 100% 或磁盘是 100%
- en: say they decide to buy a bunch of DB servers
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们决定购买一堆数据库服务器
- en: 'Diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图表：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: now they gotta figure out how to *shard* the data on the database
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在他们得想办法如何在数据库上 *分片* 数据
- en: application software now has to know which DB server to talk to
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在应用软件必须知道要与哪个数据库服务器通信。
- en: we can no longer have transactions across the whole DB
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不能再跨整个数据库进行事务了。
- en: we can no longer have single queries on the entire dataset
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不能再对整个数据集进行单个查询了。
- en: need to send separate queries to each server
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要向每个服务器发送单独的查询。
- en: you can't push this too far because after a while the shards get very small
    and you get database servers that become hotspots
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能把这一点推得太远，因为过了一段时间，分片会变得非常小，你会得到成为热点的数据库服务器
- en: Next, you notice that most of the operations in the DB are reads (if that's
    the case. It is at Facebook.)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你注意到数据库中的大多数操作都是读取操作（如果是这样的话。在 Facebook 上是这样的）。
- en: it turns out you can build a very simple memory cache that can serve half a
    million requests per second
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原来你可以构建一个非常简单的内存缓存，可以每秒服务五十万个请求。
- en: then you can remove 90% of the load on the database
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后你可以减少数据库上的 90% 的负载。
- en: 'Diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图表：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: the next bottleneck will be database writes, if you keep growing your service
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你不断扩展你的服务，下一个瓶颈将是数据库写入。
- en: 'Observation: you could use DB read-only replicas, instead of your own customized
    memcache (MC) nodes. Facebook did not do this because they wanted to separate
    their caching logic from their DB deployment:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 观察：你可以使用数据库只读副本，而不是自己定制的 memcache（MC）节点。Facebook 没有这样做，因为他们想将缓存逻辑与数据库部署分开：
- en: It was the best choice given limited engineering resources and time. Additionally,
    separating our caching layer from our persistence layer allows us to adjust each
    layer independently as our workload changes
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是在有限的工程资源和时间内做出的最佳选择。此外，将缓存层与持久性层分开允许我们根据工作负载的变化分别调整每个层。
- en: Facebook's use case
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Facebook 的用例
- en: '**Very crucial:** they do not care too much about all their users getting a
    consistent view of their system.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**非常重要：** 他们并不太在意所有用户对系统的一致视图。'
- en: The only case when the paper cares about freshness and consistency is when webapp
    clients read their own writes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Web 应用程序客户端读取自己的写入时，唯一关心新鲜度和一致性的情况。
- en: 'Their high level picture:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的高层次图片：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The reason for having multiple data centers: parallelism across the globe'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有多个数据中心的原因：全球范围内的并行性
- en: maybe also for backup purposes (paper doesn't detail too much)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许也是为了备份目的（论文没有详细说明太多）。
- en: Big lessons
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大的教训
- en: Look-aside caching can be tricky
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘缓存可能会棘手
- en: This style of look-aside caching, where the application looks in the cache to
    see what's there, is extremely easy to add to an existing system
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种看到缓存中有什么的边缘缓存样式对于现有系统非常容易添加。
- en: but there are some nasty consistency problems that appear when the caching layer
    is oblivious to what happens in the DB
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是当缓存层不知道数据库中发生的情况时，会出现一些讨厌的一致性问题。
- en: Caching is about throughput not latency
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓存是关于吞吐量而不是延迟的。
- en: It wasn't about reducing latency for the users. They were using the cache to
    increase throughput and take the load off the database.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是为了减少用户的延迟。他们使用缓存来增加吞吐量并减轻数据库的负载。
- en: no way the DB could've handled the load, which is 10x or 100x more than what
    the DB can access
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库无法处理负载，负载是数据库可以访问的10倍或100倍
- en: They can tolerate stale data
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 他们可以容忍过时的数据
- en: They want to be able to read their own writes
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 他们希望能够读取自己的写入
- en: You'd think this can be easily fixed in the application. Slightly surprised
    that this was not fixed by just having the application remembering the writes.
    Not clear why they solved it differently.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为这可以很容易地在应用程序中修复。有点惊讶的是，这并没有通过应用程序记住写入来修复。不清楚为什么他们以不同的方式解决了这个问题。
- en: Eventual consistency is good enough
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最终一致性已经足够好了
- en: They have enormous fan-out
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 他们有巨大的扇出
- en: Each webpage they serve might generate hundreds and hundreds of reads. A little
    bit surprising. So they have to do a bunch of tricks. Issue the reads in parallel.
    When a single server does this, it gets a bunch of responses back, and the amount
    of buffering in the switches and webservers is limited, so if they're not careful
    they can lose packets and thus performance when retrying.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提供的每个网页可能会生成数百次读取。有点令人惊讶。所以他们必须做一些技巧。并行发出读取请求。当单个服务器这样做时，它会收到一堆响应，交换机和web服务器中的缓冲区限制有限，因此如果他们不小心，他们可能会丢失数据包，从而在重试时降低性能。
- en: Performance
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: a lot of content about consistency in the paper
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 论文中有很多关于一致性的内容
- en: but really they were desperate to get performance which led to doing tricks,
    which led to consistency problems
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但实际上他们迫切需要性能，这导致了一些技巧，也导致了一致性问题
- en: performance comes from being able to serve a lot of `Get`'s in parallel
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能来自于能够并行提供大量的`Get`请求
- en: 'Really only two strategies:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上只有两种策略：
- en: partition data
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区数据
- en: replicate data
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制数据
- en: they use both
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们两者都使用
- en: Partitioning works if keys are roughly all as popular. Otherwise, certain partitions
    would be more popular and lead to hotspots. Replication helps with handling demand
    for popular keys. Also, replication helps with requests from remote places in
    the world.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果键大致都一样受欢迎，分区就有效。否则，某些分区会更受欢迎并导致热点。复制有助于处理受欢迎键的需求。此外，复制有助于来自世界各地的请求。
- en: You can't simply cache keys in the web app servers, because they would all fill
    their memories quickly and you would double-store a lot of data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能简单地在web应用程序服务器中缓存键，因为它们会很快填满内存，而且会重复存储大量数据。
- en: Specific problem they dealt with
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 他们处理的具体问题
- en: Each cluster has a full set of memcache servers and a full set of web servers.
    Each web server talks to memcache servers in its own cluster.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群都有一整套memcache服务器和一整套web服务器。每个web服务器都与其自己集群中的memcache服务器通信。
- en: Adding a new cluster
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加一个新的集群
- en: Sometimes, they want to add a new cluster, which will obviously have empty memcache
    servers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，他们想要添加一个新的集群，这显然会有空的memcache服务器。
- en: all webservers in new cluster will always miss on every request and will have
    to go down and contact the DB, which cannot handle the increased load
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新集群中的所有web服务器在每个请求上都会缺失，并且必须关闭并联系数据库，而数据库无法处理增加的负载
- en: instead of contacting the DB, the new cluster will contact memcache servers
    from other clusters until the new cluster's cache is warmed up
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新集群不会联系数据库，而是会联系其他集群的memcache服务器，直到新集群的缓存被预热
- en: '**Q:** what benefit do they get from adding new clusters? instead of increasing
    size of existing cluster?'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：** 添加新集群会给他们带来什么好处？而不是增加现有集群的大小？'
- en: one possibility is there are some very popular keys so over-partitioning a cluster
    won't help with that
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可能性是有一些非常受欢迎的键，因此过度分区一个集群对此没有帮助
- en: another possibility could be that it's easier to add more memcache servers by
    adding a new cluster, because of the data movement problem
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可能性是通过添加新集群更容易添加更多的memcache服务器，因为数据移动问题
- en: Memcache server goes down
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Memcache服务器宕机
- en: If a memcache server goes down, requests are redirected to a gutter server.
    The gutter machines will miss a lot initially, but at least it will be caching
    the results for the future.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个memcache服务器宕机，请求将被重定向到一个沟槽服务器。沟槽机器最初会错过很多，但至少它会为未来缓存结果。
- en: 'Homework question:'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作业问题：
- en: '**Q:** Why aren''t gutters invalidated on writes?'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：** 为什么写入时不使沟槽失效？'
- en: On a write, DB typically sends an invalidate to all the MC servers that might
    have that key. So there's a lot of deletes being sent around to a lot of MC servers.
    Maybe they don't want to overflow the gutter servers with all the deletes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入时，数据库通常会向所有可能拥有该键的内存缓存服务器发送使其无效的消息。因此，大量的删除消息被发送到大量的内存缓存服务器。也许他们不想用所有的删除消息淹没槽沟服务器。
- en: Note that gutter keys expire after a certain time, to deal with the fact that
    the keys never change.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，槽沟键会在一定时间后过期，以应对键永远不变的情况。
- en: Not clear what happens if gutter servers go down.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果槽沟服务器崩溃，会发生什么并不清楚。
- en: '**Q:** Wouldn''t it better if the DB server sent the out the new value instead
    of invalidate.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：** 如果数据库服务器发送新值而不是使其无效，会不会更好。'
- en: what's cached in MC, might not be the DB value, but it might be some function
    of the DB value, that the DB layer is not aware of
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内存缓存中缓存的内容可能不是数据库值，而可能是数据库值的某些函数，数据库层不知道。
- en: think of how a friend list is stored in a DB versus how it would be stored in
    MC
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思考一下朋友列表在数据库中是如何存储的，与在内存缓存中是如何存储的。
- en: Leases for thundering herds
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 针对雷鸣般的奔袭的租约
- en: One client sends an update to the DB and that gets a popular key invalidated.
    So now lots of lots of clients generate Get's into MC, but the key was deleted,
    which would lead to lots of DB queries and then lots of caching of the result.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个客户端向数据库发送更新请求，并使一个热门键失效。因此现在有很多客户端向内存缓存生成获取请求，但该键已被删除，这将导致大量的数据库查询，然后是大量的结果缓存。
- en: If memcache receives a get for a key that's not present, it will set a lease
    on that key and say "you're allowed to go ask the DB for this key, but please
    finish doing this in 10 seconds." When subsequent Get's come in, they are told
    "no such key, but another guy is getting it, so please wait for him instead of
    querying the DB"
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内存缓存收到一个未找到的键的获取请求，它将在该键上设置一个租约，并说“你可以去询问数据库这个键，但请在10秒内完成。”当后续的获取请求到达时，它们会被告知“没有这样的键，但有另一个人正在获取它，所以请等待他而不是查询数据库”。
- en: The lease is cancelled after 10s or when the owner sets the key.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 租约在10秒后或所有者设置键之后取消。
- en: Each cluster will generate a separate lease.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群将生成一个单独的租约。
- en: 6.824 notes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.824 笔记
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
