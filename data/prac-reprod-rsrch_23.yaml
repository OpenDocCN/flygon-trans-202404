- en: 'Reproducible Applied Statistics: Is Tagging of Therapist-Patient Interactions
    Reliable?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重现的应用统计学：治疗师-患者互动的标记可靠吗？
- en: 'Reproducible Applied Statistics: Is Tagging of Therapist-Patient Interactions
    Reliable?'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重现的应用统计学：治疗师-患者互动的标记可靠吗？
- en: K. Jarrod Millman, Kellie Ottoboni, Naomi A. P. Stark and Philip B. Stark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K. Jarrod Millman、Kellie Ottoboni、Naomi A. P. Stark 和 Philip B. Stark
- en: We are three applied statisticians (JM, KO, PS) at UC Berkeley working with
    a domain specialist (NS) at the University of Pennsylvania. Our case study involves
    assessing inter-rater reliability (IRR) of the assignment of “tags” applied by
    human raters to classify interactions during therapy sessions with children on
    the autistic spectrum.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是加州大学伯克利分校的三位应用统计学家（JM、KO、PS），与宾夕法尼亚大学的领域专家（NS）合作。我们的案例研究涉及对自闭症谱系儿童治疗过程中人际可靠性（IRR）的评估，这涉及到人类评分员对治疗过程中的互动进行分类标记。
- en: An extended version of this case study along with the analysis script and results
    can be found at [https://github.com/statlab/nsgk](https://github.com/statlab/nsgk).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究的扩展版本以及分析脚本和结果可在 [https://github.com/statlab/nsgk](https://github.com/statlab/nsgk)
    找到。
- en: Workflow
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作流程
- en: '![Diagram](millmanOttoboniStark.png) Our project arose from a pilot study NS
    was working on with Dr. Gilbert Kliman of the Children’s Psychological Health
    Center in San Francisco. To investigate therapeutic interventions with children
    on the autistic spectrum, Dr. Kliman and NS collected some observational data
    (described below). NS approached PS about the data and the problem NS was studying.
    After investigating the problem further, PS emailed JM and KO a one-page proposal
    for a stratified permutation test to assess inter-rater reliability using stratified
    samples. We (JM, KO, PS) had recently begun developing a general purpose Python
    package for permutation tests, called [`permute`](http://statlab.github.io/permute/),
    based on our collaborations. PS suggested this would be an interesting example
    to include.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![Diagram](millmanOttoboniStark.png) 我们的项目源自 NS 与旧金山儿童心理健康中心的 Gilbert Kliman
    博士合作的一项试点研究。为了研究对自闭症谱系儿童的治疗干预，Kliman 博士和 NS 收集了一些观察数据（下文描述）。NS 向 PS 寻求关于数据和她研究的问题的帮助。在进一步调查问题后，PS
    向 JM 和 KO 发送了一个一页的建议，建议使用分层抽样的分层排列测试来评估人际可靠性。我们（JM、KO、PS）最近开始开发一个通用的 Python 包用于排列测试，称为[`permute`](http://statlab.github.io/permute/)，基于我们的合作。PS
    建议这将是一个有趣的案例。'
- en: After coming to an initial understanding of NS’s underlying research question
    and experiment, including how she collected the data, we (JM, KO, PS) cleaned
    the data, developed a nonparametric approach to assessing IRR appropriate to the
    experiment, implemented the approach in Python, incorporated the resulting code
    into our evolving Python package of permutation tests, applied the approach to
    the cleaned data, documented the code and the analysis, and wrote up the results
    in LaTeX.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 NS 的基本研究问题和实验有了初步了解，包括她如何收集数据后，我们（JM、KO、PS）清理了数据，开发了一个适合实验的评定 IRR 的非参数方法，在
    Python 中实现了这个方法，将结果代码纳入我们不断完善的 Python 包中的排列测试中，将这个方法应用于清理后的数据，记录了代码和分析过程，并在 LaTeX
    中写出了结果。
- en: 'We distinguish the following aspects of our project, which are typical in applied
    statistics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们区分了项目中以下典型的应用统计学方面：
- en: understand problem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解问题
- en: get and clean data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取并清理数据
- en: design algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计算法
- en: implement algorithm
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现算法
- en: analyze data
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析数据
- en: understand result
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解结果
- en: Figure 1 shows how each aspect of the project influenced the other aspects and
    gives estimates of the total person-hours we collectively spent on each aspect
    of the project. For example, if JM, KO, and PS spent an hour together discussing
    the problem in a meeting, then that meeting counts as 3 people hours.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 显示了项目的每个方面是如何影响其他方面的，并给出了我们集体在项目的每个方面上花费的总人时的估计。例如，如果 JM、KO 和 PS 一起在会议上讨论问题花了一个小时，那么这个会议就计为
    3 人时。
- en: We did not keep a detailed record of time spent, but our computational practices
    provide enough detail about who did what when that we believe our estimates provide
    an accurate qualitative account of the time demands for each aspect of the project.
    However, since these are only rough estimates, the reader should focus on the
    relative differences in the amount of time we spent on each aspect. We have found
    that researchers (ourselves included) often underestimate the time needed to understand
    the problem, acquire and clean the data, as well as understand the results, while
    overestimating the time needed for writing code. We have included our time estimates
    to give people an idea how “inexpensive” (or expensive) working more reproducibly
    is, to capture how our group understanding evolved, and in the hope that it might
    be instructive for students and collaborators.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有详细记录花费的时间，但我们的计算实践提供了关于谁在何时做了什么的足够细节，我们相信我们的估计提供了对项目各方面时间需求的准确定性描述。 但是，由于这些只是粗略估计，读者应该关注我们在每个方面花费的时间量的相对差异。
    我们发现研究人员（包括我们自己）通常低估了理解问题、获取和清理数据以及理解结果所需的时间，而高估了编写代码所需的时间。 我们包含了我们的时间估计，以便让人们了解更具有再现性工作的“廉价”（或昂贵），捕捉我们团队理解的演变，并希望这对学生和合作者有所启发。
- en: Since we view computational reproducibility as a cross-cutting concern of all
    project aspects, we have adopted a set of computational practices, which we (JM,
    KO, PS) followed (almost) whenever we were working on the project. Exceptions
    include that we did not record all of our in-person discussions or whiteboard
    work. However, we endeavored to record summaries of these activities. These computational
    practices, described in Millman & Pérez (2014), are used widely in the open source
    scientific Python community. While developed for managing software contributions,
    these practices are ideal for ensuring computational reproducibility in scientific
    and statistical research. We will illustrate how we leverage the software infrastructure
    and development practices of `permute` to conduct reproducible and collaborative
    applied statistics research with our colleagues. We discuss the software tools
    and practices briefly in Key tools and practices below.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将计算再现性视为所有项目方面的跨领域关注点，我们采用了一套计算实践，我们（JM，KO，PS）在项目中（几乎）每次工作时都遵循。 例外情况包括我们没有记录所有面对面讨论或白板工作。
    但是，我们努力记录这些活动的摘要。 这些计算实践在 Millman & Pérez（2014）中描述，在开源科学 Python 社区中被广泛使用。 虽然这些实践是为管理软件贡献而开发的，但这些实践非常适合确保科学和统计研究的计算再现性。
    我们将说明我们如何利用`permute`的软件基础设施和开发实践与同事进行可重复和协作的应用统计研究。 我们将在下面的关键工具和实践中简要讨论软件工具和实践。
- en: Understand problem (80 hours)
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解问题（80 小时）
- en: 'The Kliman-Stark research project sought to identify characteristics of effective
    clinical interactions with children on the autistic spectrum. The project first
    required developing a set of characteristics that observers could use to “tag”
    what was happening in each 30-second interval of a therapy session. After they
    developed a taxonomy of clinical interactions, Kliman and NS had a number of trained
    raters watch videos of therapy sessions and label each 30-second interval using
    the collection of tags. For the classification system to be meaningful and useful,
    different raters must generally agree on whether a given tag applies to a given
    video segment: there must be inter-rater reliability. Of course, if a tag is never
    used or is always used, inter-rater reliability will be perfect, but the tag is
    useless for distinguishing clinical interactions.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kliman-Stark 研究项目旨在确定与自闭症谱系儿童进行有效临床互动的特征。 该项目首先需要开发一组观察者可以使用的特征，以便在每个治疗会话的 30
    秒间隔内“标记”发生的情况。 在他们开发了临床互动的分类法之后，Kliman 和 NS 请了许多经过训练的评分员观看治疗会话的视频，并使用一系列标签为每个
    30 秒间隔进行标记。 为了使分类系统具有意义和实用性，不同的评分员通常必须就某个标签是否适用于某个视频片段达成一致意见：必须具有评分员间的一致性。 当然，如果某个标签从未被使用或总是被使用，评分员间的一致性将是完美的，但该标签对于区分临床互动是无用的。
- en: 'That led to a statistical question: how to assess the evidence in the tagged
    videos that different raters tag interactions the same way? After numerous conversations,
    it made sense to consider the null hypothesis to be that, conditional on the number
    of times a given rater applied a given label to a given video, all assignments
    of that label to time stamps in the video by that rater are equally likely, and
    the ratings given by different raters are exchangeable (essentially, that raters
    are independent).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那引出了一个统计问题：如何评估标记视频中的证据，以确定不同的评分者是否以相同的方式标记互动？经过多次对话后，认为考虑零假设是合理的，即在给定评分者对给定视频应用给定标签的次数的条件下，该评分者对视频中的时间戳分配给定标签的所有分配都是等可能的，并且不同评分者给出的评分是可交换的（基本上是评分者是独立的）。
- en: Once PS had an initial understanding of NS’s problem, we (JM, KO, PS) met regularly
    (approximately weekly, sometimes more) as a team to discuss the project. Initially
    these discussions involved a lot of work on whiteboards and asking a lot of probing
    questions. This helped us develop a shared understanding of the problem, understanding
    that improved by explaining things to one another and by asking hard questions
    about our planned approach and whether it could address the question of interest.
    As our understanding of the problem progressed, our work transitioned from working
    on whiteboards to testing our ideas out on a computer. We often used pair programming
    at this stage and sometimes we all sat in front of one computer, while one of
    us typed code in an interactive IPython session. This helped ensure that we all
    understood the problem well and it also helped us catch errors (typos as well
    as conceptual misunderstandings).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦PS对NS的问题有了初步的了解，我们（JM、KO、PS）定期（大约每周一次，有时更频繁）作为一个团队会面讨论项目。最初，这些讨论涉及很多白板工作和问很多深入的问题。这帮助我们发展出对问题的共同理解，通过向彼此解释事情和询问我们计划的方法是否能够解决感兴趣的问题来提高理解。随着对问题的理解的进展，我们的工作从在白板上工作转移到在计算机上测试我们的想法。在这个阶段我们经常使用配对编程，有时我们都坐在一台电脑前，其中一人在交互式IPython会话中键入代码。这有助于确保我们都对问题有很好的理解，同时也有助于我们发现错误（打字错误以及概念上的误解）。
- en: Get and clean data (13 hours)
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取并清洁数据（13小时）
- en: The tag data were collected by NS and raters working at her direction. The data
    comprise ratings of segments of 8 videos by 10 trained raters. Each video is divided
    into approximately 40 time segments. In each time segment, none, any, or all of
    183 types of activity might be taking place. The raters indicated which of those
    activities was taking place during each segment of each video.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数据由NS和她指导的评分者收集。数据包括由10名训练有素的评分者对8个视频片段的评分。每个视频大约分成40个时间段。在每个时间段中，可能正在进行183种活动中的任何一种活动，也可能没有。评分者指出了视频的每个片段中正在进行的活动。
- en: PS received the data from NS as an Excel spreadsheet that had been entered by
    hand by NS and an assistant. Understanding the “data dictionary” and vetting for
    obvious errors entailed several rounds of email between PS and NS before PS had
    a version of the data that did not have obvious errors. PS then exported the Excel
    data to comma-separated value (CSV) format. The original data contained personally
    identifying information. Using regular expressions in an interactive text editor,
    PS substituted unique numerical identifiers for raters’ names in the CSV file.
    While this step was not performed reproducibly (i.e., not scripted), it can be
    checked readily. After PS generated the original anonymized data, JM committed
    it to our repository and added a data loader with tests to ensure that if the
    data changed we would know. At this point, we (JM, PS) screened the anonymized
    data for transcription errors (e.g., duplicate rows). This involved writing a
    number of quality assurance tools (e.g., to find duplicate consecutive rows),
    which are now included in `permute`. Once we identified entries incompatible with
    our understanding of what should be in the data, JM wrote a `sed` script to “correct”
    the inferred typos. The exact commands used to clean the data are included in
    the commit corresponding to that cleaning step. After carefully examining the
    data for potential errors and documenting every change we made and why, we sent
    the cleaned data and an explanation of what we did to NS to verify that the corrections
    were appropriate. As a result, we provide the cleaned data in our project repository
    as well as a careful account of its provenance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PS 从 NS 那里收到了一份 Excel 电子表格，该表格由 NS 和一位助手手动输入。在 PS 获得一份没有明显错误的数据版本之前，需要进行几轮 PS
    和 NS 之间的电子邮件往来，以理解“数据字典”并对明显错误进行审查。然后，PS 将 Excel 数据导出为逗号分隔值（CSV）格式。原始数据包含个人身份信息。在交互式文本编辑器中使用正则表达式，PS
    在 CSV 文件中为评分人的姓名替换了唯一的数字标识符。虽然这一步骤没有可重复执行（即没有脚本化），但可以很容易地进行检查。在 PS 生成原始匿名化数据后，JM
    将其提交到我们的存储库，并添加了一个数据加载器以及用于确保数据更改后我们能够知道的测试。在此时，我们（JM、PS）对匿名化数据进行了筛查，以查找转录错误（例如，重复行）。这涉及编写一些质量保证工具（例如，查找重复连续行），这些工具现在包含在
    `permute` 中。一旦我们确定了与我们对数据应该包含的内容的理解不兼容的条目，JM 就会编写一个 `sed` 脚本来“更正”推断的拼写错误。用于清理数据的确切命令包含在对应于该清理步骤的提交中。在仔细检查数据以寻找潜在错误并记录我们进行的每一项更改及其原因后，我们向
    NS 发送了清理后的数据以及我们所做的更改及其原因的解释，以验证更正是否合适。因此，我们在项目存储库中提供了清理后的数据以及对其来源的仔细说明。
- en: Design algorithm (25 hours)
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计算法（25 小时）
- en: Although the test we eventually implemented was very similar to the original
    test proposed by PS at the start of the project, we (JM, KO, PS) spent significant
    time focused on “problem appreciation,” some of which resulted in considerable
    simplification of the algorithm used to implement the test. We also developed
    a more general terminology (see Table 1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们最终实施的测试与项目开始时由 PS 提出的原始测试非常相似，但我们（JM、KO、PS）花费了大量时间专注于“问题认知”，其中一些导致了用于实施测试的算法的大幅简化。我们还开发了更一般化的术语（见表1）。
- en: Mapping between terms from our motivating problem (NSGK) and the terms used
    in our general algorithm (IRR).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的激励问题（NSGK）中的术语与我们的通用算法（IRR）中使用的术语之间的映射。
- en: '| NSGK | IRR |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| NSGK | IRR |'
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 183 types of activity | *T* tags |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 183 种活动类型 | *T* 个标签 |'
- en: '| 8 videos | *S* strata |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 8 个视频 | *S* 个分层 |'
- en: '| 40 segments/videos | *N*[*s*] items/strata |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 40 个片段/视频 | *N*[*s*] 个项目/分层 |'
- en: '| 10 raters | *R* raters |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 10 个评分人 | *R* 个评分人 |'
- en: We decided to assess rater reliability in identifying (i.e., tagging) each of
    the 183 types of activity separately, because they are of separate interest. This
    introduces questions about whether inferences are to be made about each tag separately
    (per-comparison error rate, PCER) or simultaneously (familywise error rate, FWER),
    or whether we are concerned with the fraction of tags we conclude are reliable
    that in fact are not reliable (false discovery rate, FDR). Ultimately, we decided
    that the PCER was the most relevant error criterion, since the tags are individually
    interesting. As a “first cut” through the rating scheme, eliminating tags that
    are clearly not reliable across raters simplifies the scheme and reduces the cognitive
    burden on raters, because they do not have to keep so many categories of activity
    in mind. We imagined that if we could eliminate a substantial number of the tags
    as unreliable, there would be a repeat of the tagging using a different set of
    raters to validate or refine the results, reducing the rate of “false positives.”
    On the other hand, incorrectly rejecting tags as unreliable could eliminate a
    potentially useful predictor of successful therapeutic outcomes, so the FWER seemed
    far too stringent a criterion. See the Understand result section below for more
    discussion.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定评估鉴定（即标记）每种183种活动的评定者可靠性，因为它们是独立感兴趣的。这引发了关于是否要对每个标记分别进行推断（每次比较错误率，PCER）还是同时进行推断（整体错误率，FWER），或者我们是否关心我们得出的可靠标记的比例实际上并不可靠（虚假发现率，FDR）的问题。最终，我们决定PCER是最相关的错误标准，因为这些标记各自很有趣。作为对评分方案的“初步尝试”，消除明显不可靠的标记简化了方案，并减少了评定者的认知负担，因为他们不必记住那么多种类的活动。我们想象，如果我们能够消除大量不可靠的标记，将会有一组不同的评定者重复使用标记来验证或完善结果，从而降低“假阳性”的比率。另一方面，错误地拒绝标记为不可靠可能会消除一个潜在有用的成功治疗结果的预测因子，因此FWER似乎是一个过于严格的标准。有关更多讨论，请参见下面的理解结果部分。
- en: 'Since each of the videos contained different sessions of therapist-patient
    interactions, in general rated by different people, we stratified the test by
    video. A literature search for approaches to assessing IRR led us to conclude
    that there was no existing suitable method for several reasons: the experiment
    was stratified; there were multiple raters but not the same set for all videos;
    and standard methods required indefensible parametric assumptions or population
    models, which we hoped to avoid. After deciding to use permutation tests, we (JM,
    KO, PS) then determined that permuting each rater’s ratings within a video, independently
    across raters and across videos, made sense as the appropriate invariant under
    the null hypothesis. We chose to use concordance of ratings as our partial test
    statistic within each stratum. We (JM, PS) derived a simple expression for efficiently
    computing the concordance. To combine tests across strata, we (JM, KO, PS) used
    the nonparametric combination (NPC) of tests (Pesarin & Salmaso, 2010) with Fisher’s
    combining function. Finally, we developed a computationally efficient approach
    to finding the overall *p*-value for the NPC test.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个视频包含不同的治疗师-患者互动会话，通常由不同的人评分，我们按视频进行分层测试。对于评估IRR方法的文献搜索使我们得出结论，由于实验是分层的；有多个评定者但不是所有视频的相同集合；标准方法需要不可辩护的参数假设或人口模型，我们希望避免这种情况。在决定使用置换测试后，我们（JM，KO，PS）然后确定在视频内每个评定者的评分进行置换，在评定者之间和视频之间独立进行，这在零假设下是合理的不变量。我们选择使用评分的一致性作为每个分层内的部分检验统计量。我们（JM，PS）推导出了一个简单的表达式，用于高效计算一致性。为了跨分层组合测试，我们（JM，KO，PS）使用了非参数组合（NPC）测试（Pesarin＆Salmaso，2010）与Fisher的组合函数。最后，我们开发了一种计算效率高的方法来找到NPC测试的整体*p*值。
- en: Implement algorithm (5 hours)
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施算法（5小时）
- en: Once we had a blueprint of the algorithm, KO led the implementation effort.
    She did most of the coding; JM and PS reviewed the code and discussed the implementation.
    Following our software development practices, KO also wrote tests for every function
    she implemented. After a few iterations of coding, testing, and review, KO finalized
    our implementation and we merged it into `permute`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了算法的蓝图，KO带领实施工作。她做了大部分编码；JM和PS审查了代码并讨论了实施细节。遵循我们的软件开发实践，KO还为她实现的每个函数编写了测试。经过几轮编码、测试和审查，KO完成了我们的实现，并将其合并到`permute`中。
- en: 'KO wrote three functions to implement our general IRR algorithm:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KO编写了三个函数来实现我们的一般IRR算法：
- en: a function to compute the IRR partial test statistic from a binary matrix with
    one row per rater and one column per item;
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个函数，用于从具有每个评分者一行和每个项目一列的二进制矩阵计算IRR部分测试统计量；
- en: a function to simulate the permutation distribution of the IRR partial test
    statistic for a matrix of ratings of a single stratum;
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个函数，用于模拟单个层的评分矩阵的IRR部分测试统计量的排列分布；
- en: a function to simulate the permutation distribution of the NPC test statistic
    by combining the *S* distributions of the IRR partial test statistic for each
    of the *S* strata.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个函数，用于通过组合每个*S*层的IRR部分测试统计量的*S*分布来模拟NPC测试统计的排列分布。
- en: Analyze data (1 hour)
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析数据（1小时）
- en: Once we merged KO’s implementation of the general algorithm (including tests)
    into `permute`, KO wrote a short script (about 50 lines of Python) to analyze
    the cleaned data from NS.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将KO的通用算法实现（包括测试）合并到`permute`中，KO就编写了一个简短的脚本（大约50行Python代码）来分析NS的清理数据。
- en: 'Since we included the main workhorse functions in `permute`, the analysis script
    contained only high-level commands:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在`permute`中包含了主要的工作函数，分析脚本只包含高级命令：
- en: Load the cleaned data
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载清理后的数据
- en: 'For each of the 183 categories of activity:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于活动的183个类别中的每一个：
- en: 'For each of the 8 videos:'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这8个视频中的每一个：
- en: Compute the mean and standard deviation of the number of times the tag was applied
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算标签被应用的次数的平均值和标准差
- en: Compute the IRR partial test statistic
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算IRR部分测试统计量
- en: Simulate the permutation distribution of the NPC test statistic for each tag
    combined over the *8* videos, and report a single *p*-value
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个标签结合在一起的*8*个视频，模拟NPC测试统计的排列分布，并报告一个单一的*p*-值。
- en: Save the results to a CSV file
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果保存到CSV文件中
- en: Understand result (20 hours)
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解结果（20小时）
- en: 'At a high level, even the summary statistics we computed were useful: some
    tags were never applied by any rater to any video. Presumably, the tag taxonomy
    could be simplified by eliminating those tags from the universe of labels, reducing
    the cognitive burden on the human raters. There were also tags that were used
    so frequently that high concordance was virtually guaranteed—and therefore high
    inter-rater concordance was not evidence of inter-rater reliability. This may
    also imply that any differences in efficacy of therapy are not attributable to
    whether the corresponding activity is taking place, since it is often taking place,
    at least in these sessions. Whether it makes sense to keep such tags in the taxonomy
    depends in part on subject matter knowledge: are those interactions typical only
    in the videos in these evaluation data, or are they typical of all therapeutic
    interventions with children on the autistic spectrum?'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们计算的汇总统计量甚至也很有用：一些标签从未被任何评分者应用到任何视频上。可以推测，通过从标签的标签宇宙中消除这些标签，可以简化标签分类，减少人类评分者的认知负担。还有一些标签被使用得如此频繁，以至于高一致性几乎是肯定的——因此高评分者之间的一致性并不是评分者之间可靠性的证据。这也可能意味着疗法效果的任何差异都不能归因于相应的活动是否正在进行，因为这些活动通常至少在这些会话中进行。是否应该保留这些标签在分类中，部分取决于主题知识：这些交互在这些评估数据中的视频中是否只是典型的，还是在自闭症谱系儿童的所有治疗干预中都是典型的？
- en: 'At the other extreme, there were tags for which the concordance of use was
    quite low, but still highly significant. This raises the scientific question of
    what threshold level of agreement among raters makes a tag interesting or useful,
    separate from whether the agreement is statistically significant. That is a matter
    we need to discuss at greater length with the domain specialists. It also points
    to a frequent situation in statistics: practical significance and statistical
    significance are not the same thing, and one must consider “fitness for use” when
    devising summary statistics.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个极端，有些标签的使用一致性非常低，但仍然非常显著。这引发了一个科学问题：在评分者之间的一致性达到何种阈值水平会使标签变得有趣或有用，而不是一致性是否在统计上显著。这是我们需要与领域专家更详细地讨论的问题。这也指向了统计学中经常出现的情况：实际意义和统计显著性并不相同，而在设计汇总统计量时必须考虑“适用性”。
- en: We hope that the concrete findings will lead to a refinement of the taxonomy
    and additional tests of reliability. We hope that those tests will involve greater
    automation of data collection and transcription, to eliminate some of the sources
    of error in the data. Regardless, this work has led to a new nonparametric test
    for inter-rater reliability, now available publicly in the `permute` package.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望具体的发现将导致分类法的完善和可靠性的额外测试。我们希望这些测试将涉及更大规模的数据收集和转录自动化，以消除数据中的一些误差来源。不管怎样，这项工作已经引出了一种新的无参数测试方法，现在公开在
    `permute` 包中。
- en: Pain points
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 痛点
- en: Given our different backgrounds and experiences we (JM, KO, PS) each found different
    points in the process challenging. However, for all of us the most challenging
    aspect -- and the most time-consuming -- was the necessary struggle to understand
    the scientific question and the experiment well enough to devise an approach to
    answering the question.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们不同的背景和经验，我们（JM、KO、PS）每个人在过程中都发现了不同的挑战点。然而，对于我们所有人来说，最具挑战性的方面——也是最耗时的方面——是必须努力理解科学问题和实验，以便能够制定回答问题的方法。
- en: For KO and PS there was a learning curve to master the tools and practices.
    This involved understanding the data model used by git, acquiring habits such
    as writing tests for all functions and following a common style guide, and learning
    to contribute to the project repository indirectly through GitHub’s pull request
    mechanism. JM was already familiar with the tools and practices, and devoted significant
    time to teaching KO and PS the workflow. Once mastered, the benefits of these
    tools and habits outweigh the time and effort spent learning them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 KO 和 PS 来说，掌握工具和实践存在一定的学习曲线。这涉及理解 git 使用的数据模型，养成习惯，比如为所有函数编写测试，并遵循共同的风格指南，以及学会通过
    GitHub 的拉取请求机制间接地向项目存储库做出贡献。JM 已经熟悉这些工具和实践，并花了大量时间教导 KO 和 PS 工作流程。一旦掌握，这些工具和习惯的好处就会超过学习它们所花费的时间和精力。
- en: 'For JM the most painful part of the project was vetting hand-entered data to
    look for errors and inconsistencies. Not only was this laborious, but it involved
    inferring what the data should have been without any direct way to ensure that
    these inferences were correct: the original raters and videos were not available
    to us. The solution to this pain point is to automate data collection as much
    as possible. However, when data have already been entered by hand, there is not
    much that can be done other than being cautious when “fixing” data entry errors
    and recording every aspect of the data cleaning process.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对 JM 而言，项目中最痛苦的部分是审查手工录入的数据以查找错误和不一致之处。这不仅费时费力，而且涉及到推断数据本应该是什么，而没有任何直接的方法来确保这些推断是正确的：原始评分者和视频对我们不可用。解决这个痛点的方法是尽可能自动化数据收集。然而，当数据已经手工输入时，除了在“修复”数据输入错误时要谨慎，记录数据清理过程的每个方面之外，别无他法。
- en: Key benefits
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键优势
- en: Since Buckheit & Donoho (1995) popularized the idea of computational reproducibility,
    applied statisticians have increasingly embraced version control and process automation.
    Many of our colleagues have made the idea of computational reproducibility central
    in both the classroom and the lab. Some ask anyone working with them to follow
    a set of computational practices including version control.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Buckheit & Donoho（1995）推广了计算可重现性的概念以来，应用统计学家越来越多地采用了版本控制和流程自动化。我们许多同行已经在课堂和实验室中将计算可重现性的概念放在了核心位置。有些人要求与他们合作的人遵循一套包括版本控制在内的计算实践。
- en: 'However, the computational practices described in this study (see Key tools
    and practices) go beyond the standard work habits of our colleagues. Our computational
    practices provide the following benefits:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本研究描述的计算实践（请参阅关键工具和实践）超越了我们同行的标准工作习惯。我们的计算实践提供以下好处：
- en: it reduces the number of errors introduced by new code and changes to existing
    code
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它减少了由新代码和对现有代码的更改引入的错误数量
- en: it makes it easy to modify the analysis when errors are found, to apply the
    analysis to new datasets, and so on
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当发现错误时，修改分析变得容易，将分析应用于新数据集等。
- en: the process is self-documenting, making it easier to draft a paper about the
    results or to pick up where we left off after working on something else
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程是自我记录的，这样就更容易起草有关结果的论文，或者在做其他工作后继续进行。
- en: the methods are abstracted from the analysis and incorporated into a package
    so that others can discover, check, use, and extend our methods.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法是从分析中抽象出来并纳入包中，以便其他人可以发现、检查、使用和扩展我们的方法。
- en: Key tools and practices
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键工具和实践
- en: As part of the development of our software package `permute`, we invested significant
    effort in setting up a development infrastructure to ensure our work is tracked,
    thoroughly and continually tested, and incrementally improved and documented.
    To this end, we have adopted best practices for software development used by successful
    open source projects (Millman & Pérez, 2014).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们软件包 `permute` 的开发的一部分，我们投入了大量精力建立开发基础设施，以确保我们的工作被追踪，彻底和持续测试，并且逐步改进和记录。为此，我们采用了成功的开源项目（Millman
    & Pérez, 2014）所使用的软件开发最佳实践。
- en: Version control and code review
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 版本控制和代码审查
- en: We (JM, KO, PS) use git as our version control system (VCS) and GitHub as the
    public hosting service for our official `upstream` repository [statlab/permute](https://github.com/statlab/permute).
    Each of us has our own copy, or fork, of the `upstream` repository. We each work
    on our own repositories and use the `upstream` repository as our coordination
    or integration repository.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们（JM、KO、PS）使用 git 作为我们的版本控制系统（VCS），GitHub 作为我们官方 `upstream` 仓库 [statlab/permute](https://github.com/statlab/permute)
    的公共托管服务。我们每个人都有我们自己的 `upstream` 仓库的副本，或者叫分叉。我们每个人都在自己的仓库上工作，并使用 `upstream` 仓库作为我们的协调或集成仓库。
- en: This allows us to track and manage how our code changes over time and to review
    new functionality before merging it into the `upstream` repository. To get new
    code or text integrated in the `upstream` repository, we use GitHub’s *pull request*
    mechanism. This enables us to review code and text before integrating it. Below,
    we describe how we automate testing our code to generate reports for all pull
    requests. This way we can reduce the risk that changes to our code break existing
    functionality. Once a pull request is reviewed and accepted, it is merged into
    the `upstream` repository.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够跟踪和管理我们的代码随时间的变化，并在将新功能合并到 `upstream` 仓库之前进行审查。为了将新代码或文本集成到 `upstream`
    仓库中，我们使用 GitHub 的 *pull request* 机制。这使我们能够在集成之前审查代码和文本。下面，我们描述了如何自动测试我们的代码以为所有
    pull request 生成报告。这样我们就可以降低代码更改破坏现有功能的风险。一旦 pull request 被审查并接受，它就会被合并到 `upstream`
    仓库中。
- en: Requiring all new code to undergo review provides several benefits. Code review
    increases the quality and consistency of our code. It helps maintain a high level
    of test coverage (see below). Moreover, it also helps keep the development team
    aware of the work other team members are doing. While we are currently a small
    team and we meet regularly, having the code review system in place will make it
    easier for new people to contribute as well as capturing our design discussions
    and decisions for future reference.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要求所有新代码都经过审查具有多个好处。代码审查提高了我们代码的质量和一致性。它有助于保持高水平的测试覆盖率（见下文）。此外，它还帮助开发团队了解其他团队成员正在做的工作。虽然我们目前是一个小团队，并且定期会面，但建立代码审查系统将使新人更容易贡献，并捕获我们的设计讨论和决策以供将来参考。
- en: Testing and continuous integration
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试和持续集成
- en: We used the `nose` testing framework for automating our testing procedures.
    This is the standard testing framework used by the core packages in the scientific
    Python ecosystem. Automating testing allows us to monitor a proxy for code correctness
    when making changes as well as simplifying the code review process for new code.
    Without automated testing, we would have to manually test all the code every time
    a change is proposed. The `nose` testing framework simplifies test creation, discovery,
    and execution. It has an extensive set of plugins to add functionality for coverage
    reporting, test annotation, profiling, as well as inspecting and testing documentation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 `nose` 测试框架来自动化我们的测试过程。这是科学 Python 生态系统中核心包使用的标准测试框架。自动化测试使我们能够在进行更改时监视代码正确性的代理，同时简化新代码的代码审查流程。没有自动化测试，每次提出更改时，我们都必须手动测试所有代码。`nose`
    测试框架简化了测试的创建、发现和执行。它有一系列广泛的插件，用于添加覆盖率报告、测试注释、分析，以及检查和测试文档的功能。
- en: Our goal is to test every line of code. For example, not only do we want to
    test every function in our package, but if a specific function has a conditional
    branching structure we test each possible execution path through that function.
    Having tested each line of code increases our confidence in our code and provides
    some assurance that changes we make do not break existing code. It also increases
    our confidence that new code works, which reduces the friction of accepting contributions.
    Currently over 98% of the lines of code in `permute` get executed at least once
    by our test system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是测试每一行代码。例如，我们不仅要测试包中的每个函数，而且如果特定函数具有条件分支结构，我们会测试该函数的每个可能执行路径。测试每一行代码可以增加我们对代码的信心，并确保我们的更改不会破坏现有代码。它还增加了我们对新代码有效性的信心，减少了接受贡献的摩擦。目前，`permute`
    中超过 98% 的代码行至少会被我们的测试系统执行一次。
- en: We often work on several pull requests simultaneously. These pull requests may
    take several weeks or months before they are reviewed, improved, and accepted
    in our `upstream` repository. While we are working on one pull request, we may
    merge several others. Since the underlying code base is changing, each pull request
    may potentially introduce integration conflicts when we attempt to merge it back
    into the main line. To mitigate the difficulty in managing these conflicts we
    employ continuous integration and track our test coverage.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常同时处理几个拉取请求。这些拉取请求可能需要几个星期或几个月才能被审查、改进并被接受到我们的 `upstream` 存储库中。当我们处理一个拉取请求时，我们可能会合并其他几个拉取请求。由于基础代码库正在更改，每个拉取请求在尝试将其合并回主线时可能会引入集成冲突。为了减轻处理这些冲突的困难，我们使用持续集成并跟踪我们的测试覆盖率。
- en: 'Continuous integration works as follows: Each pull request (as well as a new
    commit to an existing pull request) triggers an automated system to run the full
    test suite on the updated code. Specifically, we have configured [Travis CI](https://travis-ci.org)
    and [`coveralls`](https://coveralls.io) to be automatically triggered whenever
    a commit is made to a pull request or the `upstream` master. These systems run
    the full test suite using different versions of our dependencies (e.g., Python
    2.7 and 3.4) every time a new commit is made to a repository or a pull is requested.
    Travis CI checks that all the tests pass, while `coveralls` generates a test coverage
    report so that we can monitor what parts of our code are checked by a test and
    which are not. This system checks whether any of our automated tests fail as well
    as tracks the percentage of our code that is covered by our automated tests. This
    means that when you review a pull request, you can immediately see whether the
    proposed changes break any tests and whether the new code decreases the overall
    test coverage.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 持续集成工作如下：每个拉取请求（以及对现有拉取请求的新提交）都会触发自动化系统对更新后的代码运行完整的测试套件。具体而言，我们已经配置了 [Travis
    CI](https://travis-ci.org) 和 [`coveralls`](https://coveralls.io) ，它们会在每次向拉取请求或
    `upstream` 主分支提交代码时自动触发。这些系统会使用我们依赖项的不同版本（例如，Python 2.7 和 3.4）运行完整的测试套件。Travis
    CI 检查所有测试是否通过，而 `coveralls` 生成测试覆盖报告，以便我们可以监视哪些部分的代码被测试覆盖，哪些部分没有。该系统检查我们的自动化测试是否有任何失败，并跟踪我们的代码被自动化测试覆盖的百分比。这意味着当您审查拉取请求时，您可以立即看到建议的更改是否破坏了任何测试，以及新代码是否减少了整体测试覆盖率。
- en: Documentation
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文档
- en: We use Sphinx as our documentation system and have extensive developer documentation
    and the foundation for high-quality user documentation. Sphinx is the standard
    documentation system for Python and is used by the core scientific Python packages.
    We use Python docstrings and follow the [NumPy docstring standard](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt)
    to document all the modules and functions in `permute`. Using Sphinx and some
    NumPy extensions, we have a system for autogenerating the project documentation
    (as HTML or PDF) using the docstrings as well as stand-alone text written in a
    light-weight markdown-like language, called [reStructuredText](http://docutils.sourceforge.net/rst.html).
    This system enables us to easily embed references, figures, code that is auto-run
    during documentation generation, as well as mathematics using LaTeX.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Sphinx作为我们的文档系统，并拥有广泛的开发者文档和高质量用户文档的基础。Sphinx是Python的标准文档系统，被核心科学Python包使用。我们使用Python文档字符串并遵循[NumPy文档字符串标准](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt)来记录`permute`中的所有模块和函数。使用Sphinx和一些NumPy扩展，我们有一个系统可以根据文档字符串以及用一种类似于轻量级Markdown的语言编写的独立文本（称为[reStructuredText](http://docutils.sourceforge.net/rst.html)）自动生成项目文档（作为HTML或PDF）。这个系统使我们能够轻松地嵌入引用、图表、在文档生成过程中自动运行的代码，以及使用LaTeX进行数学运算。
- en: Release management
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 发布管理
- en: Our development workflow ensures that the official `upstream` repository is
    always stable and ready for use. This means anyone can install our official upstream
    master at any time and start using it. We also make official releases available
    as source tarballs and as Python built-packages uploaded to the Python Package
    Index, or PyPI, with release announcements posted to our mailing list.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的开发工作流程确保官方的`upstream`存储库始终稳定并且可以随时使用。这意味着任何人都可以随时安装我们的官方上游主分支并开始使用。我们还将官方发布作为源代码tarball和作为上传到Python软件包索引（PyPI）的Python构建包提供，并在我们的邮件列表上发布发布公告。
- en: 'By making official releases whenever we reach an important stage of an applied
    project, we are able to easily recover the exact version of our analysis at a
    later date. To install the exact version of `permute` used in this case study,
    type the following command from a shell prompt (assuming you have Python and a
    recent version of `pip`):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在应用项目的重要阶段达到时制作官方发布，我们能够在以后轻松恢复我们分析的确切版本。要安装在本案例研究中使用的`permute`的确切版本，请从shell提示符（假设您已经安装了Python和最新版本的`pip`）中键入以下命令：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Questions
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: What does "reproducibility" mean to you?
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对你来说，“可重现性”是什么意思？
- en: 'In this case study, *reproducibility* means:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，*可重现性*意味着：
- en: '*Computational reproducibility and transparency.* We have documented (and scripted)
    nearly every step of the analysis—from cleaning to coding to code execution—and
    made the code and documentation publicly available.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算可重现性和透明度*。我们已经记录（并编写脚本）了分析的几乎每一个步骤——从清洁到编码再到代码执行——并公开提供了代码和文档。'
- en: '*Scientific reproducibility and transparency.* We documented much of the discussion
    leading to our decisions to take each step in the analysis. We made the data publicly
    available in an open format, with an adequate data dictionary.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*科学可重现性和透明度*。我们记录了导致我们决定采取分析中的每一步的讨论。我们以开放格式公开提供了数据，并附有充分的数据字典。'
- en: '*Computational correctness and evidence.* We tested our code thoroughly and
    in an automated fashion, to have justifiable confidence that the code does what
    it was intended to do. We made those tests publicly available, so that others
    can see how we validated our computations.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算正确性和证据*。我们充分测试了我们的代码，并以自动化方式进行了测试，以确保代码执行了预期的操作。我们公开提供了这些测试，以便他人可以看到我们如何验证我们的计算。'
- en: '*Statistical reproducibility.* We invested time to understand the fundamental
    problem and the results of our analysis so that we do *not* draw conclusions that
    are not justified by the data, the manner in which it was acquired, and our domain
    understanding. We avoided “p-hacking” and other potentially misleading selective
    reporting, and made all our analyses publicly available.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计可重现性*。我们投入时间来理解基本问题和我们分析结果，以便我们不会得出数据、获取方式以及我们对领域的理解所不支持的结论。我们避免了“p-hacking”和其他可能具有误导性的选择性报告，并公开提供了我们所有的分析。'
- en: By keeping all code, text, and data in a public version-controlled repository,
    we have made our well-documented analysis available for anyone to examine, check,
    modify, or reuse. We published the data used in our study -- both the original
    anonymized version as well as our cleaned version including the commands necessary
    to produce the cleaned version from the anonymized one. In addition to making
    what we did transparent to anyone who is interested, working in this way means
    that when errors are found we can identify how and when those errors were introduced.
    We have written tests for almost all our code, which means we have a high level
    of confidence that as we change our code we will catch any errors we might have
    introduced, and can correct them quickly and easily. And since we have automated
    the process of running our analysis, if errors are identified and corrected, it
    is easy to rerun the entire analysis from start to finish.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将所有代码、文本和数据保存在一个公共的版本控制存储库中，我们已经使我们的完全记录的分析可供任何人检查、检查、修改或重复使用。我们发布了我们研究中使用的数据--原始匿名版本以及我们清理过的版本，包括从匿名版本产生清理版本所需的命令。除了使我们所做的工作对任何感兴趣的人都透明外，以这种方式工作意味着当发现错误时，我们可以确定这些错误是如何引入的以及何时引入的。我们几乎为我们所有的代码编写了测试，这意味着我们对我们的代码进行更改时，我们有很高的信心能够捕获我们可能引入的任何错误，并且可以快速轻松地更正它们。由于我们已经自动化了运行分析的过程，因此如果发现并纠正了错误，从头到尾重新运行整个分析就变得很容易。
- en: 'If you have standard tools on your computer and network access, you can run
    our complete analysis of the cleaned data by typing the following three commands
    from a Unix shell prompt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的计算机上有标准工具并且可以访问网络，则可以通过在Unix shell提示符下键入以下三个命令来运行我们对清理数据的完整分析：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first command creates a directory `nsgk` in your current working directory
    with a copy of the project repository (i.e., a directory with our code, data,
    and text along with the provenance of these documents). This directory contains
    this document as well as everything needed to run our analysis. Inside `nsgk/nsgk`
    there is a `Makefile`, our analysis script `analysis.py`, and the output `results.csv`
    of that script.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令在当前工作目录中创建一个名为`nsgk`的目录，其中包含项目存储库的副本（即包含我们的代码、数据和文本以及这些文档的来源的目录）。此目录包含此文档以及运行我们分析所需的所有内容。在`nsgk/nsgk`内部有一个`Makefile`，我们的分析脚本`analysis.py`以及该脚本的输出`results.csv`。
- en: 'When you enter the command `make`, the following commands will be run:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当你输入命令`make`时，将运行以下命令：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first command creates a new virtual environment (`venv`) for Python 2.7\.
    Using this new virtual environment (`venv`) the subsequent commands respectively
    upgrade the Python package manager (`pip`) to the most recent version, install
    the necessary Python package dependencies (`numpy 1.11.0`, `scipy 0.17.0`, and
    `permute 0.1a2`), and run the analysis script `analysis.py`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令创建一个新的Python 2.7虚拟环境（`venv`）。使用这个新的虚拟环境（`venv`），随后的命令分别将Python软件包管理器（`pip`）升级到最新版本，安装所需的Python软件包依赖项（`numpy
    1.11.0`，`scipy 0.17.0`和`permute 0.1a2`），并运行分析脚本`analysis.py`。
- en: References
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: Buckheit, J. B., & Donoho, D. L. (1995). Wavelab and reproducible research.
    In A. Antoniadis & G. Oppenheim (Eds.), *Wavelets and statistics*. Springer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Buckheit, J. B., & Donoho, D. L. (1995). Wavelab and reproducible research.
    In A. Antoniadis & G. Oppenheim (Eds.), *Wavelets and statistics*. Springer.
- en: Millman, K. J., & Pérez, F. (2014). Developing open-source scientific practice.
    In V. Stodden, F. Leisch, & R. D. Peng (Eds.), *Implementing reproducible research*
    (pp. 149–183). Chapman; Hall/CRC.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Millman, K. J., & Pérez, F. (2014). Developing open-source scientific practice.
    In V. Stodden, F. Leisch, & R. D. Peng (Eds.), *Implementing reproducible research*
    (pp. 149–183). Chapman; Hall/CRC.
- en: 'Pesarin, F., & Salmaso, L. (2010). *Permutation tests for complex data: Theory,
    applications and software.* John Wiley & Sons.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Pesarin, F., & Salmaso, L. (2010). *Permutation tests for complex  Theory, applications
    and software.* John Wiley & Sons.
