["```\n# http://blog.aloni.org/posts/backprop-with-tensorflow/\n# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\n# WIP\nimport tensorflow as tf\n\ntf.set_random_seed(777)  # reproducibility\n\n# tf Graph Input\nx_data = [[1.],\n          [2.],\n          [3.]]\ny_data = [[1.],\n          [2.],\n          [3.]]\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 1])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\n# Set wrong model weights\nW = tf.Variable(tf.truncated_normal([1, 1]))\nb = tf.Variable(5.)\n\n# Forward prop\nhypothesis = tf.matmul(X, W) + b\n\n# diff\nassert hypothesis.shape.as_list() == Y.shape.as_list()\ndiff = (hypothesis - Y)\n\n# Back prop (chain rule)\nd_l1 = diff\nd_b = d_l1\nd_w = tf.matmul(tf.transpose(X), d_l1)\n\nprint(X, W, d_l1, d_w)\n\n# Updating network using gradients\nlearning_rate = 0.1\nstep = [\n    tf.assign(W, W - learning_rate * d_w),\n    tf.assign(b, b - learning_rate * tf.reduce_mean(d_b)),\n]\n\n# 7\\. Running and testing the training process\nRMSE = tf.reduce_mean(tf.square((Y - hypothesis)))\n\nsess = tf.InteractiveSession()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(1000):\n    print(i, sess.run([step, RMSE], feed_dict={X: x_data, Y: y_data}))\n\nprint(sess.run(hypothesis, feed_dict={X: x_data})) \n```"]