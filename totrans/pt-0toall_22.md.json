["```\n # Original code from\n# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n\n#import matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader\nfrom text_loader import TextDataset\nimport seq2seq_models as sm\nfrom seq2seq_models import cuda_variable, str2tensor, EOS_token, SOS_token\n\nN_LAYERS = 1\nBATCH_SIZE = 1\nN_EPOCH = 100\nN_CHARS = 128  # ASCII\nHIDDEN_SIZE = N_CHARS\n\n# Simple test to show how our train works\ndef test():\n    encoder_test = sm.EncoderRNN(10, 10, 2)\n    decoder_test = sm.AttnDecoderRNN(10, 10, 2)\n\n    if torch.cuda.is_available():\n        encoder_test.cuda()\n        decoder_test.cuda()\n\n    encoder_hidden = encoder_test.init_hidden()\n    word_input = cuda_variable(torch.LongTensor([1, 2, 3]))\n    encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n    print(encoder_outputs.size())\n\n    word_target = cuda_variable(torch.LongTensor([1, 2, 3]))\n    decoder_attns = torch.zeros(1, 3, 3)\n    decoder_hidden = encoder_hidden\n\n    for c in range(len(word_target)):\n        decoder_output, decoder_hidden, decoder_attn = \\\n            decoder_test(word_target[c],\n                         decoder_hidden, encoder_outputs)\n        print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n        decoder_attns[0, c] = decoder_attn.squeeze(0).cpu().data\n\n# Train for a given src and target\n# To demonstrate seq2seq, We don't handle batch in the code,\n# and our encoder runs this one step at a time\n# It's extremely slow, and please do not use in practice.\n# We need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\ndef train(src, target):\n    loss = 0\n\n    src_var = str2tensor(src)\n    target_var = str2tensor(target, eos=True)  # Add the EOS token\n\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    for c in range(len(target_var)):\n        # First, we feed SOS. Others, we use teacher forcing.\n        token = target_var[c - 1] if c else str2tensor(SOS_token)\n        output, hidden, attention = decoder(token, hidden, encoder_outputs)\n        loss += criterion(output, target_var[c])\n\n    encoder.zero_grad()\n    decoder.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.data[0] / len(target_var)\n\n# Translate the given input\ndef translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n    input_var = str2tensor(enc_input)\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    predicted = ''\n    dec_input = str2tensor(SOS_token)\n    attentions = []\n    for c in range(predict_len):\n        output, hidden, attention = decoder(dec_input, hidden, encoder_outputs)\n        # Sample from the network as a multi nominal distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        attentions.append(attention.view(-1).data.cpu().numpy().tolist())\n\n        # Stop at the EOS\n        if top_i is EOS_token:\n            break\n\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n\n        dec_input = str2tensor(predicted_char)\n\n    return predicted, attentions\n\nif __name__ == '__main__':\n    encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n    decoder = sm.AttnDecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n\n    if torch.cuda.is_available():\n        decoder.cuda()\n        encoder.cuda()\n    print(encoder, decoder)\n    # test()\n\n    params = list(encoder.parameters()) + list(decoder.parameters())\n    optimizer = torch.optim.Adam(params, lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(dataset=TextDataset(),\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              num_workers=2)\n\n    print(\"Training for %d epochs...\" % N_EPOCH)\n    for epoch in range(1, N_EPOCH + 1):\n        # Get srcs and targets from data loader\n        for i, (srcs, targets) in enumerate(train_loader):\n            train_loss = train(srcs[0], targets[0])\n\n            if i % 1000 is 0:\n                print('[(%d/%d %d%%) %.4f]' %\n                      (epoch, N_EPOCH, i * len(srcs) * 100 / len(train_loader), train_loss))\n                output, _ = translate(srcs[0])\n                print(srcs[0], output, '\\n')\n\n                output, attentions = translate()\n                print('thisissungkim.iloveyou.', output, '\\n')\n\n        # plt.matshow(attentions)\n        # plt.show()\n        # print(attentions) \n```"]