- en: Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, an ideal "intelligent" machine is a flexible rational agent
    that perceives its environment and takes actions that maximize its chance of success
    at some goal.
  prefs: []
  type: TYPE_NORMAL
- en: What means rational
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here we will try to explain what means acting rational:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximally achieve pre-defined goals. (Expected utility)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goals expressed in terms of utility (non dimensional scalar value that represent
    "happiness")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be rational means maximize your expected utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Have a rational (for us intelligent) decision, is only related to the quality
    of the decision(utility), not the process that lead to that decision, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: You gave the right answer because you brute-force all possible outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You gave the right answer because you used a fancy state of the art method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically you don't care about the method just the decision.
  prefs: []
  type: TYPE_NORMAL
- en: As the AI field evolve what was considered intelligent, is not considered anymore
    after someone discover how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: A system is rational if he does the right thing with the information available.
  prefs: []
  type: TYPE_NORMAL
- en: Why we only care to be rational
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Basically we don't understand how our brain work, even with the current advances
    on neuro-science, and the advances on computation power. We don't know how the
    brain works and take decisions.
  prefs: []
  type: TYPE_NORMAL
- en: The only cool thing that we know about the brain, is that to do good decisions,
    **we need memory and simulation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Central AI problems:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reasoning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Knowledge
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Planning: Make prediction about their actions, and choose the one that'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perception
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a system (ie: software program) which observes the world through sensors
    and acts upon an environment using actuators. It directs it''s activity towards
    achieving goals. Intelligent agents may also learn or use knowledge to achieve
    their goals.'
  prefs: []
  type: TYPE_NORMAL
- en: Type of agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a lot of agents types, but here we're going to separate them in 2
    classes
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflex Agents: Don''t care about future effects of it''s actions (Just use
    a if-table)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Planning Agents: Simulate actions consequences before committing to them, using
    a model of the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both reflex and planning agents can be rational, again we just care about the
    result action, if the action maximize it's expected utility, then it is rational.
  prefs: []
  type: TYPE_NORMAL
- en: we need to identify which type of agent is needed to have a rational behavior.
  prefs: []
  type: TYPE_NORMAL
- en: A reflex or planning agent can be sub-optimal, but normally planning is a good
    idea to follow.
  prefs: []
  type: TYPE_NORMAL
- en: On this book we're going to see a lot of tools used to address planning. For
    instance searching is a kind of tool for planning.
  prefs: []
  type: TYPE_NORMAL
- en: Search problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A search problem finds a solution which is a sequence of actions (a plan) that
    transform the start state to the goal state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Types of search:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Uninformed Search: Keep searching everywhere until a solution is found'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Informed Search: Has kind of "information" saying if we''re close or not to
    the solution (Heuristic)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A search problem consist on the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A State space: Has the states needed to do planning'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A successor function: For any state x, return a set of states reachable from
    x with one action'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A start state and goal test: Gives initial point and how to check when planning
    is over'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example our objective is to travel from Arad, to Bucharest
  prefs: []
  type: TYPE_NORMAL
- en: '![](RomaniaMap.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Above you have the world state, we don't need so many details, we only need
    the cities, how the connect and the distances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](travel_romania.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On this search problem we detect the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'State space: The cities (The only variable pertinent to this search problem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start state: Arad'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Successor Function: Go to adjacent city with cost as distance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the map above as a graph, it contains nodes, that don't repeat and
    how they connect along with the costs of it's connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](StateSpaceGraph_vs_Tree.PNG)'
  prefs: []
  type: TYPE_IMG
- en: 'On way to do planning is convert the state space graph to a search Tree, then
    use some algorithms that search for a goal state on the tree. Here we observe
    the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: The start state will be the tree root node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node children represent the possible outcomes for each state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem is that both Search Trees, or State space graphs can be to big to
    fit inside the computer, for instance the following state space graph has a infinite
    search tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](infinite_tree.PNG)'
  prefs: []
  type: TYPE_IMG
- en: What to do on those cases, basically you don't keep on memory all the possible
    solutions of the tree or graph, you navigate the tree for a finite amount of depth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Expanding_Tree.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance, look to the state graph of the Romania, we start on Arad (Start
    state)
  prefs: []
  type: TYPE_NORMAL
- en: Arad has 3 possible child nodes, Sibiu, Timisoara and Zerind
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We choose the leftmost child node Sibiu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we choose the leftmost child node Arad, which is bad, so we try Fagaras,
    then Oradea, etc...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem is that if one of the tree branches is infinite, or is to big and
    has no solution we will keep looking on it's branch until the end.
  prefs: []
  type: TYPE_NORMAL
- en: At the point that we choose Sibiu on the first step, we need to keep the other
    possible nodes (Timisoara, and Zerind) this is called the tree fringe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important ideas to keep in mind on tree search:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all tree search is a mechanism used for planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning means that you have a model of the world, if the model is bad your
    solution will also be bad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fringe: Or cache of other possible solutions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to explore the current branch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Branching Factor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Branching factor is the number children on each node on a tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](branching_factor.png)'
  prefs: []
  type: TYPE_IMG
- en: Old problems like tic-tac-toe or other simple problems can be solved with a
    search tree or some sort of optimized tree algorithm. But games like chess or
    go has a huge branching factor, so you could not process them in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: On the animation bellow we compare the chess and go branching factor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](BranchingFactor.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will explore more about trees.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tree Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we will learn about some ways to do tree search. Just to remember
    from the introduction tree search is one of the mechanisms to do planning. Planning
    means that the agent will simulate possible actions on a model of the word, and
    choose the one that will maximize it's utility.
  prefs: []
  type: TYPE_NORMAL
- en: '![](types_tree_search.png)'
  prefs: []
  type: TYPE_IMG
- en: On this chapter we will learn the following techniques to tree searching
  prefs: []
  type: TYPE_NORMAL
- en: Depth first search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breadth-First search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uniform Cost search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A-star search A*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned before we cannot hold the whole tree on memory, so what we do is
    to expand the tree only when you needed it and you keep track of the other options
    that you did not explored yet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Tree_Search_Algo.png)'
  prefs: []
  type: TYPE_IMG
- en: To those parts that are still on memory but not expanded yet we call fringe.
  prefs: []
  type: TYPE_NORMAL
- en: '![](TreeAndFringe.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Depth first search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Markov Decision process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Markov Decision process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision process(MDP) is a framework used to help to make decisions on
    a stochastic environment. Our goal is to find a policy, which is a map that gives
    us all optimal actions on each state on our environment.
  prefs: []
  type: TYPE_NORMAL
- en: MDP is somehow more powerful than simple planning, because your policy will
    allow you to do optimal actions even if something went wrong along the way. Simple
    planning just follow the plan after you find the best strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](policy.png)'
  prefs: []
  type: TYPE_IMG
- en: What is a State
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider state as a summary (then called state-space) of all information needed
    to determine what happens next. There are 2 types of state space:'
  prefs: []
  type: TYPE_NORMAL
- en: 'World-State: Normally huge, and not available to the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agent-State: Smaller, have all variables needed to make a decision related
    to the agent expected utility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markovian Property
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically you don't need past states to do a optimal decision, all you need
    is the current state ![](15b18eec.png). This is because you could encode on your
    current state everything you need from the past to do a good decision. Still history
    matters...
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify our universe imagine the grid world, here your agent objective
    is to arrive on the green block, and avoid the red block. Your available actions
    are: ![](f1150405.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Enviroment_MDP.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The problem is that we don''t live on a perfect deterministic world, so our
    actions could have have different outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](perfect_vs_markov.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance when we choose the up action we have 80% probability of actually
    going up, and 10% of going left or right. Also if you choose to go left or right
    you have 80% chance of going left and 10% going up or down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the most important parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'States: A set of possible states ![](3e20d922.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model: ![](1764bdbb.png) Probability to go to state ![](503136bb.png) when
    you do the action ![](3238ebd4.png) while you were on state ![](15b18eec.png),
    is also called transition model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action: ![](27bfe991.png), things that you can do on a particular state ![](15b18eec.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward: ![](e4b7b91.png), scalar value that you get for been on a state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policy: ![](b324d96.png), our goal, is a map that tells the optimal action
    for every state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimal policy: ![](8199e226.png), is a policy that maximize your expected
    reward ![](e4b7b91.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In reinforcement learning we're going to learn a optimal policy by trial and
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we will learn the basics for Reinforcement learning (Rl). Basically
    an RL agent differently of solving a MDP where a graph is given, does not know
    anything about the environment, it learns what to do by exploring the environment.
    It uses actions, and receive states and rewards. You can only change your environment
    through actions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the big difficulties of Rl is that some actions take time to create a
    reward, and learning this dynamics can be challenging. Also the reward received
    by the environment is not related to the last action, but some action on the past.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Agents take actions in an environment and receive states and rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goal is to find a policy ![](91190153.png) that maximize it's utility function
    ![](2165b7f1.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspired by research on psychology and animal learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](RL_1.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Here we don't know which actions will produce rewards, also we don't know when
    an action will produce rewards, some times you do an action that will take time
    to produce rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Basically all is learned with interactions with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement learning components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent: Our robot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environment: The game, or where the agent lives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of states ![](c93c5d9f.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policy: Map between state to actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward Function ![](bb0b4258.png): Gives immediate reward for each state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Function: Gives the total amount of reward the agent can expect from
    a particular state to all possible states from that state. With the value function
    you can find a policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model ![](d38145c9.png) (Optional): Used to do planning, instead of simple
    trial-and-error approach common to Reinforcement learning. Here ![](503136bb.png)
    means the possible state after we do an action ![](3238ebd4.png) on the state
    ![](15b18eec.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our minds we will still think that there is a Markov decision process (MDP),
    which have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](RL_2.PNG)'
  prefs: []
  type: TYPE_IMG
- en: We're looking for a policy ![](4b9d31fb.png), which means a map that give us
    optimal actions for every state
  prefs: []
  type: TYPE_NORMAL
- en: The only problem is that we don't have now explicitly ![](d38145c9.png) or ![](bb0b4258.png),
    so we don't know which states are good or what the actions do. The only way to
    learn those things is to try them out and learn from our samples.
  prefs: []
  type: TYPE_NORMAL
- en: On Reinforcement learning we know that we can move fast or slow (Actions) and
    if we're cool, warm or overheated (states). But we don't know what our actions
    do in terms of how they change states.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RL_3.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Offline (MDPs) vs Online (RL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Another difference is that while a normal MDP planning agent, find the optimal
    solution, by means of searching and simulation (Planning). A Rl agent learns from
    trial and error, so it will do something bad before knowing that it should not
    do. Also to learn that something is real bad or good, the agent will repeat that
    a lot of times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](OnlineVsOffline.PNG)'
  prefs: []
  type: TYPE_IMG
- en: How it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We're going to learn an approximation of what the actions do and what rewards
    we get by experience. For instance we could randomly do actions
  prefs: []
  type: TYPE_NORMAL
- en: Late Reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We force the MDP to have good rewards as soon as possible by giving some discount
    ![](c9b9fa39.png) reward over time. Basically you modulate how in a rush your
    agent by giving more negative values to ![](e4b7b91.png) over time.
  prefs: []
  type: TYPE_NORMAL
- en: Also you can change the behavior of your agent by giving the amount of time
    that your agent have.
  prefs: []
  type: TYPE_NORMAL
- en: '![](negative_reward.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploration and Exploitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems of Reinforcement learning is the exploration vs exploitation
    dilemma.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploitation: Make the best decision with the knowledge that we already know
    (ex: Choose the action with biggest Q-value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploration: Gather more information by doing different (stochastic) actions
    from known states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Restaurant'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploitation: Go to favorite restaurant, when you are hungry (gives known reward)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploration: Try new restaurant (Could give more reward)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One technique to keep always exploring a bit is the usage of ![](f62cc344.png)
    exploration where before we take an action we add some random factor.
  prefs: []
  type: TYPE_NORMAL
- en: Q_Learning_Simple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q_Learning_Simple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q_Learning is a model free reinforcement learning technique. Here we are interested
    on finding through experiences with the environment the action-value function
    Q. When the Q function is found we can achieve optimal policy just by selecting
    the action that gives the biggest expected utility(reward).
  prefs: []
  type: TYPE_NORMAL
- en: Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a robot that need to learn how to leave a house with the best path
    possible, on this example we have a house with 5 rooms, and one "exit" room.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Q_LearningSimpleExample.png)'
  prefs: []
  type: TYPE_IMG
- en: Above we show the house, plant and also a graph representing it. On this graph
    all rooms are nodes, and the arrows the actions that can be taken on each node.
    The arrow values, are the immediate rewards that the agent receive by taking some
    action on a specific room. We choose our reinforcement learning environment to
    give 0 reward for all rooms that are not the exit room. On our target room we
    give a 100 reward.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize
  prefs: []
  type: TYPE_NORMAL
- en: 'Actions: 0,1,2,3,4,5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'States: 0,1,2,3,4,5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewards:0,100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal state: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can represent all this mechanics on a reward table.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RewardTable.gif)'
  prefs: []
  type: TYPE_IMG
- en: On this table the rows represent the rooms, and the columns the actions. The
    values on this matrix represent the rewards, the value (-1) indicate that some
    specific action is not available.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example looking the row 4 on this matrix gives the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Available actions: Go to room 0,3,5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Possible rewards from room 4: 0 (room 4 to 0),0 (room 4 to 3),100 (room 4 to
    5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole point of Q learning is that the matrix R is available only to the
    environment, the agent need to learn R by himself through experience.
  prefs: []
  type: TYPE_NORMAL
- en: What the agent will have is a Q matrix that encodes, the state,action,rewards,
    but is initialized with zero, and through experience becomes like the matrix R.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Q_matrix.gif)'
  prefs: []
  type: TYPE_IMG
- en: As seen on previous chapters, after we have found the Q matrix, we have an optimum
    policy, and we're done. Basically we just need to use the Q table to choose the
    action that gives best expected reward. You can also imagine the Q table as the
    memory of what the agent learned so far through it's experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Just as a quick reminder let's describe the steps on the Q-Learning algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q matrix with zeros
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a random initial state
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each episode (set of actions that starts on the initial state and ends on
    the goal state)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While state is not goal state
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Select a random possible action for the current state
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using this possible action consider going to this next state
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Get maximum Q value for this next state (All actions from this next state)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](1935fbad.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
- en: 'After we have a good Q table we just need to follow it:'
  prefs: []
  type: TYPE_NORMAL
- en: Set current state = initial state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From current state, find the action with the highest Q value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set current state = next state(state from action chosen on 2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat Steps 2 and 3 until current state = goal state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-Learning on manual
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Let's exercise what we learned so far by doing some episodes by hand. Consider
    ![](68af480d.png) and our initial node(state) to be "room 1"
  prefs: []
  type: TYPE_NORMAL
- en: As we don't have any prior experience we start our Q matrix with zeros
  prefs: []
  type: TYPE_NORMAL
- en: '![](Q_matrix.gif)'
  prefs: []
  type: TYPE_IMG
- en: Now take a look on our reward table (Part of the environment)
  prefs: []
  type: TYPE_NORMAL
- en: '![](R_possible_states_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Episode 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we start from state "room 1" (second row) there are only the actions 3(reward
    0) or 5( reward 100) do be done, imagine that we choose randomly the action 5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](R_possible_states_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On this new (next state 5) there are 3 possible actions: 1 , 4 or 5, with their
    rewards 0,0,100\. Basically is all positive values from row "5", and we''re just
    interested on the one with biggest value. We need to select the biggest Q value
    with those possible actions by selecting Q(5,1), Q(5,4), Q(5,5), then using a
    "max" function. But remember that at this state the Q table is still filled with
    zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](fc0a4ccc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the new state is 5 and this state is the goal state, we finish our episode.
    Now at the end of this episode the Q matrix will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Q_end_episode1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Episode 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On this new state we randomly selected the state "room 3", by looking the R
    matrix we have 3 possible actions on this state, also now by chance we chose the
    action 1
  prefs: []
  type: TYPE_NORMAL
- en: '![](R_possible_states_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By selecting the action "1" as our next state will have now the following possible
    actions
  prefs: []
  type: TYPE_NORMAL
- en: '![](R_possible_states_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s update our Q table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](31713139.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](New_Q_episode_2.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'As our new current state 1 is not the goal state, we continue the process.
    Now by chance from the possible actions of state 1, we choose the action 5\. From
    the action 5 we have the possible actions: 1,4,5 [Q(5,1), Q(5,4), Q(5,5)] unfortunately
    we did not computed yet this values and our Q matrix remain unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: Episode 100000
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After a lot of episodes our Q matrix can be considered to have convergence,
    on this case Q will be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Q_conv_not_normlize.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'if we divide all of the nonzero elements by it''s greatest value (on this case
    500) we normalize the Q table (Optional):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Converged_Q.gif)'
  prefs: []
  type: TYPE_IMG
- en: What to do with converged Q table.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now with the good Q table, imagine that we start from the state "room 2". If
    we keep choosing the action that gives maximum Q value, we're going from state
    2 to 3, then from 3 to 1, then from 1 to 5, and keeps at 5\. In other words we
    choose the actions [2,3,1,5].
  prefs: []
  type: TYPE_NORMAL
- en: Just one point to pay attention. On state 3 we have the options to choose action
    1 or 4, because both have the same max value, we choose the action 1\. But the
    other action would also give the same cumulative reward. [0+0+100]
  prefs: []
  type: TYPE_NORMAL
- en: '![](Using_Q_policy.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](BestPolicy_from_Q.png)'
  prefs: []
  type: TYPE_IMG
- en: Working out this example in Matlab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we're ready to mix those things on the computer, we're going to develop
    2 functions, one for representing our agent, and the other for the enviroment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Diagram_Simple_RL.png)'
  prefs: []
  type: TYPE_IMG
- en: Enviroment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We start by modeling the environment. Which is the part that receives an action
    from the agent and give as feedback, a immediate reward and state information.
    This state is actually the state of the environment after some action is made.
    Just to remember, if our system is markovian all information necessary for choosing
    the best future action, is encoded on this state.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the environment has the matrix R and all models needed to completely
    implement the universe. For example the environment could be a game, our real
    world (Grid World) etc...
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now on the agent side, we must train to gather experience from the environment.
    After this we use our learned Q table to act optimally, which means just follow
    the Q table looking for the actions that gives maximum expected reward. As you
    may expect the agent interface with the external world through the "simple_RL_enviroment"
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Deep Q Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Q Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](DQNBreakoutBlocks.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the previous chapter we learned about the "old school" Q learning, we used
    matrices to represent our Q tables. This somehow implies that you at least know
    how many states (rows) you have on your environment, the problem is that sometimes
    this is not true.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Using_Q_policy.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On this Q table we can say the the expectation of wining the game by taking
    the action 1 at state 3 is 80\. Bigger Q values means that we expect to win.
  prefs: []
  type: TYPE_NORMAL
- en: Also we learned that reinforcement learning is about learning how to behave
    on some environment where our only feedback is some sparse and time delayed "labels"
    called rewards. They are time delayed because there are cases where the environment
    will only tell if your action was good or bad some time after you actually moved.
  prefs: []
  type: TYPE_NORMAL
- en: Some Formalization before continue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Episode:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Considering our environment "Markovian" which means that our state encode everything
    needed to take a decision. We define an episode (game) as finite set of states,
    actions and rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '![](88061f2f.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance here "![](dd6aae3a.png)" means the reward that we take by taking
    the action ![](271a23cb.png) on state ![](b923a530.png). An episode always finish
    on an end state (Game over).
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Policy is defined as a "map" from states to actions, is the reinforcement learning
    objective to find the optimal policy. An optimal policy can be derived from a
    Q function.
  prefs: []
  type: TYPE_NORMAL
- en: Return or Future Total Reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We define return as the sum of all immediate rewards on an episode. Which means
    the sum of rewards until the episode finish.
  prefs: []
  type: TYPE_NORMAL
- en: '![](62432496.png)'
  prefs: []
  type: TYPE_IMG
- en: Future Discounted Reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To give some sort of flexibility we add to our total reward a parameter called
    gamma(![](c9b9fa39.png)). If gamma=0 all future rewards will be discarded, if
    gamma=1 all future rewards will be considered.
  prefs: []
  type: TYPE_NORMAL
- en: '![](75996783.png)'
  prefs: []
  type: TYPE_IMG
- en: Q function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We define the function Q(s,a) as maximum expected "Future Discounted Reward"
    that we can get if we take an action "a" on state "s", and continue optimally
    from that point until the end of the episode. When we say "continue optimally"
    means that we continue choosing actions from the policy derived from Q.
  prefs: []
  type: TYPE_NORMAL
- en: '![](589f6cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bellow we show how to derive a policy from the Q function, basically we want
    the action that gives the biggest Q value on state "s":'
  prefs: []
  type: TYPE_NORMAL
- en: '![](62ffd5be.png)'
  prefs: []
  type: TYPE_IMG
- en: The function "![](b2461618.png)" will search for the action "a" that maximizes
    Q(s,a) You can think that the Q function is "the best possible score at the end
    of the game after performing action a in state s". So if you have the Q function
    you have everything needed to win all games!.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we choose an action that "ALWAYS" maximize the "Discounted Future Reward",
    you are acting greedy. This means that you are not exploring and you could miss
    some better actions. This is called exploration-exploitation problem. To solve
    this we use an algorithm called ![](f62cc344.png), where a small probability ![](9fe1930b.png)
    will choose a completely random action from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](eGreedyAlgo.png)'
  prefs: []
  type: TYPE_IMG
- en: How to get the Q function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Basically we get the Q function by experience, using an iterative process called
    bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](3870fb60.png)'
  prefs: []
  type: TYPE_IMG
- en: In terms of algorithm. ![](Q_Learning_Algo.png) On the beginning the estimated
    Q(s,a) will be wrong but with time, the experiences with the environment, will
    give a true "r" and this reward will slowly shape our estimation to the true Q(s,a).
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Q learning is about using deep learning techniques to represent the
    Q table. Is also a kind of recipe to use Q learning on games.
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing on this recipe is to get our input, as we may imagine we take
    information directly form the screen. To add some notion of time we actually get
    4 consecutive screens.
  prefs: []
  type: TYPE_NORMAL
- en: Get the images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rescale to 84x84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert to 8 bit grayscale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have 84x84x256x4 "stuff" as our environment state, this means 7 million
    states. To find structure on this data we will need a convolution neural network!.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Convolution Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will give those 84x84x4 tensor to an CNN, this model will have one output
    for each actions which will represent a Q-value for each possible action. So for
    each forward propagation we will have all possible Q values for a particular state
    encoded on the screen. It's valid to point out that this is not a classification
    problem but a regression, our Q-value output is a scalar number not a class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DQN_Struct_Model.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the Atari paper this CNN had the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeepmindCNN.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that there is not pooling layer, this is done because want to retain
    the spatial information.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As out CNN is a regression model our loss function will be a "squared error
    loss"
  prefs: []
  type: TYPE_NORMAL
- en: '![](7edc9fb9.png)'
  prefs: []
  type: TYPE_IMG
- en: How to get the Deep Q function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now to iterate and find the real Q value using deep learning we must follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Do a forward pass for the current state "![](15b18eec.png)" (screens) to get
    all possible Q values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do a forward pass on the new state ![](9e821f3.png) and find the action ![](52cda3d8.png)
    (Action with biggest Q value)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set Q-value target for action to r + γmax a’ Q(s’, a’) (use the max calculated
    in step 2). For all other actions, set the Q-value target to the same as originally
    returned from step 1, making the error 0 for those outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use back-propagation and mini-batches stochastic gradient descent to update
    the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Problems with this approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unfortunately, we need to solve some problems with this approach. But before
    talking about the possible solutions let's check which are the problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploration-Exploitation issue: This is easy, we just add some random action
    along the way. (just use ![](5dcbc848.png))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Local-Minima: During the training we''re going to have a lot of screens that
    are high correlated, this may guide your network to learn just a replay of the
    episode. To solve this we need somehow to shuffle the input mini-batch with some
    other playing data. (Of course of the same game but at different time)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience Replay
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned we need to break the similarities of continuous frames during our
    update step. To do this we will store all game experiences during the episode
    inside a "replay memory" then during training we will take random mini-batches
    of this memory. Also you can add some human-experience by adding on the replay
    memory some human episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Complete Recipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](DeepQAlgorithm.png)'
  prefs: []
  type: TYPE_IMG
- en: A complete tensorflow example can be found [here](https://github.com/asrivat1/DeepLearningVideoGames).
  prefs: []
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](conv_agent.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On this chapter we will learn the effects of merging Deep Neural Networks with
    Reinforcement learning. If you follow AI news you may heard about some stuff that
    AI is not capable to do without any specific programming:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to play atari from raw image pixels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to beat Go champions (Huge branching factor)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robots learning how to walk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All those achievements fall on the Reinforcement Learning umbrella, more specific
    Deep Reinforcement Learning. Basically all those achievements arrived not due
    to new algorithms, but due to more Data and more powerful resources (GPUs, FPGAs,
    ASICs). Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Atari: Old Q-Learning algorithm but with a CNN as function aproximator (Since
    1988 people talk about standard RL with function approximators)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AlphaGo: Policy gradients that use Monte Carlo Tree Search (MCTS), which is
    pretty standard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nowadays Policy Gradients it's the favorite choice for attacking Reinforcement
    learning(RL) problems. Previously it was DQN (Deep Q learning). One advantage
    of Policy Gradients is because it can be learned end-to-end.
  prefs: []
  type: TYPE_NORMAL
- en: Policy Gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start our study with policy gradient using the pong game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](pong.gif)'
  prefs: []
  type: TYPE_IMG
- en: Here we have to possible actions (UP/DOWN) and our objective is make the ball
    pass the opponent paddle. Our inputs are going to be a 80x80x1 image pre-processed
    from a 210x160x3 image. Our game enviroment (openAI gym) will give a reward of
    +1 if you win the opponent, -1 if you lose or 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeepReinforcementLearning.png)'
  prefs: []
  type: TYPE_IMG
- en: Here each node is a particular game state, and each edge is a possible transition,
    also each edge can gives a reward. Our goal is to find a policy (map from states
    to actions) that will give us the best action to do on each state. The whole point
    is that we don't know this graph, otherwise the whole thing would be just a reflex
    agent, and also sometimes we cannot fit all this on memory.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this pong example we do some operations on the original 210x160x3 (RGB) image
    to a simpler grayscale 80x80\. This is done to simplify the neural network size.
  prefs: []
  type: TYPE_NORMAL
- en: Before
  prefs: []
  type: TYPE_NORMAL
- en: '![](pong_I_before.png)'
  prefs: []
  type: TYPE_IMG
- en: After
  prefs: []
  type: TYPE_NORMAL
- en: '![](pong_I_after.png)'
  prefs: []
  type: TYPE_IMG
- en: Also to give some notion of time to the network a difference between the current
    image and the previous one is calculated. (On the DQN paper it was used a convolution
    neural network with 6 image samples)
  prefs: []
  type: TYPE_NORMAL
- en: Policy Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea is to use a machine learning model that will learn a good policy
    from playing the game, and receiving rewards. So adding the machine learning part.
    We''re going to define a policy network (ex: 2 layer neural network). This simple
    neural network will receive the entire image and output the probability of going
    up.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](policy_newtork.png)'
  prefs: []
  type: TYPE_IMG
- en: After the probability of going up is calculated we sample an action (UP/DOWN)
    from a uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](policy_gradient_graph.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy Gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'One difference between supervised learning and reinforcement learning is that
    we don''t have the correct labels to calculate the error to then back-propagate.
    Here is where the policy gradient idea comes. Imagine the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](policy_gradient_backprop.png)'
  prefs: []
  type: TYPE_IMG
- en: 'During the training our policy network gave a "UP" probability of 30% (0.3),
    therefor our "DOWN" probability will be 100-30=70% (0.7). During our action sampling
    from this distribution the "DOWN" action was selected. Now what we do is choose
    a unit gradient "1" and wait for the possible rewards (0,+1,-1). This will modulate
    our gradient to 3 possible values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero: Our weights will remain the same'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Positive: Make the network more likely to repeat this action on the future'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Negative: Make the network less likely to repeat this action on the future'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So this is the magic of policy gradient, we choose a unit gradient and modulate
    with our rewards. After a lot of good/bad actions been selected and properly rewarded
    the policy network map a "rational/optimum" policy.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we show how to initialize our policy network as a 2 layer neural network
    with the first layer having 200 neurons and the second output layer 1 neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '![](initializa_policy_network.png)'
  prefs: []
  type: TYPE_IMG
- en: On the code above, D is our input difference image after pre-processing and
    H is the number of neurons on the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we have the python code for this network forward propagation (policy_forward),
    on this neural network we didn't use bias, but you could.
  prefs: []
  type: TYPE_NORMAL
- en: '![](policy_network_python.png)'
  prefs: []
  type: TYPE_IMG
- en: From the code above we have 2 set of weights W1,W2. Where W1 or the weights
    of the hidden layer can detect states from the images (Ball is above/bellow paddle),
    and W2 can decide what to do (action up/down) on those states. So the only problem
    now is to find W1 and W2 that lead to expert (rational) policy.
  prefs: []
  type: TYPE_NORMAL
- en: Actually we do a forward propagation to get the score for the "UP" action given
    an input image "x". Then from this "score" we "toss a biased coin" to actually
    do our atari joystick action (UP-2, Down-5).
  prefs: []
  type: TYPE_NORMAL
- en: By doing this we hope that some times we're going to do a good action and if
    we do this a lot of times, our policy network will learn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](action_probability.png)'
  prefs: []
  type: TYPE_IMG
- en: Training the policy network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: First we will initialize randomly W1,W2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we will play 20 games (one eposide).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep track of all games and their result (win/loose)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After a configured number of episodes we update our policy network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming that each game has 200 frames, and each episode has 20 games, we have
    to make 4000 decisions (up/down) per episode. Suppose that we run 100 games 12
    we win (+1) and 88 we loose(-1). We need to take all the 12x200=2400 decisions
    and do a positive(+1) update. (This will encourage do do those actions in the
    future for the same detected states). Now all the 88x200=17600 decisions that
    make us loose the game we do a negative(-1) update.
  prefs: []
  type: TYPE_NORMAL
- en: '![](episodes.png)'
  prefs: []
  type: TYPE_IMG
- en: The network will now become slightly more likely to repeat actions that worked,
    and slightly less likely to repeat actions that didn't work.
  prefs: []
  type: TYPE_NORMAL
- en: Some words about Policy Gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The policy gradient, are the current (2016) state of the art, but is still very
    far from human reasoning. For example, you don't need to crash your car hundred
    of times to start avoiding it. Policy gradient need a lot of samples to start
    to internalize correct actions, and it must have constant rewards of it. You can
    imagine that you are doing a kind of "brute-force" search where on the beginning
    we jitter some actions, and accidentally we do a good one. Also this good actions
    should be repeated hundred of times before the policy network start to repeat
    good actions. That's why the agent had to train for 3 days before start to play
    really good.
  prefs: []
  type: TYPE_NORMAL
- en: 'When policy gradient will shine:'
  prefs: []
  type: TYPE_NORMAL
- en: When the problem does not rely on very long-term planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it receive frequent reward signals for good/bad actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also if the action space start to be to complex (a lot of actions commands)
    there are other variants, like "deterministic policy gradients". This variant
    uses another network called "critic" that learns the score function.
  prefs: []
  type: TYPE_NORMAL
