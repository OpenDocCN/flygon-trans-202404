["```\n import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Cross entropy example\nimport numpy as np\n# One hot\n# 0: 1 0 0\n# 1: 0 1 0\n# 2: 0 0 1\nY = np.array([1, 0, 0])\n\nY_pred1 = np.array([0.7, 0.2, 0.1])\nY_pred2 = np.array([0.1, 0.3, 0.6])\nprint(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\nprint(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))\n\n# Softmax + CrossEntropy (logSoftmax + NLLLoss)\nloss = nn.CrossEntropyLoss()\n\n# target is of size nBatch\n# each element in target has to have 0 <= value < nClasses (0-2)\n# Input is class, not one-hot\nY = Variable(torch.LongTensor([0]), requires_grad=False)\n\n# input is of size nBatch x nClasses = 1 x 4\n# Y_pred are logits (not softmax)\nY_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\nY_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n\nl1 = loss(Y_pred1, Y)\nl2 = loss(Y_pred2, Y)\n\nprint(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)\n\nprint(\"Y_pred1=\", torch.max(Y_pred1.data, 1)[1])\nprint(\"Y_pred2=\", torch.max(Y_pred2.data, 1)[1])\n\n# target is of size nBatch\n# each element in target has to have 0 <= value < nClasses (0-2)\n# Input is class, not one-hot\nY = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n\n# input is of size nBatch x nClasses = 2 x 4\n# Y_pred are logits (not softmax)\nY_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n                                 [1.1, 0.1, 0.2],\n                                 [0.2, 2.1, 0.1]]))\n\nY_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n                                 [0.2, 0.3, 0.5],\n                                 [0.2, 0.2, 0.5]]))\n\nl1 = loss(Y_pred1, Y)\nl2 = loss(Y_pred2, Y)\n\nprint(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data) \n```"]