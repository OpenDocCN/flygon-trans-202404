["```\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.examples.tutorials.mnist import input_data\n%matplotlib inline\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) \n```", "```\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz \n```", "```\nmnist.train.images.shape \n```", "```\n(55000, 784) \n```", "```\nclass Model:\n    \"\"\"Network Model Class\n\n    Note that this class has only the constructor.\n    The actual model is defined inside the constructor.\n\n    Attributes\n    ----------\n    X : tf.float32\n        This is a tensorflow placeholder for MNIST images\n        Expected shape is [None, 784]\n\n    y : tf.float32\n        This is a tensorflow placeholder for MNIST labels (one hot encoded)\n        Expected shape is [None, 10]\n\n    mode : tf.bool\n        This is used for the batch normalization\n        It's `True` at training time and `False` at test time\n\n    loss : tf.float32\n        The loss function is a softmax cross entropy\n\n    train_op\n        This is simply the training op that minimizes the loss\n\n    accuracy : tf.float32\n        The accuracy operation\n\n    Examples\n    ----------\n    >>> model = Model(\"Batch Norm\", 32, 10)\n\n    \"\"\"\n    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n        \"\"\" Constructor\n\n        Parameters\n        --------\n        name : str\n            The name of this network\n            The entire network will be created under `tf.variable_scope(name)`\n\n        input_dim : int\n            The input dimension\n            In this example, 784\n\n        output_dim : int\n            The number of output labels\n            There are 10 labels\n\n        hidden_dims : list (default: [32, 32])\n            len(hidden_dims) = number of layers\n            each element is the number of hidden units\n\n        use_batchnorm : bool (default: True)\n            If true, it will create the batchnormalization layer\n\n        activation_fn : TF functions (default: tf.nn.relu)\n            Activation Function\n\n        optimizer : TF optimizer (default: tf.train.AdamOptimizer)\n            Optimizer Function\n\n        lr : float (default: 0.01)\n            Learning rate\n\n        \"\"\"\n        with tf.variable_scope(name):\n            # Placeholders are defined\n            self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n            self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n            self.mode = tf.placeholder(tf.bool, name='train_mode')            \n\n            # Loop over hidden layers\n            net = self.X\n            for i, h_dim in enumerate(hidden_dims):\n                with tf.variable_scope('layer{}'.format(i)):\n                    net = tf.layers.dense(net, h_dim)\n\n                    if use_batchnorm:\n                        net = tf.layers.batch_normalization(net, training=self.mode)\n\n                    net = activation_fn(net)\n\n            # Attach fully connected layers\n            net = tf.contrib.layers.flatten(net)\n            net = tf.layers.dense(net, output_dim)\n\n            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n            self.loss = tf.reduce_mean(self.loss, name='loss')    \n\n            # When using the batchnormalization layers,\n            # it is necessary to manually add the update operations\n            # because the moving averages are not included in the graph \n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n            with tf.control_dependencies(update_ops):                     \n                self.train_op = optimizer(lr).minimize(self.loss)\n\n            # Accuracy etc \n            softmax = tf.nn.softmax(net, name='softmax')\n            self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32)) \n```", "```\nclass Solver:\n    \"\"\"Solver class\n\n    This class will contain the model class and session\n\n    Attributes\n    ----------\n    model : Model class\n    sess : TF session\n\n    Methods\n    ----------\n    train(X, y)\n        Run the train_op and Returns the loss\n\n    evalulate(X, y, batch_size=None)\n        Returns \"Loss\" and \"Accuracy\"\n        If batch_size is given, it's computed using batch_size\n        because most GPU memories cannot handle the entire training data at once\n\n    Example\n    ----------\n    >>> sess = tf.InteractiveSession()\n    >>> model = Model(\"BatchNorm\", 32, 10)\n    >>> solver = Solver(sess, model)\n\n    # Train\n    >>> solver.train(X, y)\n\n    # Evaluate\n    >>> solver.evaluate(X, y)\n    \"\"\"\n    def __init__(self, sess, model):\n        self.model = model\n        self.sess = sess\n\n    def train(self, X, y):\n        feed = {\n            self.model.X: X,\n            self.model.y: y,\n            self.model.mode: True\n        }\n        train_op = self.model.train_op\n        loss = self.model.loss\n\n        return self.sess.run([train_op, loss], feed_dict=feed)\n\n    def evaluate(self, X, y, batch_size=None):\n        if batch_size:\n            N = X.shape[0]\n\n            total_loss = 0\n            total_acc = 0\n\n            for i in range(0, N, batch_size):\n                X_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n\n                feed = {\n                    self.model.X: X_batch,\n                    self.model.y: y_batch,\n                    self.model.mode: False\n                }\n\n                loss = self.model.loss\n                accuracy = self.model.accuracy\n\n                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n\n                total_loss += step_loss * X_batch.shape[0]\n                total_acc += step_acc * X_batch.shape[0]\n\n            total_loss /= N\n            total_acc /= N\n\n            return total_loss, total_acc\n\n        else:\n            feed = {\n                self.model.X: X,\n                self.model.y: y,\n                self.model.mode: False\n            }\n\n            loss = self.model.loss            \n            accuracy = self.model.accuracy\n\n            return self.sess.run([loss, accuracy], feed_dict=feed) \n```", "```\ninput_dim = 784\noutput_dim = 10\nN = 55000\n\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\n\n# We create two models: one with the batch norm and other without\nbn = Model('batchnorm', input_dim, output_dim, use_batchnorm=True)\nnn = Model('no_norm', input_dim, output_dim, use_batchnorm=False)\n\n# We create two solvers: to train both models at the same time for comparison\n# Usually we only need one solver class\nbn_solver = Solver(sess, bn)\nnn_solver = Solver(sess, nn) \n```", "```\nepoch_n = 10\nbatch_size = 32\n\n# Save Losses and Accuracies every epoch\n# We are going to plot them later\ntrain_losses = []\ntrain_accs = []\n\nvalid_losses = []\nvalid_accs = [] \n```", "```\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor epoch in range(epoch_n):\n    for _ in range(N//batch_size):\n        X_batch, y_batch = mnist.train.next_batch(batch_size)\n\n        _, bn_loss = bn_solver.train(X_batch, y_batch)\n        _, nn_loss = nn_solver.train(X_batch, y_batch)       \n\n    b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n    n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n\n    # Save train losses/acc\n    train_losses.append([b_loss, n_loss])\n    train_accs.append([b_acc, n_acc])\n    print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n\n    b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n    n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n\n    # Save valid losses/acc\n    valid_losses.append([b_loss, n_loss])\n    valid_accs.append([b_acc, n_acc])\n    print(f'[Epoch {epoch}-VALID] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n    print() \n```", "```\n[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.18456(94.19%) vs No Batchnorm Loss(Acc): 0.31917(91.01%)\n[Epoch 0-VALID] Batchnorm Loss(Acc): 0.19054(94.10%) vs No Batchnorm Loss(Acc): 0.31920(91.00%)\n\n[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.10349(96.78%) vs No Batchnorm Loss(Acc): 0.16142(95.34%)\n[Epoch 1-VALID] Batchnorm Loss(Acc): 0.11720(96.48%) vs No Batchnorm Loss(Acc): 0.18348(94.96%)\n\n[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.11239(96.43%) vs No Batchnorm Loss(Acc): 0.17737(94.79%)\n[Epoch 2-VALID] Batchnorm Loss(Acc): 0.12829(96.30%) vs No Batchnorm Loss(Acc): 0.20401(94.34%)\n\n[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.07526(97.69%) vs No Batchnorm Loss(Acc): 0.15240(95.65%)\n[Epoch 3-VALID] Batchnorm Loss(Acc): 0.09549(97.12%) vs No Batchnorm Loss(Acc): 0.20025(95.16%)\n\n[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.07339(97.68%) vs No Batchnorm Loss(Acc): 0.15641(95.53%)\n[Epoch 4-VALID] Batchnorm Loss(Acc): 0.10588(96.96%) vs No Batchnorm Loss(Acc): 0.19816(94.86%)\n\n[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.08164(97.38%) vs No Batchnorm Loss(Acc): 0.15969(95.67%)\n[Epoch 5-VALID] Batchnorm Loss(Acc): 0.11476(96.52%) vs No Batchnorm Loss(Acc): 0.22123(95.10%)\n\n[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.05879(98.10%) vs No Batchnorm Loss(Acc): 0.18191(94.92%)\n[Epoch 6-VALID] Batchnorm Loss(Acc): 0.09402(97.30%) vs No Batchnorm Loss(Acc): 0.25907(94.50%)\n\n[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.05014(98.38%) vs No Batchnorm Loss(Acc): 0.23831(93.59%)\n[Epoch 7-VALID] Batchnorm Loss(Acc): 0.08446(97.58%) vs No Batchnorm Loss(Acc): 0.28310(93.46%)\n\n[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.04956(98.41%) vs No Batchnorm Loss(Acc): 0.12616(96.48%)\n[Epoch 8-VALID] Batchnorm Loss(Acc): 0.08479(97.48%) vs No Batchnorm Loss(Acc): 0.18636(95.44%)\n\n[Epoch 9-TRAIN] Batchnorm Loss(Acc): 0.04351(98.61%) vs No Batchnorm Loss(Acc): 0.12277(96.54%)\n[Epoch 9-VALID] Batchnorm Loss(Acc): 0.08275(97.66%) vs No Batchnorm Loss(Acc): 0.19641(95.74%) \n```", "```\nbn_solver.evaluate(mnist.test.images, mnist.test.labels) \n```", "```\n[0.089340471, 0.97370011] \n```", "```\nnn_solver.evaluate(mnist.test.images, mnist.test.labels) \n```", "```\n[0.20733583, 0.95130014] \n```", "```\ndef plot_compare(loss_list: list, ylim=None, title=None) -> None:\n\n    bn = [i[0] for i in loss_list]\n    nn = [i[1] for i in loss_list]\n\n    plt.figure(figsize=(15, 10))\n    plt.plot(bn, label='With BN')\n    plt.plot(nn, label='Without BN')\n    if ylim:\n        plt.ylim(ylim)\n\n    if title:\n        plt.title(title)\n    plt.legend()\n    plt.grid('on')\n    plt.show() \n```", "```\nplot_compare(train_losses, title='Training Loss at Epoch') \n```", "```\nplot_compare(train_accs, [0, 1.0], title=\"Training Acc at Epoch\") \n```", "```\nplot_compare(valid_losses, title='Validation Loss at Epoch') \n```", "```\nplot_compare(valid_accs, [0, 1.], title='Validation Acc at Epoch') \n```"]