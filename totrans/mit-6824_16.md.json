["```\nSingle machine\n-------------------\n| Web app         |\n|                 | \n|                 |\n|                 |\n|       | DB | <-----> |Disk|\n------------------ \n```", "```\nWeb app -----\nWeb app      \\ -----> |DB| <-> Disk\n...\nWeb app \n```", "```\nWeb app             |DB1| <-> Disk  (users A-M)\nWeb app             |DB2| <-> Disk  (users N-T)\n...                 |DB3| <-> Disk  ...\nWeb app             ... \n```", "```\nWeb app -> |MC| --> |DB1| <-> Disk  (users A-M)\nWeb app    |MC|     |DB2| <-> Disk  (users N-T)\n...         ..      |DB3| <-> Disk  ...\nWeb app             ... \n```", "```\nRegions (data centers):\n    Master region (writable)\n   ----------------- \n  | Web1 Web2 ...   |\n  | MC1  MC2  ...   |\n  | DB1  DB2  ... <--- complete copy of all data\n   -----------------\n\n    Slave regions (read only)\n   ----------------- \n  | Web1 Web2 ...   |\n  | MC1  MC2  ...   |\n  | DB1  DB2  ... <--- complete copy of all data\n   ----------------- \n```", "```\nScaling Memcache at Facebook, by Nishtala et al, NSDI 2013\n\nwhy are we reading this paper?\n  it's an experience paper, not about new ideas/techniques\n  three ways to read it:\n    cautionary tale of problems from not taking consistency seriously\n    impressive story of super high capacity from mostly-off-the-shelf s/w\n    fundamental struggle between performance and consistency\n  we can argue with their design, but not their success\n\nhow do web sites scale up with growing load?\n  a typical story of evolution over time:\n  1\\. one machine, web server, application, DB\n     DB stores on disk, crash recovery, transactions, SQL\n     application queries DB, formats, HTML, &c\n     but the load grows, your PHP application takes too much CPU time\n  2\\. many web FEs, one shared DB\n     an easy change, since web server + app already separate from storage\n     FEs are stateless, all sharing (and concurrency control) via DB\n     but the load grows; add more FEs; soon single DB server is bottleneck\n  3\\. many web FEs, data sharded over cluster of DBs\n     partition data by key over the DBs\n       app looks at key (e.g. user), chooses the right DB\n     good DB parallelism if no data is super-popular\n     painful -- cross-shard transactions and queries probably don't work\n       hard to partition too finely\n     but DBs are slow, even for reads, why not cache read requests?\n  4\\. many web FEs, many caches for reads, many DBs for writes\n     cost-effective b/c read-heavy and memcached 10x faster than a DB\n       memcached just an in-memory hash table, very simple\n     complex b/c DB and memcacheds can get out of sync\n     (next bottleneck will be DB writes -- hard to solve)\n\nthe big facebook infrastructure picture\n  lots of users, friend lists, status, posts, likes, photos\n    fresh/consistent data apparently not critical\n    because humans are tolerant?\n  high load: billions of operations per second\n    that's 10,000x the throughput of one DB server\n  multiple data centers (at least west and east coast)\n  each data center -- \"region\":\n    \"real\" data sharded over MySQL DBs\n    memcached layer (mc)\n    web servers (clients of memcached)\n  each data center's DBs contain full replica\n  west coast is master, others are slaves via MySQL async log replication\n\nhow do FB apps use mc?\n  read:\n    v = get(k) (computes hash(k) to choose mc server)\n    if v is nil {\n      v = fetch from DB\n      set(k, v)\n    }\n  write:\n    v = new value\n    send k,v to DB\n    delete(k)\n  application determines relationship of mc to DB\n    mc doesn't know anything about DB\n  FB uses mc as a \"look-aside\" cache\n    real data is in the DB\n    cached value (if any) should be same as DB\n\nwhat does FB store in mc?\n  paper does not say\n  maybe userID -> name; userID -> friend list; postID -> text; URL -> likes\n  basically copies of data from DB\n\npaper lessons:\n  look-aside is much trickier than it looks -- consistency\n    paper is trying to integrate mutually-oblivious storage layers\n  cache is critical:\n    not really about reducing user-visible delay\n    mostly about surviving huge load!\n    cache misses and failures can create intolerable DB load\n  they can tolerate modest staleness: no freshness guarantee\n  stale data nevertheless a big headache\n    want to avoid unbounded staleness (e.g. missing a delete() entirely)\n    want read-your-own-writes\n    each performance fix brings a new source of staleness\n  huge \"fan-out\" => parallel fetch, in-cast congestion\n\nlet's talk about performance first\n  majority of paper is about avoiding stale data\n  but staleness only arose from performance design\n\nperformance comes from parallel get()s by many mc servers\n  driven by parallel processing of HTTP requests by many web servers\n  two basic parallel strategies for storage: partition vs replication\n\nwill partition or replication yield most mc throughput?\n  partition: server i, key k -> mc server hash(k)\n  replicate: server i, key k -> mc server hash(i)\n  partition is more memory efficient (one copy of each k/v)\n  partition works well if no key is very popular\n  partition forces each web server to talk to many mc servers (overhead)\n  replication works better if a few keys are very popular\n\nperformance and regions (Section 5)\n\nQ: what is the point of regions -- multiple complete replicas?\n   lower RTT to users (east coast, west coast)\n   parallel reads of popular data due to replication\n   (note DB replicas help only read performance, no write performance)\n   maybe hot replica for main site failure?\n\nQ: why not partition users over regions?\n   i.e. why not east-coast users' data in east-coast region, &c\n   social net -> not much locality\n   very different from e.g. e-mail\n\nQ: why OK performance despite all writes forced to go to the master region?\n   writes would need to be sent to all regions anyway -- replicas\n   users probably wait for round-trip to update DB in master region\n     only 100ms, not so bad\n   users do not wait for all effects of writes to finish\n     i.e. for all stale cached values to be deleted\n\nperformance within a region (Section 4)\n\nmultiple mc clusters *within* each region\n  cluster == complete set of mc cache servers\n    i.e. a replica, at least of cached data\n\nwhy multiple clusters per region?\n  why not add more and more mc servers to a single cluster?\n  1\\. adding mc servers to cluster doesn't help single popular keys\n     replicating (one copy per cluster) does help\n  2\\. more mcs in cluster -> each client req talks to more servers\n     and more in-cast congestion at requesting web servers\n     client requests fetch 20 to 500 keys! over many mc servers\n     MUST request them in parallel (otherwise total latency too large)\n     so all replies come back at the same time\n     network switches, NIC run out of buffers\n  3\\. hard to build network for single big cluster\n     uniform client/server access\n     so cross-section b/w must be large -- expensive\n     two clusters -> 1/2 the cross-section b/w\n\nbut -- replicating is a waste of RAM for less-popular items\n  \"regional pool\" shared by all clusters\n  unpopular objects (no need for many copies)\n  decided by *type* of object\n  frees RAM to replicate more popular objects\n\nbringing up new mc cluster was a serious performance problem\n  new cluster has 0% hit rate\n  if clients use it, will generate big spike in DB load\n    if ordinarily 1% miss rate, and (let's say) 2 clusters,\n      adding \"cold\" third cluster will causes misses for 33% of ops.\n    i.e. 30x spike in DB load!\n  thus the clients of new cluster first get() from existing cluster (4.3)\n    and set() into new cluster\n    basically lazy copy of existing cluster to new cluster\n  better 2x load on existing cluster than 30x load on DB\n\nimportant practical networking problems:\n  n^2 TCP connections is too much state\n    thus UDP for client get()s\n  UDP is not reliable or ordered\n    thus TCP for client set()s\n    and mcrouter to reduce n in n^2\n  small request per packet is not efficient (for TCP or UDP)\n    per-packet overhead (interrupt &c) is too high\n    thus mcrouter batches many requests into each packet\n\nmc server failure?\n  can't have DB servers handle the misses -- too much load\n  can't shift load to one other mc server -- too much\n  can't re-partition all data -- time consuming\n  Gutter -- pool of idle servers, clients only use after mc server fails\n\nThe Question:\n  why don't clients send invalidates to Gutter servers?\n  my guess: would double delete() traffic\n    and send too many delete()s to small gutter pool\n    since any key might be in the gutter pool\n\nthundering herd\n  one client updates DB and delete()s a key\n  lots of clients get() but miss\n    they all fetch from DB\n    they all set()\n  not good: needless DB load\n  mc gives just the first missing client a \"lease\"\n    lease = permission to refresh from DB\n    mc tells others \"try get() again in a few milliseconds\"\n  effect: only one client reads the DB and does set()\n    others re-try get() later and hopefully hit\n\nlet's talk about consistency now\n\nthe big truth\n  hard to get both consistency (== freshness) and performance\n  performance for reads = many copies\n  many copies = hard to keep them equal\n\nwhat is their consistency goal?\n  *not* read sees latest write\n    since not guaranteed across clusters\n  more like \"not more than a few seconds stale\"\n    i.e. eventual\n  *and* writers see their own writes\n    read-your-own-writes is a big driving force\n\nfirst, how are DB replicas kept consistent across regions?\n  one region is master\n  master DBs distribute log of updates to DBs in slave regions\n  slave DBs apply\n  slave DBs are complete replicas (not caches)\n  DB replication delay can be considerable (many seconds)\n\nhow do we feel about the consistency of the DB replication scheme?\n  good: eventual consistency, b/c single ordered write stream\n  bad: longish replication delay -> stale reads\n\nhow do they keep mc content consistent w/ DB content?\n  1\\. DBs send invalidates (delete()s) to all mc servers that might cache\n     + Do they wait for ACK? I'm guessing no.\n  2\\. writing client also invalidates mc in local cluster\n     for read-your-writes\n\nwhy did they have consistency problems in mc?\n  client code to copy DB to mc wasn't atomic:\n    1\\. writes: DB update ... mc delete()\n    2\\. read miss: DB read ... mc set()\n  so *concurrent* clients had races\n\nwhat were the races and fixes?\n\nRace 1: one client's cached get(k) replaces another client's updated k\n  k not in cache\n  C1: MC::get(k), misses\n  C1: v = read k from DB\n    C2: updates k in DB\n    C2: and DB calls MC::delete(k) -- k is not cached, so does nothing\n  C1: set(k, v)\n  now mc has stale data, delete(k) has already happened\n  will stay stale indefinitely, until key is next written\n  solved with leases -- C1 gets a lease, but C2's delete() invalidates lease,\n    so mc ignores C1's set\n    key still missing, so next reader will refresh it from DB\n\nRace 2: updating(k) in cold cluster, but getting stale k from warm cluster \n  during cold cluster warm-up\n  remember clients try get() in warm cluster, copy to cold cluster\n  k starts with value v1\n  C1: updates k to v2 in DB\n  C1: delete(k) -- in cold cluster\n  C2: get(k), miss -- in cold cluster\n  C2: v1 = get(k) from warm cluster, hits\n  C2: set(k, v1) into cold cluster\n  now mc has stale v1, but delete() has already happened\n    will stay stale indefinitely, until key is next written\n  solved with two-second hold-off, just used on cold clusters\n    after C1 delete(), cold ignores set()s for two seconds\n    by then, delete() will propagate via DB to warm cluster\n\nRace 3: writing to master region, but reading stale from local\n  k starts with value v1\n  C1: is in a slave region\n  C1: updates k=v2 in master DB\n  C1: delete(k) -- local region\n  C1: get(k), miss\n  C1: read local DB  -- sees v1, not v2!\n  later, v2 arrives from master DB\n  solved by \"remote mark\"\n    C1 delete() marks key \"remote\"\n    get()/miss yields \"remote\"\n      tells C1 to read from *master* region\n    \"remote\" cleared when new data arrives from master region\n\nQ: aren't all these problems caused by clients copying DB data to mc?\n   why not instead have DB send new values to mc, so clients only read mc?\n     then there would be no racing client updates &c, just ordered writes\nA:\n  1\\. DB doesn't generally know how to compute values for mc\n     generally client app code computes them from DB results,\n       i.e. mc content is often not simply a literal DB record\n  2\\. would increase read-your-own writes delay\n  3\\. DB doesn't know what's cached, would end up sending lots\n     of values for keys that aren't cached\n\nPNUTS does take this alternate approach of master-updates-all-copies\n\nFB/mc lessons for storage system designers?\n  cache is vital to throughput survival, not just a latency tweak\n  need flexible tools for controlling partition vs replication\n  need better ideas for integrating storage layers with consistency \n```"]