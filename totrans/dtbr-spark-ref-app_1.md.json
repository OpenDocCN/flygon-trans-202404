["```\n<dependency> <!-- Spark -->\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.10</artifactId>\n    <version>1.1.0</version>\n</dependency> \n```", "```\npublic class LogAnalyzer {\n  public static void main(String[] args) {\n    // Create a Spark Context.\n    SparkConf conf = new SparkConf().setAppName(\"Log Analyzer\");\n    JavaSparkContext sc = new JavaSparkContext(conf);\n\n    // Load the text file into Spark.\n    if (args.length == 0) {\n      System.out.println(\"Must specify an access logs file.\");\n      System.exit(-1);\n    }\n    String logFile = args[0];\n    JavaRDD<String> logLines = sc.textFile(logFile);\n\n    // TODO: Insert code here for processing logs.\n\n    sc.stop();\n  }\n} \n```", "```\n// Convert the text log lines to ApacheAccessLog objects and\n// cache them since multiple transformations and actions\n// will be called on the data.\nJavaRDD<ApacheAccessLog> accessLogs =\n    logLines.map(ApacheAccessLog::parseFromLogLine).cache(); \n```", "```\nprivate static Function2<Long, Long, Long> SUM_REDUCER = (a, b) -> a + b; \n```", "```\n// Calculate statistics based on the content size.\n// Note how the contentSizes are cached as well since multiple actions\n//   are called on that RDD.\nJavaRDD<Long> contentSizes =\n   accessLogs.map(ApacheAccessLog::getContentSize).cache();\nSystem.out.println(String.format(\"Content Size Avg: %s, Min: %s, Max: %s\",\n    contentSizes.reduce(SUM_REDUCER) / contentSizes.count(),\n    contentSizes.min(Comparator.naturalOrder()),\n    contentSizes.max(Comparator.naturalOrder()))); \n```", "```\n// Compute Response Code to Count.\nList<Tuple2<Integer, Long>> responseCodeToCount = accessLogs\n        .mapToPair(log -> new Tuple2<>(log.getResponseCode(), 1L))\n        .reduceByKey(SUM_REDUCER)\n        .take(100);\nSystem.out.println(String.format(\"Response code counts: %s\", responseCodeToCount)); \n```", "```\nList<String> ipAddresses =\n    accessLogs.mapToPair(log -> new Tuple2<>(log.getIpAddress(), 1L))\n        .reduceByKey(SUM_REDUCER)\n        .filter(tuple -> tuple._2() > 10)\n        .map(Tuple2::_1)\n        .take(100);\nSystem.out.println(String.format(\"IPAddresses > 10 times: %s\", ipAddresses)); \n```", "```\nprivate static class ValueComparator<K, V>\n   implements Comparator<Tuple2<K, V>>, Serializable {\n  private Comparator<V> comparator;\n\n  public ValueComparator(Comparator<V> comparator) {\n    this.comparator = comparator;\n  }\n\n  @Override\n  public int compare(Tuple2<K, V> o1, Tuple2<K, V> o2) {\n    return comparator.compare(o1._2(), o2._2());\n  }\n} \n```", "```\nList<Tuple2<String, Long>> topEndpoints = accessLogs\n    .mapToPair(log -> new Tuple2<>(log.getEndpoint(), 1L))\n    .reduceByKey(SUM_REDUCER)\n    .top(10, new ValueComparator<>(Comparator.<Long>naturalOrder()));\nSystem.out.println(\"Top Endpoints: \" + topEndpoints); \n```", "```\n<dependency> <!-- Spark SQL -->\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.10</artifactId>\n    <version>1.1.0</version>\n</dependency> \n```", "```\npublic class LogAnalyzerSQL {\n  public static void main(String[] args) {\n    // Create the spark context.\n    SparkConf conf = new SparkConf().setAppName(\"Log Analyzer SQL\");\n    JavaSparkContext sc = new JavaSparkContext(conf);\n    JavaSQLContext sqlContext = new JavaSQLContext(sc);\n\n    if (args.length == 0) {\n      System.out.println(\"Must specify an access logs file.\");\n      System.exit(-1);\n    }\n    String logFile = args[0];\n    JavaRDD<ApacheAccessLog> accessLogs = sc.textFile(logFile)\n        .map(ApacheAccessLog::parseFromLogLine);\n\n    // TODO: Insert code for computing log stats.\n\n    sc.stop();\n  }\n} \n```", "```\nJavaSchemaRDD schemaRDD = sqlContext.applySchema(accessLogs,\n    ApacheAccessLog.class);\nschemaRDD.registerTempTable(\"logs\");\nsqlContext.sqlContext().cacheTable(\"logs\"); \n```", "```\n// Calculate statistics based on the content size.\nTuple4<Long, Long, Long, Long> contentSizeStats =\n    sqlContext.sql(\"SELECT SUM(contentSize), COUNT(*), MIN(contentSize), MAX(contentSize) FROM logs\")\n        .map(row -> new Tuple4<>(row.getLong(0), row.getLong(1), row.getLong(2), row.getLong(3)))\n        .first();\nSystem.out.println(String.format(\"Content Size Avg: %s, Min: %s, Max: %s\",\n    contentSizeStats._1() / contentSizeStats._2(),\n    contentSizeStats._3(),\n    contentSizeStats._4()));\n\n// Compute Response Code to Count.\n// Note the use of \"LIMIT 1000\" since the number of responseCodes\n// can potentially be too large to fit in memory.\nList<Tuple2<Integer, Long>> responseCodeToCount = sqlContext\n    .sql(\"SELECT responseCode, COUNT(*) FROM logs GROUP BY responseCode LIMIT 1000\")\n    .mapToPair(row -> new Tuple2<>(row.getInt(0), row.getLong(1)));\nSystem.out.println(String.format(\"Response code counts: %s\", responseCodeToCount))\n    .collect();\n\n// Any IPAddress that has accessed the server more than 10 times.\nList<String> ipAddresses = sqlContext\n    .sql(\"SELECT ipAddress, COUNT(*) AS total FROM logs GROUP BY ipAddress HAVING total > 10 LIMIT 100\")\n    .map(row -> row.getString(0))\n    .collect();\nSystem.out.println(String.format(\"IPAddresses > 10 times: %s\", ipAddresses));\n\n// Top Endpoints.\nList<Tuple2<String, Long>> topEndpoints = sqlContext\n    .sql(\"SELECT endpoint, COUNT(*) AS total FROM logs GROUP BY endpoint ORDER BY total DESC LIMIT 10\")\n    .map(row -> new Tuple2<>(row.getString(0), row.getLong(1)))\n    .collect();\nSystem.out.println(String.format(\"Top Endpoints: %s\", topEndpoints)); \n```", "```\n<dependency> <!-- Spark Streaming -->\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-streaming_2.10</artifactId>\n    <version>1.1.0</version>\n</dependency> \n```", "```\n% tail -f [[YOUR_LOG_FILE]] | nc -lk 9999 \n```", "```\n% cat ../../data/apache.accesslog >> [[YOUR_LOG_FILE]] \n```", "```\npublic class LogAnalyzerStreamingSQL {\n  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"Log Analyzer Streaming SQL\");\n\n    // Note: Only one Spark Context is created from the conf, the rest\n    //       are created from the original Spark context.\n    JavaSparkContext sc = new JavaSparkContext(conf);\n    JavaStreamingContext jssc = new JavaStreamingContext(sc,\n        SLIDE_INTERVAL);  // This sets the update window to be every 10 seconds.\n    JavaSQLContext sqlContext = new JavaSQLContext(sc);\n\n    // TODO: Insert code here to process logs.\n\n    // Start the streaming server.\n    jssc.start();              // Start the computation\n    jssc.awaitTermination();   // Wait for the computation to terminate\n  }\n} \n```", "```\nJavaReceiverInputDStream<String> logDataDStream =\n    jssc.socketTextStream(\"localhost\", 9999); \n```", "```\nJavaDStream<ApacheAccessLog> accessLogDStream =\n    logDataDStream.map(ApacheAccessLog::parseFromLogLine).cache(); \n```", "```\nJavaDStream<ApacheAccessLog> windowDStream =\n    accessLogDStream.window(WINDOW_LENGTH, SLIDE_INTERVAL); \n```", "```\nwindowDStream.foreachRDD(accessLogs -> {\n  if (accessLogs.count() == 0) {\n    System.out.println(\"No access logs in this time interval\");\n    return null;\n  }\n\n  // Insert code verbatim from LogAnalyzer.java or LogAnalyzerSQL.java here.\n\n  // Calculate statistics based on the content size.\n  JavaRDD<Long> contentSizes =\n      accessLogs.map(ApacheAccessLog::getContentSize).cache();\n  System.out.println(String.format(\"Content Size Avg: %s, Min: %s, Max: %s\",\n      contentSizes.reduce(SUM_REDUCER) / contentSizes.count(),\n      contentSizes.min(Comparator.naturalOrder()),\n      contentSizes.max(Comparator.naturalOrder())));\n\n   //...Won't copy the rest here...\n} \n```", "```\npublic class LogAnalyzerStreamingTotal {\n  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"Log Analyzer Streaming Total\");\n    JavaSparkContext sc = new JavaSparkContext(conf);\n\n    JavaStreamingContext jssc = new JavaStreamingContext(sc,\n        new Duration(10000));  // This sets the update window to be every 10 seconds.\n\n    // Checkpointing must be enabled to use the updateStateByKey function.\n    jssc.checkpoint(\"/tmp/log-analyzer-streaming\");\n\n    // TODO: Insert code for computing log stats.\n\n    // Start the streaming server.\n    jssc.start();              // Start the computation\n    jssc.awaitTermination();   // Wait for the computation to terminate \n```", "```\n// These static variables stores the running content size values.\nprivate static final AtomicLong runningCount = new AtomicLong(0);\nprivate static final AtomicLong runningSum = new AtomicLong(0);\nprivate static final AtomicLong runningMin = new AtomicLong(Long.MAX_VALUE);\nprivate static final AtomicLong runningMax = new AtomicLong(Long.MIN_VALUE); \n```", "```\nJavaDStream<Long> contentSizeDStream =\n    accessLogDStream.map(ApacheAccessLog::getContentSize).cache();\ncontentSizeDStream.foreachRDD(rdd -> {\n  if (rdd.count() > 0) {\n    runningSum.getAndAdd(rdd.reduce(SUM_REDUCER));\n    runningCount.getAndAdd(rdd.count());\n    runningMin.set(Math.min(runningMin.get(), rdd.min(Comparator.naturalOrder())));\n    runningMax.set(Math.max(runningMax.get(), rdd.max(Comparator.naturalOrder())));\n    System.out.print(\"Content Size Avg: \" +  runningSum.get() / runningCount.get());\n    System.out.print(\", Min: \" + runningMin.get());\n    System.out.println(\", Max: \" + runningMax.get());\n  }\n  return null;\n}); \n```", "```\nprivate static Function2<List<Long>, Optional<Long>, Optional<Long>>\n   COMPUTE_RUNNING_SUM = (nums, current) -> {\n     long sum = current.or(0L);\n     for (long i : nums) {\n       sum += i;\n     }\n     return Optional.of(sum);\n   }; \n```", "```\n// Compute Response Code to Count.\n// Note the use of updateStateByKey.\nJavaPairDStream<Integer, Long> responseCodeCountDStream = accessLogDStream\n    .mapToPair(s -> new Tuple2<>(s.getResponseCode(), 1L))\n    .reduceByKey(SUM_REDUCER)\n    .updateStateByKey(COMPUTE_RUNNING_SUM);\nresponseCodeCountDStream.foreachRDD(rdd -> {\n  System.out.println(\"Response code counts: \" + rdd.take(100));\n  return null;\n});\n\n// A DStream of ipAddresses accessed > 10 times.\nJavaDStream<String> ipAddressesDStream = accessLogDStream\n    .mapToPair(s -> new Tuple2<>(s.getIpAddress(), 1L))\n    .reduceByKey(SUM_REDUCER)\n    .updateStateByKey(COMPUTE_RUNNING_SUM)\n    .filter(tuple -> tuple._2() > 10)\n    .map(Tuple2::_1);\nipAddressesDStream.foreachRDD(rdd -> {\n  List<String> ipAddresses = rdd.take(100);\n  System.out.println(\"All IPAddresses > 10 times: \" + ipAddresses);\n  return null;\n});\n\n// A DStream of endpoint to count.\nJavaPairDStream<String, Long> endpointCountsDStream = accessLogDStream\n    .mapToPair(s -> new Tuple2<>(s.getEndpoint(), 1L))\n    .reduceByKey(SUM_REDUCER)\n    .updateStateByKey(COMPUTE_RUNNING_SUM);\nendpointCountsDStream.foreachRDD(rdd -> {\n  List<Tuple2<String, Long>> topEndpoints =\n      rdd.takeOrdered(10, new ValueComparator<>(Comparator.<Long>naturalOrder()));\n  System.out.println(\"Top Endpoints: \" + topEndpoints);\n  return null;\n}); \n```", "```\npublic static JavaPairRDD<Integer, Long> responseCodeCount(\n   JavaRDD<ApacheAccessLog> accessLogRDD) {\n  return accessLogRDD\n     .mapToPair(s -> new Tuple2<>(s.getResponseCode(), 1L))\n     .reduceByKey(SUM_REDUCER);\n} \n```", "```\n// Compute Response Code to Count.\n// Notice the user transformToPair to produce the a DStream of\n// response code counts, and then updateStateByKey to accumulate\n// the response code counts for all of time.\nJavaPairDStream<Integer, Long> responseCodeCountDStream = accessLogDStream\n   .transformToPair(LogAnalyzerStreamingTotalRefactored::responseCodeCount);\nJavaPairDStream<Integer, Long> cumulativeResponseCodeCountDStream =\n   responseCodeCountDStream.updateStateByKey(COMPUTE_RUNNING_SUM);\ncumulativeResponseCodeCountDStream.foreachRDD(rdd -> {\n  System.out.println(\"Response code counts: \" + rdd.take(100));\n  return null;\n}); \n```", "```\n// A DStream of ipAddresses accessed > 10 times.\nJavaDStream<String> ipAddressesDStream = accessLogDStream\n   .transformToPair(LogAnalyzerStreamingTotalRefactored::ipAddressCount)\n   .updateStateByKey(COMPUTE_RUNNING_SUM)\n   .transform(LogAnalyzerStreamingTotalRefactored::filterIPAddress);\nipAddressesDStream.foreachRDD(rdd -> {\n  List<String> ipAddresses = rdd.take(100);\n  System.out.println(\"All IPAddresses > 10 times: \" + ipAddresses);\n  return null;\n}); \n```", "```\njssc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", YOUR_ACCESS_KEY)\njssc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", YOUR_SECRET_KEY) \n```", "```\n// This methods monitors a directory for new files\n// to read in for streaming.\nJavaDStream<String> logData = jssc.textFileStream(directory); \n```", "```\nLogStatistics logStatistics = logAnalyzerRDD.processRdd(accessLogs);\n\nString outputFile = args[1];\nWriter out = new BufferedWriter(\n    new OutputStreamWriter(new FileOutputStream(outputFile)));\n\nTuple4<Long, Long, Long, Long> contentSizeStats =\n    logStatistics.getContentSizeStats();\nout.write(String.format(\"Content Size Avg: %s, Min: %s, Max: %s\\n\",\n    contentSizeStats._1() / contentSizeStats._2(),\n    contentSizeStats._3(),\n    contentSizeStats._4()));\n\nList<Tuple2<Integer, Long>> responseCodeToCount =\n    logStatistics.getResponseCodeToCount();\nout.write(String.format(\"Response code counts: %s\\n\", responseCodeToCount));\n\nList<String> ipAddresses = logStatistics.getIpAddresses();\nout.write(String.format(\"IPAddresses > 10 times: %s\\n\", ipAddresses));\n\nList<Tuple2<String, Long>> topEndpoints = logStatistics.getTopEndpoints();\nout.write(String.format(\"Top Endpoints: %s\\n\", topEndpoints));\n\nout.close(); \n```", "```\npublic class LogAnalyzerExportRDD {\n  // Optionally modify this based as makes sense for your dataset.\n  public static final int NUM_PARTITIONS = 2;\n\n  public static void main(String[] args) throws IOException {\n    // Create the spark context.\n    SparkConf conf = new SparkConf().setAppName(\"Log Analyzer SQL\");\n    JavaSparkContext sc = new JavaSparkContext(conf);\n\n    if (args.length < 2) {\n      System.out.println(\"Must specify an access logs file and an output file.\");\n      System.exit(-1);\n    }\n    String inputFile = args[0];\n    String outputDirectory = args[1];\n    JavaRDD<ApacheAccessLog> accessLogs = sc.textFile(inputFile)\n        .map(ApacheAccessLog::parseFromLogLine)\n        .repartition(NUM_PARTITIONS); // Optionally, change this.\n\n    accessLogs.saveAsTextFile(outputDirectory);\n\n    sc.stop();\n  }\n} \n```"]