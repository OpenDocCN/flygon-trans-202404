["```\n # https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom text_loader import TextDataset\nimport seq2seq_models as sm\nfrom seq2seq_models import str2tensor, EOS_token, SOS_token\n\nHIDDEN_SIZE = 100\nN_LAYERS = 1\nBATCH_SIZE = 1\nN_EPOCH = 100\nN_CHARS = 128  # ASCII\n\n# Simple test to show how our network works\ndef test():\n    encoder_hidden = encoder.init_hidden()\n    word_input = str2tensor('hello')\n    encoder_outputs, encoder_hidden = encoder(word_input, encoder_hidden)\n    print(encoder_outputs)\n\n    decoder_hidden = encoder_hidden\n\n    word_target = str2tensor('pytorch')\n    for c in range(len(word_target)):\n        decoder_output, decoder_hidden = decoder(\n            word_target[c], decoder_hidden)\n        print(decoder_output.size(), decoder_hidden.size())\n\n# Train for a given src and target\n# To demonstrate seq2seq, We don't handle batch in the code,\n# and our encoder runs this one step at a time\n# It's extremely slow, and please do not use in practice.\n# We need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\ndef train(src, target):\n    src_var = str2tensor(src)\n    target_var = str2tensor(target, eos=True)  # Add the EOS token\n\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n\n    hidden = encoder_hidden\n    loss = 0\n\n    for c in range(len(target_var)):\n        # First, we feed SOS\n        # Others, we use teacher forcing\n        token = target_var[c - 1] if c else str2tensor(SOS_token)\n        output, hidden = decoder(token, hidden)\n        loss += criterion(output, target_var[c])\n\n    encoder.zero_grad()\n    decoder.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.data[0] / len(target_var)\n\n# Translate the given input\ndef translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n    input_var = str2tensor(enc_input)\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    predicted = ''\n    dec_input = str2tensor(SOS_token)\n    for c in range(predict_len):\n        output, hidden = decoder(dec_input, hidden)\n\n        # Sample from the network as a multi nominal distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n\n        # Stop at the EOS\n        if top_i is EOS_token:\n            break\n\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n\n        dec_input = str2tensor(predicted_char)\n\n    return enc_input, predicted\n\nencoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\ndecoder = sm.DecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n\nif torch.cuda.is_available():\n    decoder.cuda()\n    encoder.cuda()\nprint(encoder, decoder)\ntest()\n\nparams = list(encoder.parameters()) + list(decoder.parameters())\noptimizer = torch.optim.Adam(params, lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ntrain_loader = DataLoader(dataset=TextDataset(),\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=2)\n\nprint(\"Training for %d epochs...\" % N_EPOCH)\nfor epoch in range(1, N_EPOCH + 1):\n    # Get srcs and targets from data loader\n    for i, (srcs, targets) in enumerate(train_loader):\n        train_loss = train(srcs[0], targets[0])  # Batch is 1\n\n        if i % 100 is 0:\n            print('[(%d %d%%) %.4f]' %\n                  (epoch, epoch / N_EPOCH * 100, train_loss))\n            print(translate(srcs[0]), '\\n')\n            print(translate(), '\\n') \n```"]