- en: Log Analysis with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log Analysis with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project demonstrates how easy it is to do log analysis with Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Log analysis is an ideal use case for Spark. It's a very large, common data
    source and contains a rich set of information. Spark allows you to store your
    logs in files to disk cheaply, while still providing a quick and simple way to
    process them. We hope this project will show you how to use Apache Spark on your
    organization's production logs and fully harness the power of that data. Log data
    can be used for monitoring your servers, improving business and customer intelligence,
    building recommendation systems, preventing fraud, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: How to use this project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This project is broken up into sections with bite-sized examples for demonstrating
    new Spark functionality for log processing. This makes the examples easy to run
    and learn as they cover just one new topic at a time. At the end, we assemble
    some of these examples to form a sample log analysis application.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 1: Introduction to Apache Spark](index2.html)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Apache Spark library is introduced, as well as Spark SQL and Spark Streaming.
    By the end of this chapter, a reader will know how to call transformations and
    actions and work with RDDs and DStreams.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 2: Importing Data](index3.html)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section includes examples to illustrate how to get data into Spark and
    starts covering concepts of distributed computing. The examples are all suitable
    for datasets that are too large to be processed on one machine.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3: Exporting Data](index4.html)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section includes examples to illustrate how to get data out of Spark. Again,
    concepts of a distributed computing environment are reinforced, and the examples
    are suitable for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4: Logs Analyzer Application](index5.html)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section puts together some of the code in the other chapters to form a
    sample log analysis application.
  prefs: []
  type: TYPE_NORMAL
- en: More to come...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While that's all for now, there's definitely more to come over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 1: Introduction to Apache Spark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Section 1: Introduction to Apache Spark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we demonstrate how simple it is to analyze web logs using Apache
    Spark. We'll show how to load a Resilient Distributed Dataset (**RDD**) of access
    log lines and use Spark tranformations and actions to compute some statistics
    for web server monitoring. In the process, we'll introduce the Spark SQL and the
    Spark Streaming libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In this explanation, the code snippets are in [Java 8](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/java8).
    However, there is also sample code in [Java 6](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/java6),
    [Scala](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/scala),
    and [Python](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/python)
    included in this directory. In those folders are README's for instructions on
    how to build and run those examples, and the necessary build files with all the
    required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[First Log Analyzer in Spark](spark.html) - This is a first Spark standalone
    logs analysis application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Spark SQL](sql.html) - This example does the same thing as the above example,
    but uses SQL syntax instead of Spark transformations and actions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Spark Streaming](streaming.html) - This example covers how to calculate log
    statistics using the streaming library.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First Log Analyzer in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First Logs Analyzer in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before beginning this section, go through [Spark Quick Start](https://spark.apache.org/docs/latest/quick-start.html)
    and familiarize with the [Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)
    first.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section requires a dependency on the Spark Core library in the maven file
    - note update this dependency based on the version of Spark you have installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can begin, we need two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An Apache access log file**: If you have one, it''s more interesting to use
    real data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is trivial sample one provided at [data/apache.access.log](../data/apache.accesslog).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Or download a better example here: [http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A parser and model for the log file**: See [ApacheAccessLog.java](ApacheAccessLog.java).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The example code uses an Apache access log file since that's a well known and
    common log format. It would be easy to rewrite the parser for a different log
    format if you have data in another log format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following statistics will be computed:'
  prefs: []
  type: TYPE_NORMAL
- en: The average, min, and max content size of responses returned from the server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A count of response code's returned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All IPAddresses that have accessed this server more than N times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top endpoints requested by count.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's first walk through the code first before running the example at [LogAnalyzer.java](LogAnalyzer.java).
  prefs: []
  type: TYPE_NORMAL
- en: The main body of a simple Spark application is below. The first step is to bring
    up a Spark context. Then the Spark context can load data from a text file as an
    RDD, which it can then process. Finally, before exiting the function, the Spark
    context is stopped.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Given an RDD of log lines, use the `map` function to transform each line to
    an ApacheAccessLog object. The ApacheAccessLog RDD is cached in memory, since
    multiple transformations and actions will be called on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It's useful to define a sum reducer - this is a function that takes in two integers
    and returns their sum. This is used all over our example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's calculate the average, minimum, and maximum content size of the
    response returned. A `map` transformation extracts the content sizes, and then
    different actions (`reduce`, `count`, `min`, and `max`) are called to output various
    stats. Again, call `cache` on the context size RDD to avoid recalculating those
    values for each action called on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To compute the response code counts, we have to work with key-value pairs -
    by using `mapToPair` and `reduceByKey`. Notice that we call `take(100)` instead
    of `collect()` to gather the final output of the response code counts. Use extreme
    caution before calling `collect()` on an RDD since all that data will be sent
    to a single Spark driver and can cause the driver to run out of memory. Even in
    this case where there are only a limited number of response codes and it seems
    safe - if there are malformed lines in the Apache access log or a bug in the parser,
    there could be many invalid response codes to cause an.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To compute any IPAddress that has accessed this server more than 10 times, we
    call the `filter` tranformation and then `map` to retrieve only the IPAddress
    and discard the count. Again we use `take(100)` to retrieve the values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Last, let's calculate the top endpoints requested in this log file. We define
    an inner class, `ValueComparator` to help with that. This function tells us, given
    two tuples, which one is first in ordering. The key of the tuple is ignored, and
    ordering is based just on the values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can use the `ValueComparator` with the `top` action to compute the
    top endpoints accessed on this server according to how many times the endpoint
    was accessed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These code snippets are from [LogAnalyzer.java](LogAnalyzer.java). Now that
    we've walked through the code, try running that example. See the README for language
    specific instructions for building and running.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should go through the [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
    before beginning this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section requires an additioal dependency on Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For those of you who are familiar with SQL, the same statistics we calculated
    in the previous example can be done using Spark SQL rather than calling Spark
    transformations and actions directly. We walk through how to do that here.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to create a SQL Spark context. Note how we create one Spark Context,
    and then use that to instantiate different flavors of Spark contexts. You should
    not initialize multiple Spark contexts from the SparkConf in one process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need a way to register our logs data into a table. In Java, Spark
    SQL can infer the table schema on a standard Java POJO - with getters and setters
    as we''ve done with [ApacheAccessLog.java](ApacheAccessLog.java). (Note: if you
    are using a different language besides Java, there is a different way for Spark
    to infer the table schema. The examples in this directory work out of the box.
    Or you can also refer to the [Spark SQL Guide on Data Sources](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources)
    for more details.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to start running some SQL queries on our table. Here''s the
    code to compute the identical statistics in the previous section - it should look
    very familiar for those of you who know SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that the default SQL dialect does not allow using reserved keyworks as
    alias names. In other words, `SELECT COUNT(*) AS count` will cause errors, but
    `SELECT COUNT(*) AS the_count` runs fine. If you use the HiveQL parser though,
    then you should be able to use anything as an identifier.
  prefs: []
  type: TYPE_NORMAL
- en: Try running [LogAnalyzerSQL.java](LogAnalyzerSQL.java) now.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go through the [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
    before beginning this section. In particular, it covers the concept of DStreams.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section requires another dependency on the Spark Streaming library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The earlier examples demonstrates how to compute statistics on an existing log
    file - but not how to do realtime monitoring of logs. Spark Streaming enables
    that functionality.
  prefs: []
  type: TYPE_NORMAL
- en: To run the streaming examples, you will `tail` a log file into `netcat` to send
    to Spark. This is not the ideal way to get data into Spark in a production system,
    but is an easy workaround for a first Spark Streaming example. We will cover best
    practices for [how to import data for Spark Streaming in Chapter 2](streaming1.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a terminal window, just run this command on a logfile which you will append
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don''t have a live log file that is being updated on the fly, you can
    add lines manually with the included data file or another your own log file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When data is streamed into Spark, there are two common use cases covered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Windowed Calculations](windows.html) means that you only care about data received
    in the last N amount of time. When monitoring your web servers, perhaps you only
    care about what has happened in the last hour.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark Streaming conveniently splits the input data into the desired time windows
    for easy processing, using the `window` function of the streaming library.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forEachRDD` function allows you to access the RDD's created each time interval.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cumulative Calculations](total.html) means that you want to keep cumulative
    statistics, while streaming in new data to refresh those statistics. In that case,
    you need to maintain the state for those statistics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark Streaming library has some convenient functions for maintaining state
    to support this use case, `updateStateByKey`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reusing code from Batching](reuse.html) covers how to organize business logic
    code from the batch examples so that code can be reused in Spark Streaming.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark Streaming library has `transform` functions which allow you to apply
    arbitrary RDD-to-RDD functions, and thus to reuse code from the batch mode of
    Spark.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Windowed Calculations: window()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Windowed Calculations: window()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical use case for log analysis is monitoring a web server, in which case
    you may only be interested in what's happened for the last one hour of time and
    want those statistics to refresh every minute. One hour is the *window length*,
    while one minute is the *slide interval*. In this example, we use a window length
    of 30 seconds and a slide interval of 10 seconds as a comfortable choice for development.
  prefs: []
  type: TYPE_NORMAL
- en: The windows feature of Spark Streaming makes it very easy to compute stats for
    a window of time, using the `window` function.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to initalize the SparkConf and context objects - in particular
    a streaming context. Note how only one SparkContext is created from the conf and
    the streaming and sql contexts are created from those. Next, the main body should
    be written. Finally, the example calls `start()` on the streaming context, and
    `awaitTermination()`to keep the streaming context running and accepting streaming
    input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first step of the main body is to create a DStream from reading the socket.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, call the `map` transformation to convert the logDataDStream into a ApacheAccessLog
    DStream.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, call `window` on the accessLogDStream to create a windowed DStream. The
    `window` function nicely packages the input data that is being streamed into RDDs
    containing a window length of data, and creates a new RDD every SLIDE_INTERVAL
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Then call `foreachRDD` on the windowDStream. The function passed into `forEachRDD`
    is called on each new RDD in the windowDStream as the RDD is created, so every
    *slide_interval*. The RDD passed into the function contains all the input for
    the last *window_length* of time. Now that there is an RDD of ApacheAccessLogs,
    simply reuse code from either two batch examples (regular or SQL). In this example,
    the code was just copied and pasted, but you could refactor this code into one
    place nicely for reuse in your production code base - you can reuse all your batch
    processing code for streaming!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've walked through the code, run [LogAnalyzerStreaming.java](LogAnalyzerStreaming.java)
    and/or [LogAnalyzerStreamingSQL.java](LogAnalyzerStreamingSQL.java) now. Use the
    `cat` command as explained before to add data to the log file periodically once
    you have your program up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cumulative Calculations: updateStateByKey()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cumulative Calculations: updateStateByKey()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To keep track of the log statistics for all of time, state must be maintained
    between processing RDD's in a DStream.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain state for key-pair values, the data may be too big to fit in memory
    on one machine - Spark Streaming can maintain the state for you. To do that, call
    the `updateStateByKey` function of the Spark Streaming library.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in order to use `updateStateByKey`, checkpointing must be enabled on
    the streaming context. To do that, just call `checkpoint` on the streaming context
    with a directory to write the checkpoint data. Here is part of the main function
    of a streaming application that will save state for all of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To compute the content size statistics, simply use static variables to save
    the current running sum, count, min and max of the content sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To update those values, first call map on the AccessLogDStream to retrieve
    a contentSizeDStream. Then just update the values for the static variables by
    calling foreachRDD on the contentSizeDstream, and calling actions on the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For the other statistics, since they make use of key value pairs, static variables
    can't be used anymore. The amount of state that needs to be maintained is potentially
    too big to fit in memory. So for those stats, we'll make use of `updateStateByKey`
    so Spark streaming will maintain a value for every key in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: But before we can call `updateStateByKey`, we need to create a function to pass
    into it. `updateStateByKey` takes in a different reduce function. While our previous
    sum reducer just took in two values and output their sum, this reduce function
    takes in a current value and an iterator of values, and outputs one new value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can compute the keyed statistics for all of time with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Run [LogAnalyzerStreamingTotal.java](LogAnalyzerStreamingTotal.java) now for
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusing Code from Batching: transform()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reusing Code from Batching: transform()'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have noticed, while the functions you called on a DStream are named
    the same as those you called on an RDD in the batch example, they are not the
    same methods, and it may not be clear how to reuse the code from the batch examples.
    In this section, we refactor the code from the batch examples and show how to
    reuse it here.
  prefs: []
  type: TYPE_NORMAL
- en: DStreams have `transform` functions which allows you to call any arbitrary RDD
    to RDD functions to RDD's in the DStream. The `transform` functions are perfect
    for reusing any RDD to RDD functions that you may have written in batch code and
    want to port over to streaming. Let's look at some code to illustrate this point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have separated out a function, `responseCodeCount` from our batch
    example that can compute the response code count given the apache access logs
    RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The responseCodeCountDStream can be created by calling `transformToPair` with
    the `responseCodeCount` function to the accessLogDStream. Then, you can finish
    up by calling `updateStateByKey` to keep a running count of the response codes
    for all of time, and use `forEachRDD` to print the values out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to combine `transform` functions before and after an `updateStateByKey`
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Take a closer look at [LogAnalyzerStreamingTotalRefactored.java](LogAnalyzerStreamingTotalRefactored.java)
    now to see how that code has been refactored to reuse code from the batch example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 2: Importing Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Section 2: Importing Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section we covered how to get started with Spark for log analysis,
    but in those examples, data was just pulled in from a local file and the statistics
    were printed to standard out. In this chapter, we cover techniques for loading
    and exporting data that is suitable for a production system. In particular, the
    techniques must scale to handle large production volumes of logs.
  prefs: []
  type: TYPE_NORMAL
- en: To scale, Apache Spark is meant to be deployed on a cluster of machines. Read
    the [Spark Cluster Overview Guide](https://spark.apache.org/docs/latest/cluster-overview.html),
    so that you understand the different between the Spark driver vs. the executor
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: While you could continue running the examples in local mode, it is recommended
    that you set up a Spark cluster to run the remaining examples on and get practice
    working with the cluster - such as familiarizing yourself with the web interface
    of the cluster. You can run a small cluster on your local machine by following
    the instructions for [Spark Standalone Mode](https://spark.apache.org/docs/latest/spark-standalone.html).
    Optionally, if you have access to more machines - such as on AWS or your organization
    has its own datacenters, consult the [cluster overview guide](https://spark.apache.org/docs/latest/cluster-overview.html)
    to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you get a Spark cluster up:'
  prefs: []
  type: TYPE_NORMAL
- en: Use spark-submit to run your jobs rather than using the JVM parameter. Run one
    of the examples from the previous chapter to check your set up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poke around and familiarize with the web interfaces for Spark. It's at [http://localhost:8080](http://localhost:8080)
    if you set up a local cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways to import data into Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Batch Data Import](batch.html) - if you are loading a dataset all at once.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Streaming Data Import](streaming1.html) - if you wish to continuously stream
    data into Spark.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch Data Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers batch importing data into Apache Spark, such as seen in
    the non-streaming examples from Chapter 1\. Those examples load data from files
    all at once into one RDD, processes that RDD, the job completes, and the program
    exits. In a production system, you could set up a cron job to kick off a batch
    job each night to process the last day's worth of log files and then publish statistics
    for the last day.
  prefs: []
  type: TYPE_NORMAL
- en: '[Importing From Files](importing_from_files.html) covers caveats when importing
    data from files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Importing from Databases](importing_from_databases.html) links to examples
    of reading data from databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing from Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Importing from Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To support batch import of data on a Spark cluster, the data needs to be accessible
    by all machines on the cluster. Files that are only accessible on one worker machine
    and cannot be read by the others will cause failures.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a small dataset that can fit on one machine, you could manually
    copy your files onto all the nodes on your Spark cluster, perhaps using `rsync`
    to make that easier.
  prefs: []
  type: TYPE_NORMAL
- en: '**NFS** or some other network file system makes sure all your machines can
    access the same files without requiring you to copy the files around. But NFS
    isn''t fault tolerant to machine failures and if your dataset is too big to fit
    on one NFS volume - you''d have to store the data on multiple volumes and figure
    out which volume a particular file is on - which could get cumbersome.'
  prefs: []
  type: TYPE_NORMAL
- en: '**HDFS** and **S3** are great file systems for massive datasets - built to
    store a lot of data and give all the machines on the cluster access to those files,
    while still being fault tolerant. We give a few more tips on running Spark with
    these file systems since they are recommended.'
  prefs: []
  type: TYPE_NORMAL
- en: '[S3](s3.html) is an Amazon AWS solution for storing files in the cloud, easily
    accessible to anyone who signs up for an account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HDFS](hdfs.html) is a distributed file system that is part of Hadoop and can
    be installed on your own datacenters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The good news is that regardless of which of these file systems you choose,
    you can run the same code to read from them - these file systems are all "Hadoop
    compatible" file systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you should try running [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java)
    on any files on your file system of choice. There is nothing new in this code
    - it's just a refactor of the [First Log Analyzer from Chapter One](spark.html).
    Try passing in "*" or "?" for the textFile path, and Spark will read in all the
    files that match that pattern to create the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S3 is Amazon Web Services's solution for storing large files in the cloud. On
    a production system, you want your Amazon EC2 compute nodes on the same zone as
    your S3 files for speed as well as cost reasons. While S3 files can be read from
    other machines, it would take a long time and be expensive (Amazon S3 data transfer
    prices differ if you read data within AWS vs. to somewhere else on the internet).
  prefs: []
  type: TYPE_NORMAL
- en: See [running Spark on EC2](https://spark.apache.org/docs/latest/ec2-scripts.html)
    if you want to launch a Spark cluster on AWS - charges apply.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to run this example with a local Spark cluster on your machine
    rather than EC2 compute nodes to read the files in S3, use a small data input
    source!
  prefs: []
  type: TYPE_NORMAL
- en: Sign up for an [Amazon Web Services](https://aws.amazon.com/) Account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load example log files to s3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log into the [AWS console for S3](https://console.aws.amazon.com/s3/)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an S3 bucket.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload a couple of example log files to that bucket.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your files will be at the path: s3n://YOUR_BUCKET_NAME/YOUR_LOGFILE.log'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configure your security credentials for AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and [download your security credentials](https://console.aws.amazon.com/iam/home?#security_credential)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to
    the correct values on all machines on your cluster. These can also be set in your
    SparkContext object programmatically like this:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, run [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java) passing
    in the s3n path to your files.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HDFS is a file system that is meant for storing large data sets and being fault
    tolerant. In a production system, your Spark cluster should ideally be on the
    same machines as your Hadoop cluster to make it easy to read files. The Spark
    binary you run on your clusters must be compiled with the same HDFS version as
    the one you wish to use.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to install HDFS, but heading to the [Hadoop homepage](http://hadoop.apache.org/)
    is one way to get started and run hdfs locally on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Run [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java) on any file pattern
    on your hdfs directory.
  prefs: []
  type: TYPE_NORMAL
- en: Importing from Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading from Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most likely, you aren't going to be storing your logs data in a database (that
    is likely too expensive), but there may be other data you want to input to Spark
    that is stored in a database. Perhaps that data can be joined with the logs to
    provide more information.
  prefs: []
  type: TYPE_NORMAL
- en: The same way file systems have evolved over time to scale, so have databases.
  prefs: []
  type: TYPE_NORMAL
- en: A simple database to begin with is a single database - SQL databases are quite
    common. When that fills, one option is to buy a larger machine for the database.
    The price of these larger machines gets increasingly expensive (even price per
    unit of storage) and it is eventually no longer possible to buy a machine big
    enough at some point. A common choice then is to switch to sharded databases.
    With that option, application level code is written to determine on which database
    shard a piece of data should be read or written to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read data in from a SQL database, the JdbcRDD is one option for a moderate
    amount of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/JdbcRDD.html](https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/JdbcRDD.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently, there has been a movement in the database world towards **NoSQL**
    or **Key-Value** databases that were designed to scale. For these databases, it's
    usually transparent to the application developer that the underlying database
    stores data on multiple machines. **Cassandra** is one very popular NoSQL database.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read data from Cassandra into Spark, see the Spark Cassandra Connector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/datastax/spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use a different database, Spark may have a built-in library for importing
    from that database, but more often 3rd parties offer Spark integration - so search
    for that.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, reading a small amount of data from a database is much easier than
    reading a ton of data. It's important to understand your database and Spark's
    distributed programming model in order to write optimal code for importing a very
    large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming Data Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers importing data for streaming. The streaming example in the
    previous chapter received data through a single socket - which is not a scalable
    solution. In a real production system, there are many servers continuously writing
    logs, and we want to process all of those files. This section contains scalable
    solutions for data import. Since streaming is now used, there is no longer the
    need for a nightly batch job to process logs, but instead - this logs processing
    program can be long-lived - continuously receiving new logs data, processing the
    data, and computing log stats.
  prefs: []
  type: TYPE_NORMAL
- en: '[Built In Methods for Streaming Import](built_in.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Kafka](kafka.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Built In Methods for Streaming Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Built In Methods for Streaming Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The StreamingContext has many built in methods for importing data to streaming.
    `socketTextStream` was introduced in the previous chapter, and `textFileStream`
    is introduced here. The `textFileStream` method monitors any Hadoop-compatible
    filesystem directory for new files and when it detects a new file - reads it into
    Spark Streaming. Just replace the call to `socketTextStream` with `textFileStream`,
    and pass in the directory to monitor for log files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Try running [LogAnalyzerStreamingImportDirectory.java](LogAnalyzerStreamingImportDirectory.java)
    by specifying a directory. You'll also need to drop or copy some new log files
    into that directory while the program is running to see the calculated values
    update.
  prefs: []
  type: TYPE_NORMAL
- en: There are more built-in input methods for streaming - check them out in the
    reference API documents for the StreamingContext.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the previous example picks up new log files right away - the log files
    aren't copied over until a long time after the HTTP requests in the logs actually
    occurred. While that enables auto-refresh of log data, that's still not realtime.
    To get realtime logs processing, we need a way to send over log lines immediately.
    Kafka is a high-throughput distributed message system that is perfect for that
    use case. Spark contains an external module importing data from Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is some useful documentation to set up Kafka for Spark Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kafka Documentation](http://kafka.apache.org/documentation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KafkaUtils class in the external module of the Spark project](https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala)
    - This is the external module that has been written that imports data from Kafka
    into Spark Streaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Spark Streaming Example of using Kafka](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java)
    - This is an example that demonstrates how to call KafkaUtils.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section 3: Exporting Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exporting Data out of Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section contains methods for exporting data out of Spark into systems.
    First, you'll have to figure out if your output data is small (meaning can fit
    on memory on one machine) or large (too big to fit into memory on one machine).
    Consult these two sections based on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[Small Datasets](small.html) - If you have a small dataset, you can call an
    action on this dataset to retrieve objects in memory on the driver program, and
    then write those objects out any way you want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Large Datasets](large.html) - For a large dataset, it''s important to remember
    that this dataset is too large to fit in memory on the driver program. In that
    case, you can either call Spark to write the data to files directly from the Spark
    workers or you can implement your own custom solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exporting Small Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the data you are exporting out of Spark is small, you can just use an action
    to convert the RDD into objects in memory on the driver program, and then write
    that output directly to any data storage solution of your choosing. You may remember
    that we called the `take(N)` action where N is some finite number instead of the
    `collect()` action to ensure the output fits in memory - no matter how big the
    input data set may be - this is good practice. This section walks through example
    code where you'll write the log statistics to a file.
  prefs: []
  type: TYPE_NORMAL
- en: It may not be that useful to have these stats output to a file - in practice,
    you might write these statistics to a database for your presentation layer to
    access.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, run [LogAnalyzerExportSmallData.java](java8/src/main/java/com/databricks/apps/logs/chapter2/LogAnalyzerExportSmallData.java).
    Try modifying it to write to a database of your own choosing.
  prefs: []
  type: TYPE_NORMAL
- en: Large Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exporting Large Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are exporting a very large dataset, you can't call `collect()` or a similar
    action to read all the data from the RDD onto the single driver program - that
    could trigger out of memory problems. Instead, you have to be careful about saving
    a large RDD. See these two sections for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '[Save the RDD to Files](save_the_rdd_to_files.html) - There are built in methods
    in Spark for saving a large RDD to files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Save the RDD to a Database](save_an_rdd_to_a_database.html) - This section
    contains recommended best practices for saving a large RDD to a database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the RDD to Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Save the RDD to files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDD's have some built in methods for saving them to disk. Once in files, many
    of the Hadoop databases can bulk load in data directly from files, as long as
    they are in a specific format.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code example, we demonstrate the simple `.saveAsTextFile()`
    method. This will write the data to simple text files where the `.toString()`
    method is called on each RDD element and one element is written per line. The
    number of files output is equal to the the number of partitions of the RDD being
    saved. In this sample, the RDD is repartitioned to control the number of output
    files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Run [LogAnalyzerExportRDD.java](java8/src/main/java/com/databricks/apps/logs/chapter2/LogAnalyzerExportRDD.java)
    now. Notice that the number of files output is the same as the number of partitionds
    of the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the API documentation for other built in methods for saving to file.
    There are different built in methods for saving RDD's to files in various formats,
    so skim the whole RDD package to see if there is something to suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Sqoop](http://http://sqoop.apache.org/) is a very useful tool that can import
    Hadoop files into various databases, and is thus very useful to use for getting
    the data written into files from Spark into your production database.'
  prefs: []
  type: TYPE_NORMAL
- en: Save the RDD to a Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Save an RDD to a Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can write your own custom writer and call a transform on your RDD to write
    each element to a database of your choice, but there''s a lot of ways to write
    something that looks like it would work, but does not work well in a distributed
    environment. Here are some things to watch out for:'
  prefs: []
  type: TYPE_NORMAL
- en: A common naive mistake is to open a connection on the Spark driver program,
    and then try to use that connection on the Spark workers. The connection should
    be opened on the Spark worker, such as by calling `forEachPartition` and opening
    the connection inside that function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use partitioning to control the parallelism for writing to your data storage.
    Your data storage may not support too many concurrent connections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use batching for writing out multiple objects at a time if batching is optimal
    for your data storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure your write mechanism is resilient to failures. Writing out a very
    large dataset can take a long time, which increases the chance something can go
    wrong - a network failure, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider utilizing a static pool of database connections on your Spark workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are writing to a sharded data storage, partition your RDD to match your
    sharding strategy. That way each of your Spark workers only connects to one database
    shard, rather than each Spark worker connecting to every database shard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be cautious when writing out so much data, and make sure you understand the
    distributed nature of Spark!
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 4: Log Analyzer Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logs Analyzer Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This directory contains code from the chapters, assembled together to form
    a sample logs analyzer application. Other libraries that are not discussed have
    been used to make this a more finished application. These are the features of
    our MVP (minimal viable product) logs analyzer application:'
  prefs: []
  type: TYPE_NORMAL
- en: Reads in new log files from a directory and inputs those new files into Spark
    Streaming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute stats on the logs using Spark - stats for the last 30 seconds are calculated
    as well as for all of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the calculated stats to an html file on the local file system that gets
    refreshed on a set interval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Log Analyzer MVP Application](app_diagram.png)'
  prefs: []
  type: TYPE_IMG
- en: You can use this simple application as a skeleton and combine features from
    the chapters to produce your own custom logs analysis application. The main class
    is [LogAnalyzerAppMain.java](LogAnalyzerAppMain.java).
  prefs: []
  type: TYPE_NORMAL
