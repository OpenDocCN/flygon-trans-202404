- en: Log Analysis with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行日志分析
- en: Log Analysis with Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行日志分析
- en: This project demonstrates how easy it is to do log analysis with Apache Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目演示了使用Apache Spark进行日志分析有多么容易。
- en: Log analysis is an ideal use case for Spark. It's a very large, common data
    source and contains a rich set of information. Spark allows you to store your
    logs in files to disk cheaply, while still providing a quick and simple way to
    process them. We hope this project will show you how to use Apache Spark on your
    organization's production logs and fully harness the power of that data. Log data
    can be used for monitoring your servers, improving business and customer intelligence,
    building recommendation systems, preventing fraud, and much more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 日志分析是Spark的一个理想用例。它是一个非常庞大、常见的数据源，包含着丰富的信息。Spark允许您廉价地将日志存储在磁盘上，同时提供了一种快速简单的处理方法。我们希望这个项目能向您展示如何在您组织的生产日志上使用Apache
    Spark，并充分利用这些数据的力量。日志数据可以用于监视您的服务器、改善业务和客户情报、构建推荐系统、防止欺诈等等。
- en: How to use this project
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用本项目
- en: This project is broken up into sections with bite-sized examples for demonstrating
    new Spark functionality for log processing. This makes the examples easy to run
    and learn as they cover just one new topic at a time. At the end, we assemble
    some of these examples to form a sample log analysis application.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目被分成了几个部分，每个部分都有小巧的示例，用于演示用于日志处理的新Spark功能。这使得示例易于运行和学习，因为它们每次只涵盖一个新主题。最后，我们将一些这些示例组装起来，形成一个示例日志分析应用程序。
- en: '[Section 1: Introduction to Apache Spark](index2.html)'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[第一节：介绍Apache Spark](index2.html)'
- en: The Apache Spark library is introduced, as well as Spark SQL and Spark Streaming.
    By the end of this chapter, a reader will know how to call transformations and
    actions and work with RDDs and DStreams.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 引入了Apache Spark库，以及Spark SQL和Spark Streaming。通过本章结束时，读者将知道如何调用转换和操作，并且如何使用RDD和DStreams。
- en: '[Section 2: Importing Data](index3.html)'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[第二节：导入数据](index3.html)'
- en: This section includes examples to illustrate how to get data into Spark and
    starts covering concepts of distributed computing. The examples are all suitable
    for datasets that are too large to be processed on one machine.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '本节包括示例，以说明如何将数据输入Spark，并开始涵盖分布式计算的概念。这些示例都适用于数据集太大而无法在一台机器上处理的情况。 '
- en: '[Section 3: Exporting Data](index4.html)'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[第三节：导出数据](index4.html)'
- en: This section includes examples to illustrate how to get data out of Spark. Again,
    concepts of a distributed computing environment are reinforced, and the examples
    are suitable for large datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括示例，以说明如何从Spark中获取数据。再次强调了分布式计算环境的概念，并且这些示例适用于大型数据集。
- en: '[Section 4: Logs Analyzer Application](index5.html)'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[第四节：日志分析器应用程序](index5.html)'
- en: This section puts together some of the code in the other chapters to form a
    sample log analysis application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节汇集了其他章节中的一些代码，形成了一个示例日志分析应用程序。
- en: More to come...
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容即将推出...
- en: While that's all for now, there's definitely more to come over time.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 目前就这些，但随着时间的推移，肯定会有更多内容。
- en: 'Section 1: Introduction to Apache Spark'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一节：介绍Apache Spark
- en: 'Section 1: Introduction to Apache Spark'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一节：介绍Apache Spark
- en: In this section, we demonstrate how simple it is to analyze web logs using Apache
    Spark. We'll show how to load a Resilient Distributed Dataset (**RDD**) of access
    log lines and use Spark tranformations and actions to compute some statistics
    for web server monitoring. In the process, we'll introduce the Spark SQL and the
    Spark Streaming libraries.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们演示了使用Apache Spark分析网站日志有多么简单。我们将展示如何加载一个弹性分布式数据集（**RDD**）的访问日志行，并使用Spark转换和操作来计算一些用于Web服务器监视的统计信息。在此过程中，我们将介绍Spark
    SQL和Spark Streaming库。
- en: In this explanation, the code snippets are in [Java 8](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/java8).
    However, there is also sample code in [Java 6](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/java6),
    [Scala](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/scala),
    and [Python](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/python)
    included in this directory. In those folders are README's for instructions on
    how to build and run those examples, and the necessary build files with all the
    required dependencies.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个解释中，代码片段是用[Java 8](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/java8)编写的。然而，在这个目录中还包括[Java
    6](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/java6)、[Scala](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/scala)和[Python](https://github.com/databricks/reference-apps/tree/master/logs_analyzer/chapter1/python)的示例代码。在这些文件夹中有关于如何构建和运行这些示例的README文件，以及带有所有必需依赖项的必要构建文件。
- en: 'This chapter covers the following topics:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: '[First Log Analyzer in Spark](spark.html) - This is a first Spark standalone
    logs analysis application.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Spark中的第一个日志分析器](spark.html) - 这是第一个独立使用Spark进行日志分析的应用程序。'
- en: '[Spark SQL](sql.html) - This example does the same thing as the above example,
    but uses SQL syntax instead of Spark transformations and actions.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Spark SQL](sql.html) - 这个示例与上面的示例做的事情相同，但是使用SQL语法而不是Spark的转换和操作。'
- en: '[Spark Streaming](streaming.html) - This example covers how to calculate log
    statistics using the streaming library.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Spark Streaming](streaming.html) - 这个示例涵盖了如何使用流处理库计算日志统计信息。'
- en: First Log Analyzer in Spark
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的第一个日志分析器
- en: First Logs Analyzer in Spark
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的第一个日志分析器
- en: Before beginning this section, go through [Spark Quick Start](https://spark.apache.org/docs/latest/quick-start.html)
    and familiarize with the [Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)
    first.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，请先阅读[Spark快速入门](https://spark.apache.org/docs/latest/quick-start.html)并熟悉[Spark编程指南](https://spark.apache.org/docs/latest/programming-guide.html)。
- en: 'This section requires a dependency on the Spark Core library in the maven file
    - note update this dependency based on the version of Spark you have installed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要在maven文件中依赖Spark Core库 - 请根据您安装的Spark版本更新此依赖：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Before we can begin, we need two things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们需要两样东西：
- en: '**An Apache access log file**: If you have one, it''s more interesting to use
    real data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个Apache访问日志文件**：如果有一个，使用真实数据会更有趣。'
- en: This is trivial sample one provided at [data/apache.access.log](../data/apache.accesslog).
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是提供的一个简单示例，位于[data/apache.access.log](../data/apache.accesslog)。
- en: 'Or download a better example here: [http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者在这里下载一个更好的示例：[http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)
- en: '**A parser and model for the log file**: See [ApacheAccessLog.java](ApacheAccessLog.java).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志文件的解析器和模型**：请参阅[ApacheAccessLog.java](ApacheAccessLog.java)。'
- en: The example code uses an Apache access log file since that's a well known and
    common log format. It would be easy to rewrite the parser for a different log
    format if you have data in another log format.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码使用Apache访问日志文件，因为这是一个众所周知和常见的日志格式。如果您有另一种日志格式的数据，重写解析器将会很容易。
- en: 'The following statistics will be computed:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算以下统计信息：
- en: The average, min, and max content size of responses returned from the server.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从服务器返回的响应的平均、最小和最大内容大小。
- en: A count of response code's returned.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回的响应代码计数。
- en: All IPAddresses that have accessed this server more than N times.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有访问此服务器超过N次的IP地址。
- en: The top endpoints requested by count.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求次数最多的顶级端点。
- en: Let's first walk through the code first before running the example at [LogAnalyzer.java](LogAnalyzer.java).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行[LogAnalyzer.java](LogAnalyzer.java)示例之前，让我们先仔细阅读代码。
- en: The main body of a simple Spark application is below. The first step is to bring
    up a Spark context. Then the Spark context can load data from a text file as an
    RDD, which it can then process. Finally, before exiting the function, the Spark
    context is stopped.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单Spark应用程序的主体如下。第一步是启动一个Spark上下文。然后Spark上下文可以从文本文件加载数据作为RDD，然后进行处理。最后，在退出函数之前，停止Spark上下文。
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Given an RDD of log lines, use the `map` function to transform each line to
    an ApacheAccessLog object. The ApacheAccessLog RDD is cached in memory, since
    multiple transformations and actions will be called on it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个日志行的 RDD，使用 `map` 函数将每行转换为 ApacheAccessLog 对象。ApacheAccessLog RDD 被缓存在内存中，因为将对其进行多个转换和操作调用。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It's useful to define a sum reducer - this is a function that takes in two integers
    and returns their sum. This is used all over our example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个求和的 reducer 很有用 - 这是一个接受两个整数并返回它们的和的函数。我们的示例中到处都在使用它。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, let's calculate the average, minimum, and maximum content size of the
    response returned. A `map` transformation extracts the content sizes, and then
    different actions (`reduce`, `count`, `min`, and `max`) are called to output various
    stats. Again, call `cache` on the context size RDD to avoid recalculating those
    values for each action called on it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们计算返回的响应的平均、最小和最大内容大小。`map` 转换提取内容大小，然后调用不同的操作（`reduce`、`count`、`min`
    和 `max`）来输出各种统计数据。再次，在内容大小 RDD 上调用 `cache` 以避免在对其执行每个操作时重新计算这些值。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To compute the response code counts, we have to work with key-value pairs -
    by using `mapToPair` and `reduceByKey`. Notice that we call `take(100)` instead
    of `collect()` to gather the final output of the response code counts. Use extreme
    caution before calling `collect()` on an RDD since all that data will be sent
    to a single Spark driver and can cause the driver to run out of memory. Even in
    this case where there are only a limited number of response codes and it seems
    safe - if there are malformed lines in the Apache access log or a bug in the parser,
    there could be many invalid response codes to cause an.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算响应代码计数，我们必须使用键值对 - 通过使用 `mapToPair` 和 `reduceByKey`。请注意，我们调用 `take(100)`
    而不是 `collect()` 来收集响应代码计数的最终输出。在调用 `collect()` 之前，请极度谨慎，因为所有这些数据将被发送到单个 Spark
    驱动程序，可能会导致驱动程序内存耗尽。即使在这种情况下，响应代码数量有限且看起来安全 - 如果 Apache 访问日志中有格式不正确的行或解析器中存在错误，可能会有许多无效的响应代码导致。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To compute any IPAddress that has accessed this server more than 10 times, we
    call the `filter` tranformation and then `map` to retrieve only the IPAddress
    and discard the count. Again we use `take(100)` to retrieve the values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算访问此服务器超过 10 次的任何 IPAddress，我们调用 `filter` 转换，然后 `map` 来仅检索 IPAddress 并丢弃计数。再次使用
    `take(100)` 来检索值。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Last, let's calculate the top endpoints requested in this log file. We define
    an inner class, `ValueComparator` to help with that. This function tells us, given
    two tuples, which one is first in ordering. The key of the tuple is ignored, and
    ordering is based just on the values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们计算此日志文件中请求的前端点。我们定义了一个内部类 `ValueComparator` 来帮助完成这个任务。此函数告诉我们，给定两个元组，哪一个在排序中排在前面。元组的键被忽略，排序仅基于值。
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then, we can use the `ValueComparator` with the `top` action to compute the
    top endpoints accessed on this server according to how many times the endpoint
    was accessed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `ValueComparator` 结合 `top` 操作来计算根据端点被访问的次数计算出此服务器上访问最频繁的前端点。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These code snippets are from [LogAnalyzer.java](LogAnalyzer.java). Now that
    we've walked through the code, try running that example. See the README for language
    specific instructions for building and running.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码片段来自 [LogAnalyzer.java](LogAnalyzer.java)。现在我们已经浏览了代码，请尝试运行该示例。请查看 README
    以获取特定语言的构建和运行说明。
- en: Spark SQL
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: You should go through the [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
    before beginning this section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，您应该仔细阅读 [Spark SQL 指南](https://spark.apache.org/docs/latest/sql-programming-guide.html)。
- en: 'This section requires an additioal dependency on Spark SQL:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分需要额外依赖于 Spark SQL：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For those of you who are familiar with SQL, the same statistics we calculated
    in the previous example can be done using Spark SQL rather than calling Spark
    transformations and actions directly. We walk through how to do that here.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉 SQL 的人来说，在前面的例子中计算的相同统计数据可以使用 Spark SQL 来完成，而不是直接调用 Spark 转换和操作。我们将在这里介绍如何做到这一点。
- en: First, we need to create a SQL Spark context. Note how we create one Spark Context,
    and then use that to instantiate different flavors of Spark contexts. You should
    not initialize multiple Spark contexts from the SparkConf in one process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个 SQL Spark 上下文。请注意如何创建一个 Spark 上下文，然后使用它来实例化不同类型的 Spark 上下文。您不应该在一个进程中从
    SparkConf 初始化多个 Spark 上下文。
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we need a way to register our logs data into a table. In Java, Spark
    SQL can infer the table schema on a standard Java POJO - with getters and setters
    as we''ve done with [ApacheAccessLog.java](ApacheAccessLog.java). (Note: if you
    are using a different language besides Java, there is a different way for Spark
    to infer the table schema. The examples in this directory work out of the box.
    Or you can also refer to the [Spark SQL Guide on Data Sources](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources)
    for more details.)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种方法将我们的日志数据注册到表中。在 Java 中，Spark SQL 可以根据标准的 Java POJO 推断表模式 - 使用像我们在
    [ApacheAccessLog.java](ApacheAccessLog.java) 中所做的那样的 getter 和 setter。（注意：如果您使用的是除
    Java 之外的其他语言，则 Spark 推断表模式的方式不同。此目录中的示例可以直接使用。或者您也可以参考 [Spark SQL 数据源指南](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources)
    了解更多详细信息。）
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we are ready to start running some SQL queries on our table. Here''s the
    code to compute the identical statistics in the previous section - it should look
    very familiar for those of you who know SQL:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备开始在我们的表上运行一些 SQL 查询。以下是计算上一节中相同统计信息的代码 - 对于那些了解 SQL 的人来说，这应该��起来非常熟悉：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that the default SQL dialect does not allow using reserved keyworks as
    alias names. In other words, `SELECT COUNT(*) AS count` will cause errors, but
    `SELECT COUNT(*) AS the_count` runs fine. If you use the HiveQL parser though,
    then you should be able to use anything as an identifier.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，默认的 SQL 方言不允许将保留关键字用作别名。换句话说，`SELECT COUNT(*) AS count` 会导致错误，但 `SELECT
    COUNT(*) AS the_count` 可以正常运行。如果您使用 HiveQL 解析器，则应该能够将任何内容用作标识符。
- en: Try running [LogAnalyzerSQL.java](LogAnalyzerSQL.java) now.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试运行 [LogAnalyzerSQL.java](LogAnalyzerSQL.java)。
- en: Spark Streaming
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 流处理
- en: Spark Streaming
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 流处理
- en: Go through the [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
    before beginning this section. In particular, it covers the concept of DStreams.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，请阅读 [Spark 流处理编程指南](https://spark.apache.org/docs/latest/streaming-programming-guide.html)。特别是，它涵盖了
    DStreams 的概念。
- en: 'This section requires another dependency on the Spark Streaming library:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本节需要另一个依赖项，即 Spark 流处理库：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The earlier examples demonstrates how to compute statistics on an existing log
    file - but not how to do realtime monitoring of logs. Spark Streaming enables
    that functionality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的示例演示了如何在现有日志文件上计算统计信息 - 但没有演示如何实时监控日志。Spark 流处理使这种功能成为可能。
- en: To run the streaming examples, you will `tail` a log file into `netcat` to send
    to Spark. This is not the ideal way to get data into Spark in a production system,
    but is an easy workaround for a first Spark Streaming example. We will cover best
    practices for [how to import data for Spark Streaming in Chapter 2](streaming1.html).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行流处理示例，您将通过 `tail` 命令将日志文件发送到 `netcat` 以发送给 Spark。这不是在生产系统中将数据导入 Spark 的理想方式，但对于第一个
    Spark 流处理示例来说，这是一个简单的解决方法。我们将在[第二章中介绍如何为 Spark 流处理导入数据的最佳实践](streaming1.html)。
- en: 'In a terminal window, just run this command on a logfile which you will append
    to:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端窗口中，只需在您将追加到的日志文件上运行以下命令：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you don''t have a live log file that is being updated on the fly, you can
    add lines manually with the included data file or another your own log file:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有实时更新的日志文件，可以手动添加行，使用附带的数据文件或您自己的日志文件：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When data is streamed into Spark, there are two common use cases covered:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据流入 Spark 时，有两种常见的用例：
- en: '[Windowed Calculations](windows.html) means that you only care about data received
    in the last N amount of time. When monitoring your web servers, perhaps you only
    care about what has happened in the last hour.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[窗口计算](windows.html) 意味着您只关心在最后 N 个时间段内收到的数据。在监视您的 Web 服务器时，也许您只关心过去一小时发生了什么。'
- en: Spark Streaming conveniently splits the input data into the desired time windows
    for easy processing, using the `window` function of the streaming library.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 流处理方便地将输入数据拆分为所需的时间窗口以便进行简单处理，使用流处理库的 `window` 函数。
- en: The `forEachRDD` function allows you to access the RDD's created each time interval.
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEachRDD` 函数允许您访问每个时间间隔创建的 RDD。'
- en: '[Cumulative Calculations](total.html) means that you want to keep cumulative
    statistics, while streaming in new data to refresh those statistics. In that case,
    you need to maintain the state for those statistics.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[累积计算](total.html) 意味着您希望保留累积统计信息，同时流入新数据以刷新这些统计信息。在这种情况下，您需要维护这些统计信息的状态。'
- en: The Spark Streaming library has some convenient functions for maintaining state
    to support this use case, `updateStateByKey`.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming库具有一些方便的函数来维护状态以支持这种用例，`updateStateByKey`。
- en: '[Reusing code from Batching](reuse.html) covers how to organize business logic
    code from the batch examples so that code can be reused in Spark Streaming.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从批处理中重用代码](reuse.html)介绍了如何从批处理示例中组织业务逻辑代码，以便在Spark Streaming中重用代码。'
- en: The Spark Streaming library has `transform` functions which allow you to apply
    arbitrary RDD-to-RDD functions, and thus to reuse code from the batch mode of
    Spark.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming库具有`transform`函数，允许您应用任意的RDD到RDD函数，从而重用Spark的批处理模式中的代码。
- en: 'Windowed Calculations: window()'
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口计算：window()
- en: 'Windowed Calculations: window()'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口计算：window()
- en: A typical use case for log analysis is monitoring a web server, in which case
    you may only be interested in what's happened for the last one hour of time and
    want those statistics to refresh every minute. One hour is the *window length*,
    while one minute is the *slide interval*. In this example, we use a window length
    of 30 seconds and a slide interval of 10 seconds as a comfortable choice for development.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 日志分析的典型用例是监视Web服务器，在这种情况下，您可能只对过去一小时发生的事情感兴趣，并希望这些统计信息每分钟刷新一次。一小时是*窗口长度*，而一分钟是*滑动间隔*。在此示例中，我们使用30秒的窗口长度和10秒的滑动间隔作为开发的舒适选择。
- en: The windows feature of Spark Streaming makes it very easy to compute stats for
    a window of time, using the `window` function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming的窗口功能使得非常容易为一段时间内的窗口计算统计数据，使用`window`函数。
- en: The first step is to initalize the SparkConf and context objects - in particular
    a streaming context. Note how only one SparkContext is created from the conf and
    the streaming and sql contexts are created from those. Next, the main body should
    be written. Finally, the example calls `start()` on the streaming context, and
    `awaitTermination()`to keep the streaming context running and accepting streaming
    input.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是初始化SparkConf和context对象 - 特别是流上下文。请注意，从conf中只创建一个SparkContext，并且从这些上下文中创建流和SQL上下文。接下来，应编写主体。最后，示例调用`start()`在流上下文上，并调用`awaitTermination()`以保持流上下文运行并接受流输入。
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first step of the main body is to create a DStream from reading the socket.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 主体的第一步是从读取套接字创建一个DStream。
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, call the `map` transformation to convert the logDataDStream into a ApacheAccessLog
    DStream.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`map`转换将logDataDStream转换为ApacheAccessLog DStream。
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, call `window` on the accessLogDStream to create a windowed DStream. The
    `window` function nicely packages the input data that is being streamed into RDDs
    containing a window length of data, and creates a new RDD every SLIDE_INTERVAL
    of time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在accessLogDStream上调用`window`以创建一个窗口化的DStream。`window`函数很好地打包正在流入RDD中的输入数据，其中包含一段时间的窗口长度数据，并且每隔SLIDE_INTERVAL时间创建一个新的RDD。
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Then call `foreachRDD` on the windowDStream. The function passed into `forEachRDD`
    is called on each new RDD in the windowDStream as the RDD is created, so every
    *slide_interval*. The RDD passed into the function contains all the input for
    the last *window_length* of time. Now that there is an RDD of ApacheAccessLogs,
    simply reuse code from either two batch examples (regular or SQL). In this example,
    the code was just copied and pasted, but you could refactor this code into one
    place nicely for reuse in your production code base - you can reuse all your batch
    processing code for streaming!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在windowDStream上调用`foreachRDD`。传递给`forEachRDD`的函数在windowDStream中每个新RDD创建时调用，因此每个*slide_interval*。传递给函数的RDD包含最后*window_length*时间内的所有输入。现在有了一个ApacheAccessLogs的RDD，只需从两个批处理示例（常规或SQL）中重用代码即可。在此示例中，代码只是复制并粘贴，但您可以将此代码重新整理到一个地方，以便在生产代码库中重用
    - 您可以重用所有批处理代码以进行流处理！
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that we've walked through the code, run [LogAnalyzerStreaming.java](LogAnalyzerStreaming.java)
    and/or [LogAnalyzerStreamingSQL.java](LogAnalyzerStreamingSQL.java) now. Use the
    `cat` command as explained before to add data to the log file periodically once
    you have your program up.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经浏览了代码，请立即运行[LogAnalyzerStreaming.java](LogAnalyzerStreaming.java)和/或[LogAnalyzerStreamingSQL.java](LogAnalyzerStreamingSQL.java)。在程序启动后，像之前解释的那样使用`cat`命令定期向日志文件添加数据。
- en: 'Cumulative Calculations: updateStateByKey()'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累积计算：updateStateByKey()
- en: 'Cumulative Calculations: updateStateByKey()'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累积计算：updateStateByKey()
- en: To keep track of the log statistics for all of time, state must be maintained
    between processing RDD's in a DStream.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪所有时间的日志统计信息，必须在处理RDD时在DStream中保持状态。
- en: To maintain state for key-pair values, the data may be too big to fit in memory
    on one machine - Spark Streaming can maintain the state for you. To do that, call
    the `updateStateByKey` function of the Spark Streaming library.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了维护键值对的状态，数据可能太大而无法在一台机器的内存中存储 - Spark Streaming可以为您维护状态。要实现这一点，调用Spark Streaming库的`updateStateByKey`函数。
- en: 'First, in order to use `updateStateByKey`, checkpointing must be enabled on
    the streaming context. To do that, just call `checkpoint` on the streaming context
    with a directory to write the checkpoint data. Here is part of the main function
    of a streaming application that will save state for all of time:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了使用`updateStateByKey`，必须在流上下文中启用检查点。只需在流上下文上调用`checkpoint`，并指定一个目录来写入检查点数据。以下是一个流应用程序的主要函数的一部分，将保存所有时间的状态：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To compute the content size statistics, simply use static variables to save
    the current running sum, count, min and max of the content sizes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算内容大小统计信息，只需使用静态变量保存当前运行总和、计数、最小值和最大值的内容大小。
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To update those values, first call map on the AccessLogDStream to retrieve
    a contentSizeDStream. Then just update the values for the static variables by
    calling foreachRDD on the contentSizeDstream, and calling actions on the RDD:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新这些值，首先在AccessLogDStream上调用map以检索contentSizeDStream。然后只需通过在contentSizeDstream上调用foreachRDD，并在RDD上调用操作来更新静态变量的值：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For the other statistics, since they make use of key value pairs, static variables
    can't be used anymore. The amount of state that needs to be maintained is potentially
    too big to fit in memory. So for those stats, we'll make use of `updateStateByKey`
    so Spark streaming will maintain a value for every key in our dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他统计信息，由于它们使用键值对，不能再使用静态变量。需要维护的状态量可能太大而无法在内存中存储。因此，对于这些统计信息，我们将使用`updateStateByKey`，因此Spark
    Streaming将为数据集中的每个键维护一个值。
- en: But before we can call `updateStateByKey`, we need to create a function to pass
    into it. `updateStateByKey` takes in a different reduce function. While our previous
    sum reducer just took in two values and output their sum, this reduce function
    takes in a current value and an iterator of values, and outputs one new value.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但在调用`updateStateByKey`之前，我们需要创建一个传递给它的函数。`updateStateByKey`采用不同的reduce函数。虽然我们先前的sum
    reducer只接受两个值并输出它们的总和，但这个reduce函数接受一个当前值和一个值的迭代器，并输出一个新值。
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we can compute the keyed statistics for all of time with this code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下代码计算所有时间的键控统计信息：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Run [LogAnalyzerStreamingTotal.java](LogAnalyzerStreamingTotal.java) now for
    yourself.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在自己运行[LogAnalyzerStreamingTotal.java](LogAnalyzerStreamingTotal.java)。
- en: 'Reusing Code from Batching: transform()'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从批处理中重用代码：transform()
- en: 'Reusing Code from Batching: transform()'
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从批处理中重用代码：transform()
- en: As you may have noticed, while the functions you called on a DStream are named
    the same as those you called on an RDD in the batch example, they are not the
    same methods, and it may not be clear how to reuse the code from the batch examples.
    In this section, we refactor the code from the batch examples and show how to
    reuse it here.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的，虽然您在DStream上调用的函数与批处理示例中调用的函数同名，但它们并不是相同的方法，可能不清楚如何重用批处理示例中的代码。在本节中，我们重构批处理示例中的代码，并展示如何在此处重用它。
- en: DStreams have `transform` functions which allows you to call any arbitrary RDD
    to RDD functions to RDD's in the DStream. The `transform` functions are perfect
    for reusing any RDD to RDD functions that you may have written in batch code and
    want to port over to streaming. Let's look at some code to illustrate this point.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: DStreams具有`transform`函数，允许您调用任意RDD到RDD函数来处理DStream中的RDD。`transform`函数非常适合重用您可能已经编写的批处理代码中的任何RDD到RDD函数，并希望将其移植到流处理中。让我们看一些代码来说明这一点。
- en: 'Let''s say we have separated out a function, `responseCodeCount` from our batch
    example that can compute the response code count given the apache access logs
    RDD:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从批处理示例中分离出一个名为`responseCodeCount`的函数，该函数可以根据apache访问日志RDD计算响应代码计数：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The responseCodeCountDStream can be created by calling `transformToPair` with
    the `responseCodeCount` function to the accessLogDStream. Then, you can finish
    up by calling `updateStateByKey` to keep a running count of the response codes
    for all of time, and use `forEachRDD` to print the values out:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在accessLogDStream上调用`transformToPair`并使用`responseCodeCount`函数来创建responseCodeCountDStream。然后，通过调用`updateStateByKey`来持续计算所有时间��响应代码计数，并使用`forEachRDD`来打印值：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It is possible to combine `transform` functions before and after an `updateStateByKey`
    as well:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在`updateStateByKey`之前和之后组合`transform`函数：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Take a closer look at [LogAnalyzerStreamingTotalRefactored.java](LogAnalyzerStreamingTotalRefactored.java)
    now to see how that code has been refactored to reuse code from the batch example.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在仔细查看[LogAnalyzerStreamingTotalRefactored.java](LogAnalyzerStreamingTotalRefactored.java)，看看代码是如何重构的，以便重用批处理示例中的代码。
- en: 'Section 2: Importing Data'
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：导入数据
- en: 'Section 2: Importing Data'
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：导入数据
- en: In the last section we covered how to get started with Spark for log analysis,
    but in those examples, data was just pulled in from a local file and the statistics
    were printed to standard out. In this chapter, we cover techniques for loading
    and exporting data that is suitable for a production system. In particular, the
    techniques must scale to handle large production volumes of logs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何开始使用Spark进行日志分析，但在这些示例中，数据只是从本地文件中拉取，统计数据被打印到标准输出中。在本章中，我们将介绍加载和导出适用于生产系统的数据的技术。特别是，这些技术必须能够扩展以处理大量的生产日志。
- en: To scale, Apache Spark is meant to be deployed on a cluster of machines. Read
    the [Spark Cluster Overview Guide](https://spark.apache.org/docs/latest/cluster-overview.html),
    so that you understand the different between the Spark driver vs. the executor
    nodes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展，Apache Spark旨在部署在一组机器的集群上。阅读[Spark集群概述指南](https://spark.apache.org/docs/latest/cluster-overview.html)，以便您了解Spark驱动程序与执行节点之间的区别。
- en: While you could continue running the examples in local mode, it is recommended
    that you set up a Spark cluster to run the remaining examples on and get practice
    working with the cluster - such as familiarizing yourself with the web interface
    of the cluster. You can run a small cluster on your local machine by following
    the instructions for [Spark Standalone Mode](https://spark.apache.org/docs/latest/spark-standalone.html).
    Optionally, if you have access to more machines - such as on AWS or your organization
    has its own datacenters, consult the [cluster overview guide](https://spark.apache.org/docs/latest/cluster-overview.html)
    to do that.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以继续在本地模式下运行示例，但建议您设置一个Spark集群来运行其余的示例，并练习与集群一起工作 - 比如熟悉集群的Web界面。您可以按照[Spark独立模式](https://spark.apache.org/docs/latest/spark-standalone.html)的说明在本地机器上运行一个小型集群。如果您有更多的机器访问权限
    - 比如在AWS上或者您的组织有自己的数据中心，可以参考[集群概述指南](https://spark.apache.org/docs/latest/cluster-overview.html)进行设置。
- en: 'Once you get a Spark cluster up:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您启动了一个Spark集群：
- en: Use spark-submit to run your jobs rather than using the JVM parameter. Run one
    of the examples from the previous chapter to check your set up.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用spark-submit运行您的作业，而不是使用JVM参数。运行上一章的示例之一来检查您的设置。
- en: Poke around and familiarize with the web interfaces for Spark. It's at [http://localhost:8080](http://localhost:8080)
    if you set up a local cluster.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览并熟悉Spark的Web界面。如果您设置了本地集群，它位于[http://localhost:8080](http://localhost:8080)。
- en: 'There are two ways to import data into Spark:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种将数据导入到Spark中的方式：
- en: '[Batch Data Import](batch.html) - if you are loading a dataset all at once.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[批量数据导入](batch.html) - 如果您一次性加载数据集。'
- en: '[Streaming Data Import](streaming1.html) - if you wish to continuously stream
    data into Spark.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[流式数据导入](streaming1.html) - 如果您希望持续将数据流入Spark。'
- en: Batch Import
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量导入
- en: Batch Data Import
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量数据导入
- en: This section covers batch importing data into Apache Spark, such as seen in
    the non-streaming examples from Chapter 1\. Those examples load data from files
    all at once into one RDD, processes that RDD, the job completes, and the program
    exits. In a production system, you could set up a cron job to kick off a batch
    job each night to process the last day's worth of log files and then publish statistics
    for the last day.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了将数据批量导入到Apache Spark中，例如第1章中的非流式示例。这些示例一次性将数据从文件加载到一个RDD中，处理该RDD，作业完成后程序退出。在生产系统中，您可以设置一个cron作业，每晚启动一个批处理作业，处理昨天的日志文件，并发布昨天的统计信息。
- en: '[Importing From Files](importing_from_files.html) covers caveats when importing
    data from files.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从文件导入](importing_from_files.html) 涵盖了从文件导入数据时的注意事项。'
- en: '[Importing from Databases](importing_from_databases.html) links to examples
    of reading data from databases.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从数据库导入](importing_from_databases.html) 链接到从数据库读取数据的示例。'
- en: Importing from Files
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件导入
- en: Importing from Files
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件导入
- en: To support batch import of data on a Spark cluster, the data needs to be accessible
    by all machines on the cluster. Files that are only accessible on one worker machine
    and cannot be read by the others will cause failures.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要支持 Spark 集群的批量数据导入，数据需要被集群上的所有机器访问。只有一个 worker 机器可以访问的文件，其他机器无法读取，将会导致失败。
- en: If you have a small dataset that can fit on one machine, you could manually
    copy your files onto all the nodes on your Spark cluster, perhaps using `rsync`
    to make that easier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个可以适应一个机器的小型数据集，您可以手动将文件复制到 Spark 集群的所有节点上，可能使用 `rsync` 使这一过程更加简单。
- en: '**NFS** or some other network file system makes sure all your machines can
    access the same files without requiring you to copy the files around. But NFS
    isn''t fault tolerant to machine failures and if your dataset is too big to fit
    on one NFS volume - you''d have to store the data on multiple volumes and figure
    out which volume a particular file is on - which could get cumbersome.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**NFS** 或其他网络文件系统确保所有计算机都可以访问相同的文件，而无需将文件复制到各处。但 NFS 对机器故障不具备容错性，如果您的数据集过大无法适应一个
    NFS 卷 - 您将不得不将数据存储在多个卷上，并找出特定文件位于哪个卷上 - 这可能会变得繁琐。'
- en: '**HDFS** and **S3** are great file systems for massive datasets - built to
    store a lot of data and give all the machines on the cluster access to those files,
    while still being fault tolerant. We give a few more tips on running Spark with
    these file systems since they are recommended.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**HDFS** 和 **S3** 是用于大型数据集的出色文件系统 - 构建用于存储大量数据并使集群上的所有机器都能访问这些文件，同时仍具有容错性。我们提供了一些有关使用这些文件系统运行
    Spark 的更多提示，因为它们被推荐使用。'
- en: '[S3](s3.html) is an Amazon AWS solution for storing files in the cloud, easily
    accessible to anyone who signs up for an account.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[S3](s3.html) 是亚马逊 AWS 提供的一种解决方案，用于在云中存储文件，任何注册账户的人都可以轻松访问。'
- en: '[HDFS](hdfs.html) is a distributed file system that is part of Hadoop and can
    be installed on your own datacenters.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HDFS](hdfs.html) 是 Hadoop 的一部分，是一种分布式文件系统，可以安装在您自己的数据中心。'
- en: The good news is that regardless of which of these file systems you choose,
    you can run the same code to read from them - these file systems are all "Hadoop
    compatible" file systems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，无论您选择哪种文件系统，都可以运行相同的代码来从中读取 - 这些文件系统都是“Hadoop 兼容”的文件系统。
- en: In this section, you should try running [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java)
    on any files on your file system of choice. There is nothing new in this code
    - it's just a refactor of the [First Log Analyzer from Chapter One](spark.html).
    Try passing in "*" or "?" for the textFile path, and Spark will read in all the
    files that match that pattern to create the RDD.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您应该尝试在您选择的任何文件系统上运行 [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java)。这段代码没有什么新的内容
    - 它只是对[第一章的第一个日志分析器](spark.html)的重构。尝试为 textFile 路径传入“*”或“?”，Spark 将读取所有与该模式匹配的文件以创建
    RDD。
- en: S3
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: S3
- en: S3
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: S3
- en: S3 is Amazon Web Services's solution for storing large files in the cloud. On
    a production system, you want your Amazon EC2 compute nodes on the same zone as
    your S3 files for speed as well as cost reasons. While S3 files can be read from
    other machines, it would take a long time and be expensive (Amazon S3 data transfer
    prices differ if you read data within AWS vs. to somewhere else on the internet).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: S3 是亚马逊网络服务用于在云中存储大型文件的解决方案。在生产系统中，您希望 Amazon EC2 计算节点与您的 S3 文件位于同一区域，以便出于速度和成本考虑。虽然可以从其他机器读取
    S3 文件，但这将需要很长时间且费用昂贵（如果您在 AWS 内部读取数据与在互联网其他地方读取数据，亚马逊 S3 数据传输价格会有所不同）。
- en: See [running Spark on EC2](https://spark.apache.org/docs/latest/ec2-scripts.html)
    if you want to launch a Spark cluster on AWS - charges apply.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在 AWS 上启动 Spark 集群，请参阅 [在 EC2 上运行 Spark](https://spark.apache.org/docs/latest/ec2-scripts.html)
    - 需要付费。
- en: If you choose to run this example with a local Spark cluster on your machine
    rather than EC2 compute nodes to read the files in S3, use a small data input
    source!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择在本地 Spark 集群上运行此示例而不是在 EC2 计算节点上读取 S3 中的文件，则使用小型数据输入源！
- en: Sign up for an [Amazon Web Services](https://aws.amazon.com/) Account.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册 [Amazon Web Services](https://aws.amazon.com/) 账户。
- en: Load example log files to s3.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将示例日志文件加载到 s3 中。
- en: Log into the [AWS console for S3](https://console.aws.amazon.com/s3/)
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 登录 [S3 的 AWS 控制台](https://console.aws.amazon.com/s3/)
- en: Create an S3 bucket.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 S3 存储桶。
- en: Upload a couple of example log files to that bucket.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一些示例日志文件上传到该存储桶。
- en: 'Your files will be at the path: s3n://YOUR_BUCKET_NAME/YOUR_LOGFILE.log'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的文件将位于路径：s3n://YOUR_BUCKET_NAME/YOUR_LOGFILE.log
- en: 'Configure your security credentials for AWS:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置您的 AWS 安全凭证：
- en: Create and [download your security credentials](https://console.aws.amazon.com/iam/home?#security_credential)
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并[下载您的安全凭证](https://console.aws.amazon.com/iam/home?#security_credential)
- en: 'Set the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to
    the correct values on all machines on your cluster. These can also be set in your
    SparkContext object programmatically like this:'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的集群上的所有机器上设置环境变量AWS_ACCESS_KEY_ID和AWS_SECRET_ACCESS_KEY为正确的值。这些也可以通过编程方式在您的SparkContext对象中设置，如下所示：
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, run [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java) passing
    in the s3n path to your files.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行[LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java)并传入您文件的s3n路径。
- en: HDFS
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS
- en: HDFS
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS
- en: HDFS is a file system that is meant for storing large data sets and being fault
    tolerant. In a production system, your Spark cluster should ideally be on the
    same machines as your Hadoop cluster to make it easy to read files. The Spark
    binary you run on your clusters must be compiled with the same HDFS version as
    the one you wish to use.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是一个用于存储大型数据集并具有容错能力的文件系统。在生产系统中，您的Spark集群理想情况下应该与您的Hadoop集群在同一台机器上，以便轻松读取文件。您在集群上运行的Spark二进制文件必须与您希望使用的HDFS版本编译相同。
- en: There are many ways to install HDFS, but heading to the [Hadoop homepage](http://hadoop.apache.org/)
    is one way to get started and run hdfs locally on your machine.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 安装HDFS的方法有很多种，但前往[Hadoop主页](http://hadoop.apache.org/)是一个开始并在本地机器上运行hdfs的方法。
- en: Run [LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java) on any file pattern
    on your hdfs directory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的hdfs目录上的任何文件模式上运行[LogAnalyzerBatchImport.java](LogAnalyzerBatchImport.java)。
- en: Importing from Databases
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据库导入
- en: Reading from Databases
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据库中读取
- en: Most likely, you aren't going to be storing your logs data in a database (that
    is likely too expensive), but there may be other data you want to input to Spark
    that is stored in a database. Perhaps that data can be joined with the logs to
    provide more information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，您不会将日志数据存储在数据库中（那可能太昂贵了），但可能有其他数据您想要输入到Spark中，这些数据存储在数据库中。也许这些数据可以与日志数据连接以提供更多信息。
- en: The same way file systems have evolved over time to scale, so have databases.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统随着时间的推移而发展，数据库也是如此。
- en: A simple database to begin with is a single database - SQL databases are quite
    common. When that fills, one option is to buy a larger machine for the database.
    The price of these larger machines gets increasingly expensive (even price per
    unit of storage) and it is eventually no longer possible to buy a machine big
    enough at some point. A common choice then is to switch to sharded databases.
    With that option, application level code is written to determine on which database
    shard a piece of data should be read or written to.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的数据库起步是单个数据库 - SQL数据库非常常见。当数据库填满时，一个选择是为数据库购买更大的机器。这些更大的机器的价格变得越来越昂贵（甚至是每单位存储的价格），并且最终在某个时刻不再可能购买足够大的机器。一个常见的选择是切换到分片数据库。通过该选项，编写应用程序级代码来确定数据片段应该读取或写入到哪个数据库分片。
- en: 'To read data in from a SQL database, the JdbcRDD is one option for a moderate
    amount of data:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要从SQL数据库中读取数据，JdbcRDD是适中数据量的一种选择：
- en: '[https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/JdbcRDD.html](https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/JdbcRDD.html)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/JdbcRDD.html](https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/JdbcRDD.html)'
- en: Recently, there has been a movement in the database world towards **NoSQL**
    or **Key-Value** databases that were designed to scale. For these databases, it's
    usually transparent to the application developer that the underlying database
    stores data on multiple machines. **Cassandra** is one very popular NoSQL database.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，数据库领域出现了一种向**NoSQL**或**键值**数据库发展的趋势，这些数据库旨在扩展。对于这些数据库，应用程序开发人员通常不会意识到底层数据库在多台机器上存储数据。**Cassandra**是一个非常流行的NoSQL数据库。
- en: 'To read data from Cassandra into Spark, see the Spark Cassandra Connector:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Cassandra读取数据到Spark，请参阅Spark Cassandra连接器：
- en: '[https://github.com/datastax/spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/datastax/spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)'
- en: If you use a different database, Spark may have a built-in library for importing
    from that database, but more often 3rd parties offer Spark integration - so search
    for that.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用不同的数据库，Spark可能具有用于从该数据库导入的内置库，但更常见的是第三方提供Spark集成 - 因此请搜索。
- en: As usual, reading a small amount of data from a database is much easier than
    reading a ton of data. It's important to understand your database and Spark's
    distributed programming model in order to write optimal code for importing a very
    large dataset.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从数据库中读取少量数据比读取大量数据要容易得多。了解您的数据库和 Spark 的分布式编程模型对于编写导入非常大数据集的最佳代码非常重要。
- en: Streaming Import
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式导入
- en: Streaming Data Import
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式数据导入
- en: This section covers importing data for streaming. The streaming example in the
    previous chapter received data through a single socket - which is not a scalable
    solution. In a real production system, there are many servers continuously writing
    logs, and we want to process all of those files. This section contains scalable
    solutions for data import. Since streaming is now used, there is no longer the
    need for a nightly batch job to process logs, but instead - this logs processing
    program can be long-lived - continuously receiving new logs data, processing the
    data, and computing log stats.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了用于流式处理的数据导入。前一章的流式处理示例通过单个套接字接收数据 - 这不是可扩展的解决方案。在一个真正的生产系统中，有许多服务器不断地写入日志，并且我们希望处理所有这些文件。本节包含用于数据导入的可扩展解决方案。由于现在使用了流式处理，所以不再需要每晚的批处理作业来处理日志，而是
    - 此日志处理程序可以长时间运行 - 持续接收新的日志数据，处理数据并计算日志统计信息。
- en: '[Built In Methods for Streaming Import](built_in.html)'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[流式导入的内置方法](built_in.html)'
- en: '[Kafka](kafka.html)'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Kafka](kafka.html)'
- en: Built In Methods for Streaming Import
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式导入的内置方法
- en: Built In Methods for Streaming Import
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式导入的内置方法
- en: The StreamingContext has many built in methods for importing data to streaming.
    `socketTextStream` was introduced in the previous chapter, and `textFileStream`
    is introduced here. The `textFileStream` method monitors any Hadoop-compatible
    filesystem directory for new files and when it detects a new file - reads it into
    Spark Streaming. Just replace the call to `socketTextStream` with `textFileStream`,
    and pass in the directory to monitor for log files.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: StreamingContext 有许多用于导入流数据的内置方法。 `socketTextStream` 在前一章中介绍过，`textFileStream`
    在这里介绍。 `textFileStream` 方法监视任何 Hadoop 兼容的文件系统目录中的新文件，并在检测到新文件时将其读入 Spark Streaming。只需将对
    `socketTextStream` 的调用替换为 `textFileStream`，并传入要监视日志文件的目录。
- en: '[PRE30]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Try running [LogAnalyzerStreamingImportDirectory.java](LogAnalyzerStreamingImportDirectory.java)
    by specifying a directory. You'll also need to drop or copy some new log files
    into that directory while the program is running to see the calculated values
    update.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行 [LogAnalyzerStreamingImportDirectory.java](LogAnalyzerStreamingImportDirectory.java)
    并指定一个目录。在程序运行时，您还需要删除或复制一些新的日志文件到该目录中，以查看计算值更新。
- en: There are more built-in input methods for streaming - check them out in the
    reference API documents for the StreamingContext.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理有更多的内置输入方法 - 在 StreamingContext 的参考 API 文档中查看它们。
- en: Kafka
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka
- en: Kafka
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka
- en: While the previous example picks up new log files right away - the log files
    aren't copied over until a long time after the HTTP requests in the logs actually
    occurred. While that enables auto-refresh of log data, that's still not realtime.
    To get realtime logs processing, we need a way to send over log lines immediately.
    Kafka is a high-throughput distributed message system that is perfect for that
    use case. Spark contains an external module importing data from Kafka.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的示例立即捕获新的日志文件 - 但是日志文件直到 HTTP 请求实际发生后很长一段时间才被复制过来。虽然这样可以实现日志数据的自动刷新，但这仍然不是实时的。要实现实时日志处理，我们需要一种立即发送日志行的方法。Kafka
    是一个高吞吐量的分布式消息系统，非常适合这种用例。Spark 包含一个从 Kafka 导入数据的外部模块。
- en: 'Here is some useful documentation to set up Kafka for Spark Streaming:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些有用的文档，用于为 Spark Streaming 设置 Kafka：
- en: '[Kafka Documentation](http://kafka.apache.org/documentation.html)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kafka 文档](http://kafka.apache.org/documentation.html)'
- en: '[KafkaUtils class in the external module of the Spark project](https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala)
    - This is the external module that has been written that imports data from Kafka
    into Spark Streaming.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark 项目的外部模块 KafkaUtils 类](https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala)
    - 这是已编写的从 Kafka 导入数据到 Spark Streaming 的外部模块。'
- en: '[Spark Streaming Example of using Kafka](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java)
    - This is an example that demonstrates how to call KafkaUtils.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Kafka的Spark Streaming示例](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java)
    - 这是一个演示如何调用KafkaUtils的示例。'
- en: 'Section 3: Exporting Data'
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3节：导出数据
- en: Exporting Data out of Spark
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Spark导出数据
- en: This section contains methods for exporting data out of Spark into systems.
    First, you'll have to figure out if your output data is small (meaning can fit
    on memory on one machine) or large (too big to fit into memory on one machine).
    Consult these two sections based on your use case.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分包含将数据从Spark导出到系统的方法。首先，您需要确定您的输出数据是小的（意味着可以适合一个机器的内存）还是大的（太大而无法适合一个机器的内存）。根据您的用例，请参考这两个部分。
- en: '[Small Datasets](small.html) - If you have a small dataset, you can call an
    action on this dataset to retrieve objects in memory on the driver program, and
    then write those objects out any way you want.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[小数据集](small.html) - 如果您有一个小数据集，您可以对该数据集调用一个操作，以在驱动程序内存中检索对象，然后以任何您想要的方式写出这些对象。'
- en: '[Large Datasets](large.html) - For a large dataset, it''s important to remember
    that this dataset is too large to fit in memory on the driver program. In that
    case, you can either call Spark to write the data to files directly from the Spark
    workers or you can implement your own custom solution.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大数据集](large.html) - 对于大数据集，重要的是要记住这个数据集太大，无法适合驱动程序内存。在这种情况下，您可以要么调用Spark直接从Spark工作节点将数据写入文件，要么实现您自己的自定义解决方案。'
- en: Small Datasets
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小数据集
- en: Exporting Small Datasets
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出小数据集
- en: If the data you are exporting out of Spark is small, you can just use an action
    to convert the RDD into objects in memory on the driver program, and then write
    that output directly to any data storage solution of your choosing. You may remember
    that we called the `take(N)` action where N is some finite number instead of the
    `collect()` action to ensure the output fits in memory - no matter how big the
    input data set may be - this is good practice. This section walks through example
    code where you'll write the log statistics to a file.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要从Spark导出的数据很小，您可以使用一个操作将RDD转换为驱动程序内存中的对象，然后直接将该输出写入您选择的任何数据存储解决方案。您可能记得我们调用了`take(N)`操作，其中N是某个有限数字，而不是`collect()`操作，以确保输出适合内存
    - 无论输入数据集有多大 - 这是一个好的做法。本节将演示代码，您将把日志统计数据写入文件。
- en: It may not be that useful to have these stats output to a file - in practice,
    you might write these statistics to a database for your presentation layer to
    access.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些统计数据输出到文件可能并不那么有用 - 在实践中，您可能会将这些统计数据写入数据库，以便您的展示层访问。
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, run [LogAnalyzerExportSmallData.java](java8/src/main/java/com/databricks/apps/logs/chapter2/LogAnalyzerExportSmallData.java).
    Try modifying it to write to a database of your own choosing.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行[LogAnalyzerExportSmallData.java](java8/src/main/java/com/databricks/apps/logs/chapter2/LogAnalyzerExportSmallData.java)。尝试修改它以将数据写入您选择的数据库。
- en: Large Datasets
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据集
- en: Exporting Large Datasets
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出大数据集
- en: If you are exporting a very large dataset, you can't call `collect()` or a similar
    action to read all the data from the RDD onto the single driver program - that
    could trigger out of memory problems. Instead, you have to be careful about saving
    a large RDD. See these two sections for more information.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要导出一个非常大的数据集，您不能调用`collect()`或类似的操作将所有数据从RDD读取到单个驱动程序中 - 这可能会触发内存不足的问题。相反，您必须小心保存大型RDD。请参阅这两个部分以获取更多信息。
- en: '[Save the RDD to Files](save_the_rdd_to_files.html) - There are built in methods
    in Spark for saving a large RDD to files.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将RDD保存到文件](save_the_rdd_to_files.html) - Spark中有内置方法可将大型RDD保存到文件中。'
- en: '[Save the RDD to a Database](save_an_rdd_to_a_database.html) - This section
    contains recommended best practices for saving a large RDD to a database.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将RDD保存到数据库](save_an_rdd_to_a_database.html) - 本节包含将大型RDD保存到数据库的推荐最佳实践。'
- en: Save the RDD to Files
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RDD保存到文件
- en: Save the RDD to files
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RDD保存到文件
- en: RDD's have some built in methods for saving them to disk. Once in files, many
    of the Hadoop databases can bulk load in data directly from files, as long as
    they are in a specific format.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: RDD具有一些内置方法可将它们保存到磁盘。一旦保存到文件中，许多Hadoop数据库可以直接从文件中批量加载数据，只要它们符合特定格式。
- en: In the following code example, we demonstrate the simple `.saveAsTextFile()`
    method. This will write the data to simple text files where the `.toString()`
    method is called on each RDD element and one element is written per line. The
    number of files output is equal to the the number of partitions of the RDD being
    saved. In this sample, the RDD is repartitioned to control the number of output
    files.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们演示了简单的`.saveAsTextFile()`方法。这将把数据写入简单的文本文件，其中对每个RDD元素调用了`.toString()`方法，并且每行写入一个元素。输出文件的数量等于要保存的RDD的分区数。在此示例中，RDD被重新分区以控制输出文件的数量。
- en: '[PRE32]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Run [LogAnalyzerExportRDD.java](java8/src/main/java/com/databricks/apps/logs/chapter2/LogAnalyzerExportRDD.java)
    now. Notice that the number of files output is the same as the number of partitionds
    of the RDD.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行[LogAnalyzerExportRDD.java](java8/src/main/java/com/databricks/apps/logs/chapter2/LogAnalyzerExportRDD.java)。注意输出文件的数量与RDD的分区数相同。
- en: Refer to the API documentation for other built in methods for saving to file.
    There are different built in methods for saving RDD's to files in various formats,
    so skim the whole RDD package to see if there is something to suit your needs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 参考API文档以了解其他用于保存文件的内置方法。有不同的内置方法可用于将RDD保存到各种格式的文件中，因此请浏览整个RDD包，看看是否有适合您需求的内容。
- en: '[Sqoop](http://http://sqoop.apache.org/) is a very useful tool that can import
    Hadoop files into various databases, and is thus very useful to use for getting
    the data written into files from Spark into your production database.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sqoop](http://http://sqoop.apache.org/)是一个非常有用的工具，可以将Hadoop文件导入各种数据库，因此非常适合用于将数据从Spark写入文件到生产数据库中。'
- en: Save the RDD to a Database
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RDD保存到数据库
- en: Save an RDD to a Database
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RDD保存到数据库
- en: 'You can write your own custom writer and call a transform on your RDD to write
    each element to a database of your choice, but there''s a lot of ways to write
    something that looks like it would work, but does not work well in a distributed
    environment. Here are some things to watch out for:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以编写自己的自定义写入器，并在您的RDD上调用转换以将每个元素写入您选择的数据库，但有很多方法可以编写看起来可以工作但在分布式环境中效果不佳的东西。以下是一些需要注意的事项：
- en: A common naive mistake is to open a connection on the Spark driver program,
    and then try to use that connection on the Spark workers. The connection should
    be opened on the Spark worker, such as by calling `forEachPartition` and opening
    the connection inside that function.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常见的天真错误是在Spark驱动程序上打开连接，然后尝试在Spark工作节点上使用该连接。连接应该在Spark工作节点上打开，例如通过调用`forEachPartition`并在该函数内部打开连接。
- en: Use partitioning to control the parallelism for writing to your data storage.
    Your data storage may not support too many concurrent connections.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分区控制写入数据存储的并行性。您的数据存储可能不支持太多并发连接。
- en: Use batching for writing out multiple objects at a time if batching is optimal
    for your data storage.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批处理一次性写出多个对象，如果批处理对您的数据存储是最佳的。
- en: Make sure your write mechanism is resilient to failures. Writing out a very
    large dataset can take a long time, which increases the chance something can go
    wrong - a network failure, etc.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您的写入机制对故障具有弹性。写出非常大的数据集可能需要很长时间，这增加了出现问题的机会 - 网络故障等。
- en: Consider utilizing a static pool of database connections on your Spark workers.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑在Spark工作节点上利用静态数据库连接池。
- en: If you are writing to a sharded data storage, partition your RDD to match your
    sharding strategy. That way each of your Spark workers only connects to one database
    shard, rather than each Spark worker connecting to every database shard.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您要写入分片数据存储，请将RDD分区以匹配您的分片策略。这样，您的每个Spark工作节点只连接到一个数据库分片，而不是每个Spark工作节点连接到每个数据库分片。
- en: Be cautious when writing out so much data, and make sure you understand the
    distributed nature of Spark!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在写出大量数据时要谨慎，并确保您了解Spark的分布式特性！
- en: 'Section 4: Log Analyzer Application'
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4节：日志分析器应用程序
- en: Logs Analyzer Application
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志分析器应用程序
- en: 'This directory contains code from the chapters, assembled together to form
    a sample logs analyzer application. Other libraries that are not discussed have
    been used to make this a more finished application. These are the features of
    our MVP (minimal viable product) logs analyzer application:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 该目录包含来自各章节的代码，组合在一起形成一个示例日志分析器应用程序。已经使用了其他未讨论的库，使其成为一个更完整的应用程序。这是我们的MVP（最小可行产品）日志分析器应用程序的功能：
- en: Reads in new log files from a directory and inputs those new files into Spark
    Streaming.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从目录中读取新的日志文件，并将这些新文件输入到Spark Streaming中。
- en: Compute stats on the logs using Spark - stats for the last 30 seconds are calculated
    as well as for all of time.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark对日志进行统计 - 统计最近30秒的数据以及全部时间的数据。
- en: Write the calculated stats to an html file on the local file system that gets
    refreshed on a set interval.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算出的统计数据写入本地文件系统上的一个html文件中，并在一定时间间隔内刷新。
- en: '![Log Analyzer MVP Application](app_diagram.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![日志分析 MVP 应用程序](app_diagram.png)'
- en: You can use this simple application as a skeleton and combine features from
    the chapters to produce your own custom logs analysis application. The main class
    is [LogAnalyzerAppMain.java](LogAnalyzerAppMain.java).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个简单的应用程序作为框架，并结合各章节的功能来制作自己定制的日志分析应用程序。主要类是[LogAnalyzerAppMain.java](LogAnalyzerAppMain.java)。
