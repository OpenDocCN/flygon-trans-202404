- en: Weather TimeSeries Data Application with Cassandra
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cassandra的天气时间序列数据应用
- en: Weather TimeSeries Data Application with Cassandra
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cassandra的天气时间序列数据应用
- en: This project demonstrates how to easily leverage and integrate Apache Spark,
    Spark Streaming, Apache Cassandra with the Spark Cassandra Connector and Apache
    Kafka in general, and more specifically for time series data. It also demonstrates
    how to do this in an asynchronous Akka event-driven environment. We use weather
    data and the existing hourly data format as the sample domain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目演示了如何轻松地利用和集成Apache Spark、Spark Streaming、带有Spark Cassandra连接器的Apache Cassandra和一般的Apache
    Kafka，特别是针对时间序列数据。它还演示了如何在异步Akka事件驱动环境中执行此操作。我们使用天气数据和现有的小时数据格式作为示例域。
- en: Time Series Data
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列数据
- en: The use of time series data for business analysis is not new. What is new is
    the ability to collect and analyze massive volumes of data in sequence at extremely
    high velocity to get the clearest picture to predict and forecast future market
    changes, user behavior, environmental conditions, resource consumption, health
    trends and much, much more.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 利用时间序列数据进行业务分析并非新鲜事物。新的是能够以极高的速度收集和分析海量的数据序列，以获取最清晰的图片，以预测和预测未来市场变化、用户行为、环境条件、资源消耗、健康趋势等等。
- en: Apache Cassandra is a NoSQL database platform particularly suited for these
    types of Big Data challenges. Cassandra’s data model is an excellent fit for handling
    data in sequence regardless of data type or size. When writing data to Cassandra,
    data is sorted and written sequentially to disk. When retrieving data by row key
    and then by range, you get a fast and efficient access pattern due to minimal
    disk seeks – time series data is an excellent fit for this type of pattern. Apache
    Cassandra allows businesses to identify meaningful characteristics in their time
    series data as fast as possible to make clear decisions about expected future
    outcomes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Cassandra是一种特别适用于这些大数据挑战的NoSQL数据库平台。Cassandra的数据模型非常适合处理无论数据类型或大小如何都以序列方式处理的数据。当将数据写入Cassandra时，数据被排序并顺序写入磁盘。通过按行键和范围检索数据时，由于最小的磁盘查找，你可以获得快速高效的访问模式——时间序列数据非常适合这种模式。Apache
    Cassandra允许企业尽快识别其时间序列数据中的有意义特征，以便就预期的未来结果做出清晰的决策。
- en: There are many flavors of time series data. Some can be windowed in the stream,
    others can not be windowed in the stream because queries are not by time slice
    but by specific year,month,day,hour. Spark Streaming lets you do both. In some
    cases, such as in the KafkaStreamingActor, using Spark with Cassandra (and the
    right data model) reduces the number of Spark transformations necessary on your
    data because Cassandra does the work for you in its cluster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的时间序列数据。有些可以在流中分窗，而其他的则不能在流中分窗，因为查询不是按时间片，而是按特定的年、月、日、小时。Spark Streaming可以做到两者。在某些情况下，例如在KafkaStreamingActor中，使用Spark与Cassandra（以及正确的数据模型）可以减少在数据上所需的Spark转换的数量，因为Cassandra在其集群中为您完成了这项工作。
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: WeatherApp
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 天气应用
- en: 'This is the primary compute application that processes the raw hourly data
    from the Kafka stream. As soon as each hourly raw data is received on the particular
    Kafka topic, it is mapped to a custom case class. From there, two initial actions
    are done:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理从Kafka流接收的原始小时数据的主要计算应用程序。每当特定Kafka主题上接收到每小时的原始数据时，它都会被映射到自定义案例类中。从那里，进行了两个初始操作：
- en: The raw hourly data by weather station ID is stored in Cassandra for use by
    any other processes that want it in its raw state.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按天气站ID存储的原始小时数据供其他希望以原始状态使用它的进程使用。
- en: The daily precipitation aggregate (0 to 23 hours of data) is computed and updated
    in a daily_aggregate_precipitation table in Cassandra. We can do precipitation
    in the stream because of the data model created and a Cassandra Counter used.
    Whereas aggregation of temperature require more input data so this is done by
    request with a station ID, year, month and day. * With things like daily precipitation
    and temperature aggregates pre-calculated and stored, any requests for annual
    aggregates, high-low or topK for example, can be easily computed via calls to
    specific Akka Actors handling each aggregation type (precip, temp, etc). These
    read the aggregated and persisted data from Cassandra and runs the computation
    through Spark. Data is returned to the requester in a Future - no threads are
    blocked. [Note, when in master with the correct URL of specific files available
    I can link them above so that a user can click on a specific block of code]
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每日降水量聚合（0到23小时的数据）计算并更新到Cassandra的daily_aggregate_precipitation表中。我们可以在流中进行降水量计算，因为创建了数据模型并使用了Cassandra
    Counter。而温度的聚合需要更多的输入数据，因此需要通过站点ID、年份、月份和日期的请求来完成。* 有了像每日降水量和温度聚合这样的预先计算和存储的数据，任何对于年度聚合、高低值或前K值等的请求都可以通过调用处理每种聚合类型的特定Akka
    Actors（降水、温度等）轻松计算。这些Actor从Cassandra中读取已聚合和持久化的数据，并通过Spark运行计算。数据以Future的形式返回给请求者-没有线程被阻塞。[注意，当在主服务器上具有特定文件的正确URL时，我可以将它们链接在上面，以便用户可以点击特定代码块]
- en: WeatherClient
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 天气客户端
- en: Represents a separately-deployed process that would typically be feeding data
    to Kafka and other applications or processes that would be sending data aggregation
    requests.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表一个单独部署的流程，通常会向Kafka和其他应用程序或流程提供数据聚合请求。
- en: Simulates real-time raw weather events received from a separate process which
    this sends to Kafka. It does this as a feed vs bursts on startup. The FileFeedActor
    in the client receives data per file, parses it to each raw data entry, and pipes
    the data through Akka Streams to publish to Kafka and handle life cycle.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟从一个单独流程接收到的实时原始天气事件，然后将其发送到Kafka。它以一种持续的方式而不是在启动时突发地发送数据。客户端中的FileFeedActor每个文件接收数据，将其解析为每个原始数据条目，并通过Akka
    Streams传输数据以发布到Kafka并处理生命周期。
- en: Exercises the weather app by sending requests for various aggregated data (topk,
    high-low, annual…) every n-seconds via the WeatherApiQueries Actor.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过WeatherApiQueries Actor每n秒发送请求以对天气应用程序进行练习，请求各种聚合数据（前K、高低、年度…）。
- en: Architecture and Implementation Details
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构和实现细节
- en: Asynchronous Fault Tolerant Data Pipeline
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异步容错数据管道
- en: Raw hourly data for each weather station is ingested and published to a Kafka
    topic. This would be distributed on Kafka nodes in each Data Center where the
    Spark-Cassandra application are also deployed, as are the co-located Spark and
    Cassandra nodes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个气象站的原始小时数据被摄取并发布到一个Kafka主题中。这些数据会分布在每个数据中心的Kafka节点上，同时Spark-Cassandra应用程序也会部署在这些数据中心，与Spark和Cassandra节点共存。
- en: '![Time Series data App diagram](timeseries.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![时间序列数据应用程序图表](timeseries.png)'
- en: Data is streamed from the Kafka nodes to the Spark Nodes for parallelized distributed
    data computation. The raw data is initially saved to a Cassandra keyspace and
    table. This stream from Kafka is then used for daily aggregation work in Spark,
    which is then persisted to several Cassandra daily aggregate tables. Now, future
    requests for data based on these daily aggregates (temperature, precipitation..)
    can now more quickly be computed. For example, requests for topK, high-low or
    annual precipitation for a specific weather station in a specific year (or year,
    month, day for high-low/topk) do not need to get the raw data but can start on
    the already aggregated data via Spark, now available on demand in Cassandra (replicated,
    fault tolerant and distributed across data centers).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据从Kafka节点流向Spark节点，进行并行分布式数据计算。原始数据最初保存在Cassandra的keyspace和table中。这个从Kafka流过来的数据随后在Spark中用于每日聚合工作，然后持久化到几个Cassandra每日聚合表中。现在，基于这些每日聚合数据（温度、降水量等）的未来数据请求可以更快地计算。例如，对于特定气象站在特定年份（或年份、月份、日期对于高低/前K）的最高K值、高低值或年度降水量的请求不需要获取原始数据，而是可以直接从Spark中的已聚合数据开始计算，现在可以按需在Cassandra中使用（在数据中心之间复制、容错和分布）。
- en: Running the Example
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行示例
- en: Running the Application
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行应用程序
- en: There are many flavors of time series data. Some can be windowed in the stream,
    others can not be windowed in the stream because queries are not by time slice
    but by specific year,month,day,hour. Spark Streaming lets you do both. Cassandra
    in particular is excellent for time series data, working with raw data, transformations
    with Spark to aggregate data, and so forth. In some cases, using Spark with Cassandra
    (and the right data model) reduces the number of Spark transformations necessary
    on your data because Cassandra does that for you in its cluster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多时间序列数据的变体。有些可以在流中进行窗口化处理，其他一些则不能在流中进行窗口化处理，因为查询不是按时间切片而是按特定年份、月份、日期、小时进行的。Spark
    Streaming可以同时执行这两种操作。特别是Cassandra非常适合处理时间序列数据，与Spark一起使用原始数据、通过Spark进行数据聚合等。在某些情况下，使用Spark与Cassandra（和合适的数据模型）可以减少对数据的Spark转换次数，因为Cassandra会在其集群中为您执行这些操作。
- en: When using Apache Spark & Apache Cassandra together, it is best practice to
    co-locate Spark and Cassandra nodes for data-locality and decreased network calls,
    resulting in overall reduced latency.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Apache Spark和Apache Cassandra时，最佳实践是将Spark和Cassandra节点放置在一起以获得数据局部性和减少网络调用，从而降低总体延迟。
- en: Setup
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: '[Download and install the latest Cassandra release](http://cassandra.apache.org/download/)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[下载并安装最新的Cassandra发布版](http://cassandra.apache.org/download/)'
- en: '**Configuration Step:** Modify apache-cassandra-{latest.version}/conf/cassandra.yaml
    to increase batch_size_warn_threshold_in_kb to 64.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置步骤：** 修改apache-cassandra-{最新版本}/conf/cassandra.yaml以将batch_size_warn_threshold_in_kb增加到64。'
- en: Start Cassandra.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Cassandra。
- en: '[PRE0]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Note:** If you get an error - you may need to prepend with sudo, or chown
    /var/lib/cassandra.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意：** 如果您收到错误消息 - 您可能需要以sudo为前缀，或者chown /var/lib/cassandra。'
- en: 'Run the setup cql scripts to create the schema and populate the weather stations
    table. Go to the timeseries data folder and start a cqlsh sell there:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行设置cql脚本以创建模式并填充天气站表。进入timeseries数据文件夹并在那里启动一个cqlsh shell：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会看到：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then run the script:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后运行脚本：
- en: cqlsh> source 'create-timeseries.cql'; cqlsh> quit;
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: cqlsh> source 'create-timeseries.cql'; cqlsh> quit;
- en: '[See this Github repo to find out more about the weather stations table data.](https://github.com/killrweather/killrweather/wiki/2.-Code-and-Data-Setup#data-setup)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看此 GitHub 仓库以了解更多关于天气站表数据的信息。](https://github.com/killrweather/killrweather/wiki/2.-Code-and-Data-Setup#data-setup)'
- en: Running the WeatherApp and WeatherClientApp
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行WeatherApp和WeatherClientApp
- en: To Run from an IDE
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从IDE中运行
- en: Start com.databricks.apps.WeatherApp
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动com.databricks.apps.WeatherApp
- en: Then start com.databricks.apps.WeatherClientApp
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后启动com.databricks.apps.WeatherClientApp
- en: To Run from Command Line
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从命令行运行
- en: Use SBT to run the app.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SBT运行应用程序。
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会看到：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Select option 1 to open the weather app.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择选项1打开天气应用程序。
- en: Use SBT to run the client app. Run the same commands above, but select option
    2.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SBT运行客户端应用程序。运行上述相同的命令，但选择选项2。
- en: About The Time Series Data Model
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于时间序列数据模型
- en: '[See this github repo to find out more about the Time Series Data Model](https://github.com/killrweather/killrweather/wiki/4.-Time-Series-Data-Model)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看此 GitHub 仓库以了解更多关于时间序列数据模型的信息](https://github.com/killrweather/killrweather/wiki/4.-Time-Series-Data-Model)'
