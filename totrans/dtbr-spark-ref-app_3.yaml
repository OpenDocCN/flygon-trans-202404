- en: Weather TimeSeries Data Application with Cassandra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weather TimeSeries Data Application with Cassandra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project demonstrates how to easily leverage and integrate Apache Spark,
    Spark Streaming, Apache Cassandra with the Spark Cassandra Connector and Apache
    Kafka in general, and more specifically for time series data. It also demonstrates
    how to do this in an asynchronous Akka event-driven environment. We use weather
    data and the existing hourly data format as the sample domain.
  prefs: []
  type: TYPE_NORMAL
- en: Time Series Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use of time series data for business analysis is not new. What is new is
    the ability to collect and analyze massive volumes of data in sequence at extremely
    high velocity to get the clearest picture to predict and forecast future market
    changes, user behavior, environmental conditions, resource consumption, health
    trends and much, much more.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Cassandra is a NoSQL database platform particularly suited for these
    types of Big Data challenges. Cassandra’s data model is an excellent fit for handling
    data in sequence regardless of data type or size. When writing data to Cassandra,
    data is sorted and written sequentially to disk. When retrieving data by row key
    and then by range, you get a fast and efficient access pattern due to minimal
    disk seeks – time series data is an excellent fit for this type of pattern. Apache
    Cassandra allows businesses to identify meaningful characteristics in their time
    series data as fast as possible to make clear decisions about expected future
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: There are many flavors of time series data. Some can be windowed in the stream,
    others can not be windowed in the stream because queries are not by time slice
    but by specific year,month,day,hour. Spark Streaming lets you do both. In some
    cases, such as in the KafkaStreamingActor, using Spark with Cassandra (and the
    right data model) reduces the number of Spark transformations necessary on your
    data because Cassandra does the work for you in its cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WeatherApp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the primary compute application that processes the raw hourly data
    from the Kafka stream. As soon as each hourly raw data is received on the particular
    Kafka topic, it is mapped to a custom case class. From there, two initial actions
    are done:'
  prefs: []
  type: TYPE_NORMAL
- en: The raw hourly data by weather station ID is stored in Cassandra for use by
    any other processes that want it in its raw state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The daily precipitation aggregate (0 to 23 hours of data) is computed and updated
    in a daily_aggregate_precipitation table in Cassandra. We can do precipitation
    in the stream because of the data model created and a Cassandra Counter used.
    Whereas aggregation of temperature require more input data so this is done by
    request with a station ID, year, month and day. * With things like daily precipitation
    and temperature aggregates pre-calculated and stored, any requests for annual
    aggregates, high-low or topK for example, can be easily computed via calls to
    specific Akka Actors handling each aggregation type (precip, temp, etc). These
    read the aggregated and persisted data from Cassandra and runs the computation
    through Spark. Data is returned to the requester in a Future - no threads are
    blocked. [Note, when in master with the correct URL of specific files available
    I can link them above so that a user can click on a specific block of code]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WeatherClient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Represents a separately-deployed process that would typically be feeding data
    to Kafka and other applications or processes that would be sending data aggregation
    requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulates real-time raw weather events received from a separate process which
    this sends to Kafka. It does this as a feed vs bursts on startup. The FileFeedActor
    in the client receives data per file, parses it to each raw data entry, and pipes
    the data through Akka Streams to publish to Kafka and handle life cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises the weather app by sending requests for various aggregated data (topk,
    high-low, annual…) every n-seconds via the WeatherApiQueries Actor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture and Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Asynchronous Fault Tolerant Data Pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Raw hourly data for each weather station is ingested and published to a Kafka
    topic. This would be distributed on Kafka nodes in each Data Center where the
    Spark-Cassandra application are also deployed, as are the co-located Spark and
    Cassandra nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Time Series data App diagram](timeseries.png)'
  prefs: []
  type: TYPE_IMG
- en: Data is streamed from the Kafka nodes to the Spark Nodes for parallelized distributed
    data computation. The raw data is initially saved to a Cassandra keyspace and
    table. This stream from Kafka is then used for daily aggregation work in Spark,
    which is then persisted to several Cassandra daily aggregate tables. Now, future
    requests for data based on these daily aggregates (temperature, precipitation..)
    can now more quickly be computed. For example, requests for topK, high-low or
    annual precipitation for a specific weather station in a specific year (or year,
    month, day for high-low/topk) do not need to get the raw data but can start on
    the already aggregated data via Spark, now available on demand in Cassandra (replicated,
    fault tolerant and distributed across data centers).
  prefs: []
  type: TYPE_NORMAL
- en: Running the Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running the Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many flavors of time series data. Some can be windowed in the stream,
    others can not be windowed in the stream because queries are not by time slice
    but by specific year,month,day,hour. Spark Streaming lets you do both. Cassandra
    in particular is excellent for time series data, working with raw data, transformations
    with Spark to aggregate data, and so forth. In some cases, using Spark with Cassandra
    (and the right data model) reduces the number of Spark transformations necessary
    on your data because Cassandra does that for you in its cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When using Apache Spark & Apache Cassandra together, it is best practice to
    co-locate Spark and Cassandra nodes for data-locality and decreased network calls,
    resulting in overall reduced latency.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Download and install the latest Cassandra release](http://cassandra.apache.org/download/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Configuration Step:** Modify apache-cassandra-{latest.version}/conf/cassandra.yaml
    to increase batch_size_warn_threshold_in_kb to 64.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Start Cassandra.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Note:** If you get an error - you may need to prepend with sudo, or chown
    /var/lib/cassandra.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the setup cql scripts to create the schema and populate the weather stations
    table. Go to the timeseries data folder and start a cqlsh sell there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then run the script:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: cqlsh> source 'create-timeseries.cql'; cqlsh> quit;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[See this Github repo to find out more about the weather stations table data.](https://github.com/killrweather/killrweather/wiki/2.-Code-and-Data-Setup#data-setup)'
  prefs: []
  type: TYPE_NORMAL
- en: Running the WeatherApp and WeatherClientApp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To Run from an IDE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start com.databricks.apps.WeatherApp
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then start com.databricks.apps.WeatherClientApp
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To Run from Command Line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use SBT to run the app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Select option 1 to open the weather app.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use SBT to run the client app. Run the same commands above, but select option
    2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: About The Time Series Data Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[See this github repo to find out more about the Time Series Data Model](https://github.com/killrweather/killrweather/wiki/4.-Time-Series-Data-Model)'
  prefs: []
  type: TYPE_NORMAL
