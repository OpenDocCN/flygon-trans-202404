["```\n# Lab 7 Learning rate and Evaluation\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [[1, 2, 1],\n          [1, 3, 2],\n          [1, 3, 4],\n          [1, 5, 5],\n          [1, 7, 5],\n          [1, 2, 5],\n          [1, 6, 6],\n          [1, 7, 7]]\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\n# Evaluation our model using this test dataset\nx_test = [[2, 1, 1],\n          [3, 1, 2],\n          [3, 3, 4]]\ny_test = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1]]\n\nX = tf.placeholder(\"float\", [None, 3])\nY = tf.placeholder(\"float\", [None, 3])\n\nW = tf.Variable(tf.random_normal([3, 3]))\nb = tf.Variable(tf.random_normal([3]))\n\n# tf.nn.softmax computes softmax activations\n# softmax = exp(logits) / reduce_sum(exp(logits), dim)\nhypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n\n# Cross entropy cost/loss\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n# Try to change learning_rate to small numbers\noptimizer = tf.train.GradientDescentOptimizer(\n    learning_rate=1e-10).minimize(cost)\n\n# Correct prediction Test model\nprediction = tf.arg_max(hypothesis, 1)\nis_correct = tf.equal(prediction, tf.arg_max(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(201):\n        cost_val, W_val, _ = sess.run(\n            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n        print(step, cost_val, W_val)\n\n    # predict\n    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n    # Calculate the accuracy\n    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n\n'''\nwhen lr = 1.5\n0 5.73203 [[-0.30548954  1.22985029 -0.66033536]\n [-4.39069986  2.29670858  2.99386835]\n [-3.34510708  2.09743214 -0.80419564]]\n1 23.1494 [[ 0.06951046  0.29449689 -0.0999819 ]\n [-1.95319986 -1.63627958  4.48935604]\n [-0.90760708 -1.65020132  0.50593793]]\n2 27.2798 [[ 0.44451016  0.85699677 -1.03748143]\n [ 0.48429942  0.98872018 -0.57314301]\n [ 1.52989244  1.16229868 -4.74406147]]\n3 8.668 [[ 0.12396193  0.61504567 -0.47498202]\n [ 0.22003263 -0.2470119   0.9268558 ]\n [ 0.96035379  0.41933775 -3.43156195]]\n4 5.77111 [[-0.9524312   1.13037777  0.08607888]\n [-3.78651619  2.26245379  2.42393875]\n [-3.07170963  3.14037919 -2.12054014]]\n5 inf [[ nan  nan  nan]\n [ nan  nan  nan]\n [ nan  nan  nan]]\n6 nan [[ nan  nan  nan]\n [ nan  nan  nan]\n [ nan  nan  nan]]\n\n ...\nPrediction: [0 0 0]\nAccuracy:  0.0\n\n-------------------------------------------------\nWhen lr = 1e-10\n0 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n1 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n2 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n...\n\n198 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n199 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n200 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\nPrediction: [0 0 0]\nAccuracy:  0.0\n-------------------------------------------------\nWhen lr = 0.1\n\n0 5.73203 [[ 0.72881663  0.71536207 -1.18015325]\n [-0.57753736 -0.12988332  1.60729778]\n [ 0.48373488 -0.51433605 -2.02127004]]\n1 3.318 [[ 0.66219079  0.74796319 -1.14612854]\n [-0.81948912  0.03000021  1.68936598]\n [ 0.23214608 -0.33772916 -1.94628811]]\n2 2.0218 [[ 0.64342022  0.74127686 -1.12067163]\n [-0.81161296 -0.00900121  1.72049117]\n [ 0.2086665  -0.35079569 -1.909742  ]]\n\n...\n\n199 0.672261 [[-1.15377033  0.28146935  1.13632679]\n [ 0.37484586  0.18958236  0.33544877]\n [-0.35609841 -0.43973011 -1.25604188]]\n200 0.670909 [[-1.15885413  0.28058422  1.14229572]\n [ 0.37609792  0.19073224  0.33304682]\n [-0.35536593 -0.44033223 -1.2561723 ]]\nPrediction: [2 2 2]\nAccuracy:  1.0\n''' \n```"]