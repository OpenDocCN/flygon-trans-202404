["```\n # Original source from\n# https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch.nn.functional as F\nimport numpy as np\nimport itertools\n\ndef flatten(l):\n    return list(itertools.chain.from_iterable(l))\n\nseqs = ['ghatmasala', 'nicela', 'chutpakodas']\n\n# make <pad> idx 0\nvocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n\n# make model\nembedding_size = 3\nembed = nn.Embedding(len(vocab), embedding_size)\nlstm = nn.LSTM(embedding_size, 5)\n\nvectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\nprint(\"vectorized_seqs\", vectorized_seqs)\n\nprint([x for x in map(len, vectorized_seqs)])\n# get the length of each seq in your batch\nseq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n\n# dump padding everywhere, and place seqs on the left.\n# NOTE: you only need a tensor as big as your longest sequence\nseq_tensor = Variable(torch.zeros(\n    (len(vectorized_seqs), seq_lengths.max()))).long()\nfor idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n\nprint(\"seq_tensor\", seq_tensor)\n\n# SORT YOUR TENSORS BY LENGTH!\nseq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\nseq_tensor = seq_tensor[perm_idx]\n\nprint(\"seq_tensor after sorting\", seq_tensor)\n\n# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n# Otherwise, give (L,B,D) tensors\nseq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\nprint(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n\n# embed your sequences\nembeded_seq_tensor = embed(seq_tensor)\nprint(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n\n# pack them up nicely\npacked_input = pack_padded_sequence(\n    embeded_seq_tensor, seq_lengths.cpu().numpy())\n\n# throw them through your LSTM (remember to give batch_first=True here if\n# you packed with it)\npacked_output, (ht, ct) = lstm(packed_input)\n\n# unpack your output if required\noutput, _ = pad_packed_sequence(packed_output)\nprint(\"Lstm output\", output.size(), output.data)\n\n# Or if you just want the final hidden state?\nprint(\"Last output\", ht[-1].size(), ht[-1].data) \n```"]