["```\n% Some tests on Gradient descent\n%% Define parameters start at 3.5\nx_old=3.5; alpha=0.01; precision=0.0001;\n\n%% Define function\nx_input = [-1:0.01:3.5];\nf = @(x) x.^4 - 3*x.^3 + 2;\ndf = @(x) 4*x.^3 - 9*x.^2;\ny_output = f(x_input);\nplot(x_input, y_output);\n\n%% Gradient descent algorithm\n% Keep repeating until convergence\nwhile 1\n    % Evalulate gradients\n    tmpDelta = x_old - alpha*(df(x_old));    \n    % Check Convergence\n    diffOldTmp = abs(tmpDelta - x_old);\n    if diffOldTmp < precision\n        break;\n    end\n    % Update parameters\n    x_old = tmpDelta;     \nend\nfprintf('The local minimum is at %d\\n', x_old); \n```", "```\nTheta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\nTheta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\nTheta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON; \n```", "```\ndef rnn_step_forward(x, prev_h, Wx, Wh, b):\n  # We separate on steps to make the backpropagation easier\n  #forward pass in steps\n  # step 1\n  xWx = np.dot(x, Wx)\n\n  # step 2\n  phWh = np.dot(prev_h,Wh)\n\n  # step 3\n  # total\n  affine = xWx + phWh + b.T\n\n  # step 4\n  next_h = np.tanh(t)\n\n  # Cache iputs, state, and weights\n  # we are having prev_h.copy() since python params are pass by reference.\n  cache = (x, prev_h.copy(), Wx, Wh, next_h, affine)\n\n  return next_h, cache \n```", "```\ndef rnn_step_backward(dnext_h, cache):\n    (x, prev_h, Wx, Wh, next_h, affine) = cache\n\n    #backward in step\n    # step 4\n    # dt delta of total\n    # Gradient of tanh times dnext_h\n    dt = (1 - np.square(np.tanh(affine))) * (dnext_h)\n\n    # step 3\n    # Gradient of sum block\n    dxWx = dt\n    dphWh = dt\n    db = np.sum(dt, axis=0)\n\n    # step 2\n    # Gradient of the mul block\n    dWh = prev_h.T.dot(dphWh)\n    dprev_h = Wh.dot(dphWh.T).T\n\n    # step 1\n    # Gradient of the mul block\n    dx = dxWx.dot(Wx.T)\n    dWx = x.T.dot(dxWx)\n\n    return dx, dprev_h, dWx, dWh, db \n```", "```\ndef rnn_forward(x, h0, Wx, Wh, b):\n  \"\"\"\n  Run a vanilla RNN forward on an entire sequence of data. We assume an input\n  sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n  size of H, and we work over a minibatch containing N sequences. After running\n  the RNN forward, we return the hidden states for all timesteps.\n\n  Inputs:\n  - x: Input data for the entire timeseries, of shape (N, T, D).\n  - h0: Initial hidden state, of shape (N, H)\n  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n  - b: Biases of shape (H,)\n\n  Returns a tuple of:\n  - h: Hidden states for the entire timeseries, of shape (N, T, H).\n  - cache: Values needed in the backward pass\n  \"\"\"\n\n  # Get shapes\n  N, T, D = x.shape\n  # Initialization\n  h, cache = None, None\n  H = h0.shape[1]\n  h = np.zeros((N,T,H))\n\n  # keeping the inital value in the last element\n  # it will be overwritten\n  h[:,-1,:] = h0\n  cache = []\n\n  # For each time-step\n  for t in xrange(T):\n    h[:,t,:], cache_step = rnn_step_forward(x[:,t,:], h[:,t-1,:], Wx, Wh, b)\n    cache.append(cache_step)\n\n  # Return current state and cache\n  return h, cache \n```", "```\ndef rnn_backward(dh, cache):\n  \"\"\"\n  Compute the backward pass for a vanilla RNN over an entire sequence of data.\n\n  Inputs:\n  - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n\n  Returns a tuple of:\n  - dx: Gradient of inputs, of shape (N, T, D)\n  - dh0: Gradient of initial hidden state, of shape (N, H)\n  - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n  - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n  - db: Gradient of biases, of shape (H,)\n  \"\"\"\n  dx, dh0, dWx, dWh, db = None, None, None, None, None\n  # Get shapes\n  N,T,H = dh.shape\n  D = cache[0][0].shape[1] # D taken from x in cache\n\n  # Initialization keeping the gradients with the same shape it's respective inputs/weights\n  dx, dprev_h = np.zeros((N, T, D)),np.zeros((N, H))\n  dWx, dWh, db = np.zeros((D, H)), np.zeros((H, H)), np.zeros((H,))\n  dh = dh.copy()\n\n  # For each time-step\n  for t in reversed(xrange(T)):\n    dh[:,t,:]  += dprev_h # updating the previous layer dh\n    dx_, dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dh[:,t,:], cache[t])\n    # Observe that we sum each time-step gradient\n    dx[:,t,:] += dx_\n    dWx += dWx_\n    dWh += dWh_\n    db += db_\n\n  dh0 = dprev_h\n\n  return dx, dh0, dWx, dWh, db \n```", "```\ndef lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n  N,H = prev_c.shape\n\n  #forward pass in steps\n  # step 1: calculate activation vector\n  a = np.dot(x, Wx) + np.dot(prev_h,Wh) + b.T\n\n  # step 2: input gate\n  a_i = sigmoid(a[:,0:H])\n\n  # step 3: forget gate\n  a_f = sigmoid(a[:,H:2*H])\n\n  # step 4: output gate\n  a_o = sigmoid(a[:,2*H:3*H])\n\n  # step 5: block input gate\n  a_g= np.tanh(a[:,3*H:4*H])\n\n  # step 6: next cell state\n  next_c = a_f * prev_c +  a_i * a_g\n\n  # step 7: next hidden state\n  next_h = a_o * np.tanh(next_c)\n\n  # we are having *.copy() since python params are pass by reference.\n  cache = (x, prev_h.copy(), prev_c.copy(), a, a_i, a_f, a_o, a_g, next_h, next_c, Wx, Wh)\n\n  return next_h, next_c, cache \n```", "```\ndef lstm_step_backward(dnext_h, dnext_c, cache):\n  (x, prev_h, prev_c, a, a_i, a_f, a_o, a_g, next_h, next_c, Wx, Wh) = cache\n  N,H = dnext_h.shape\n  da = np.zeros(a.shape)\n\n  # step 7:\n  dnext_c = dnext_c.copy()\n  dnext_c += dnext_h * a_o * (1 - np.tanh(next_c) ** 2)\n  da_o = np.tanh(next_c) * dnext_h\n\n  # step 6:\n  da_f    = dnext_c * prev_c\n  dprev_c = dnext_c * a_f\n  da_i    = dnext_c * a_g\n  da_g    = dnext_c * a_i\n\n  # step 5:\n  da[:,3*H:4*H] = (1 - np.square(a_g)) * da_g\n\n  # step 4:\n  da[:,2*H:3*H] = (1 - a_o) * a_o * da_o\n\n  # step 3:\n  da[:,H:2*H] = (1 - a_f) * a_f * da_f\n\n  # step 2:\n  da[:,0:H] = (1 - a_i) * a_i * da_i\n\n  # step 1:\n  db = np.sum(da, axis=0)\n  dx = da.dot(Wx.T)\n  dWx = x.T.dot(da)\n  dprev_h = da.dot(Wh.T)\n  dWh = prev_h.T.dot(da)\n\n  return dx, dprev_h, dprev_c, dWx, dWh, db \n```", "```\n>> x\n\nx(:,:,1) =\n\n     1     1     0     2     0\n     2     2     2     2     1\n     0     0     0     2     1\n     2     2     2     2     1\n     2     0     2     2     1\n\nx(:,:,2) =\n\n     2     1     0     0     0\n     0     2     0     1     0\n     1     0     1     2     0\n     1     2     0     2     1\n     1     2     1     2     2\n\nx(:,:,3) =\n\n     2     1     1     2     2\n     1     1     1     0     0\n     2     0     1     0     2\n     0     2     0     2     1\n     0     0     2     1     0\n\n>> padarray(x,[1 1])\n\nans(:,:,1) =\n\n     0     0     0     0     0     0     0\n     0     1     1     0     2     0     0\n     0     2     2     2     2     1     0\n     0     0     0     0     2     1     0\n     0     2     2     2     2     1     0\n     0     2     0     2     2     1     0\n     0     0     0     0     0     0     0\n\nans(:,:,2) =\n\n     0     0     0     0     0     0     0\n     0     2     1     0     0     0     0\n     0     0     2     0     1     0     0\n     0     1     0     1     2     0     0\n     0     1     2     0     2     1     0\n     0     1     2     1     2     2     0\n     0     0     0     0     0     0     0\n\nans(:,:,3) =\n\n     0     0     0     0     0     0     0\n     0     2     1     1     2     2     0\n     0     1     1     1     0     0     0\n     0     2     0     1     0     2     0\n     0     0     2     0     2     1     0\n     0     0     0     2     1     0     0\n     0     0     0     0     0     0     0 \n```", "```\n%% Convolution n dimensions\n% The following code is just a extension of conv2d_vanila for n dimensions.\n% Parameters:\n% Input: H x W x depth\n% K: kernel F x F x depth\n% S: stride (How many pixels he window will slide on the input)\n% This implementation is like the 'valid' parameter on normal convolution\n\nfunction outConv = convn_vanilla(input, kernel, S)\n% Get the input size in terms of rows and cols. The weights should have\n% same depth as the input volume(image)\n[rowsIn, colsIn, ~] = size(input);\n\n% Get volume dimensio\ndepthInput = ndims(input);\n\n% Get the kernel size, considering a square kernel always\nF = size(kernel,1);\n\n%% Initialize outputs\nsizeRowsOut = ((rowsIn-F)/S) + 1;\nsizeColsOut = ((colsIn-F)/S) + 1;\noutConvAcc = zeros(sizeRowsOut , sizeColsOut, depthInput);\n\n%% Do the convolution\n% Convolve each channel on the input with it's respective kernel channel,\n% at the end sum all the channel results.\nfor depth=1:depthInput\n    % Select input and kernel current channel\n    inputCurrDepth = input(:,:,depth);\n    kernelCurrDepth = kernel(:,:,depth);\n    % Iterate on every row and col, (using stride)\n    for r=1:S:(rowsIn-1)\n        for c=1:S:(colsIn-1)\n            % Avoid sampling out of the image.\n            if (((c+F)-1) <= colsIn) && (((r+F)-1) <= rowsIn)\n                % Select window on input volume (patch)\n                sampleWindow = inputCurrDepth(r:(r+F)-1,c:(c+F)-1);\n                % Do the dot product\n                dotProd = sum(sampleWindow(:) .* kernelCurrDepth(:));\n                % Store result\n                outConvAcc(ceil(r/S),ceil(c/S),depth) = dotProd;\n            end\n        end\n    end\nend\n\n% Sum elements over the input volume dimension (sum all the channels)\noutConv = sum(outConvAcc,depthInput);\nend \n```", "```\n[img_height, img_width, img_channels] = size(img);\nnewImgHeight = floor(((img_height + 2*P - ksize) / S)+1);\nnewImgWidth = floor(((img_width + 2*P - ksize) / S)+1);        \ncols = single(zeros((img_channels*ksize*ksize),(newImgHeight * newImgWidth))); \n```", "```\nfunction [activations] = ForwardPropagation(obj, input, weights, bias)\n    % Tensor format (rows,cols,channels, batch) on matlab\n    [H,W,~,N] = size(input);\n    [HH,WW,C,F] = size(weights);\n\n    % Calculate output sizes\n    H_prime = (H+2*obj.m_padding-HH)/obj.m_stride +1;\n    W_prime = (W+2*obj.m_padding-WW)/obj.m_stride +1;\n\n    % Alocate memory for output\n    activations = zeros([H_prime,W_prime,F,N]);\n\n    % Preparing filter weights\n    filter_col = reshape(weights,[HH*WW*C F]);\n    filter_col_T = filter_col';\n\n    % Preparing bias\n    if ~isempty(bias)\n        bias_m = repmat(bias,[1 H_prime*W_prime]);\n    else\n        b = zeros(size(filter_col,1),1);\n        bias_m = repmat(b,[1 H_prime*W_prime]);\n    end\n\n    % Here we convolve each image on the batch in a for-loop, but the im2col\n    % could also handle a image batch at the input, so all computations would\n    % be just one big matrix multiplication. We opted now for this to test the\n    % par-for implementation with OpenMP on CPU\n    for idxBatch = 1:N\n        im = input(:,:,:,idxBatch);    \n        im_col = im2col_ref(im,HH,WW,obj.m_stride,obj.m_padding,1);\n        mul = (filter_col_T * im_col) + bias_m;\n        activations(:,:,:,idxBatch) =  reshape_row_major(mul,[H_prime W_prime size(mul,1)]);                                                \n    end\n\n    % Cache results for backpropagation\n    obj.activations = activations;\n    obj.weights = weights;\n    obj.biases = bias;\n    obj.previousInput = input;            \nend \n```", "```\nfunction [gradient] = BackwardPropagation(obj, dout)\n    dout = dout.input;  \n    [H,W,~,N] = size(obj.previousInput);\n    [HH,WW,C,F] = size(obj.weights);  \n\n    % Calculate output sizes\n    H_prime = (H+2*obj.m_padding-HH)/obj.m_stride +1;\n    W_prime = (W+2*obj.m_padding-WW)/obj.m_stride +1;\n\n    % Preparing filter weights \n    filter_col_T = reshape_row_major(obj.weights,[F HH*WW*C]);                                                \n\n    % Initialize gradients\n    dw = zeros(size(obj.weights));\n    dx = zeros(size(obj.previousInput));            \n    % Get the bias gradient which will be the sum of dout over the\n    % dimensions (batches(4), rows(1), cols(2))\n    db = sum(sum(sum(dout, 1), 2), 4);\n\n    for idxBatch = 1:N\n        im = obj.previousInput(:,:,:,idxBatch);    \n        im_col = im2col_ref(im,HH,WW,obj.m_stride,obj.m_padding,1);\n        dout_i = dout(:,:,:,idxBatch);\n\n        dout_i_reshaped = reshape_row_major(dout_i,[F, H*W]);                \n\n        dw_before_reshape = dout_i_reshaped * im_col';                \n        dw_i = reshape(dw_before_reshape',[HH, WW, C, F]);\n        dw = dw + dw_i;\n\n        % We now have the gradient just before the im2col\n        grad_before_im2col = (dout_i_reshaped' * filter_col_T);                \n        % Now we need to backpropagate im2col (im2col_back),\n        % results will padded by one always\n        dx_padded = im2col_back_ref(grad_before_im2col,H_prime, W_prime, obj.m_stride, HH, WW, C);                \n        % Now we need to take out the pading\n        dx(:,:,:,idxBatch) = dx_padded(2:end-1, 2:end-1,:);\n    end\n\n    %% Output gradients \n    gradient.bias = db;\n    gradient.input = dx;  \n    gradient.weight = dw;\n\nend \n```", "```\nfunction [ img_matrix ] = im2col_ref( inputImg, k_height, k_width, S , P, isConv )\n%IM2COL Convert image to a matrix, this step is used to accelerate\n%convolutions, implementing the convolution as a matrix multiplication\n% This version currently does not support batch of images, we choose this\n% because we're first going to use the CPU mode, and we want to relly on\n% parfor (OpenMP)\ncoder.extrinsic('warning')\n% Get Image dimensions\n[imgHeight, imgWidth, imgChannels] = size(inputImg);\n\n% Calculate convolved result size.\nnewImgHeight = ((imgHeight + 2*P - k_height) / S)+1;\nnewImgWidth = ((imgWidth + 2*P - k_width) / S)+1;\noffset_K_Height = 0;\noffset_K_Width = 0;\n\n% Check if it is a real number\nif rem(newImgHeight,1) ~= 0 || rem(newImgWidth,1) ~= 0\n    warning('warning: Invalid stride or pad for input\\n');\n    if isConv\n        % Convolution do a floor\n        newImgHeight = floor(((imgHeight + 2*P - k_height) / S)+1);\n        newImgWidth = floor(((imgWidth + 2*P - k_width) / S)+1);        \n    else\n        % Pooling do a ceil and adapt the sampling window\n        newImgHeight = ceil(((imgHeight + 2*P - k_height) / S)+1);\n        newImgWidth = ceil(((imgWidth + 2*P - k_width) / S)+1);\n        offset_K_Height = k_height - 1;\n        offset_K_Width = k_width - 1;\n    end\nend\n\n% Allocate output sizes\nimg_matrix = zeros(...\n    (imgChannels*k_height*k_width),(newImgHeight * newImgWidth) ...\n    );\n\n% Only pad if needed\nif P ~= 0\n    inputImg_Padded = padarray(inputImg,[P P]);\n    % Get dimensions again before iterate on padded image, otherwise we will\n    % keep sampling with the old (unpadded size)\n    [imgHeight, imgWidth, ~] = size(inputImg_Padded);\nend\n\n% Iterate on the input image like a convolution\ncont = 1;\nfor r=1:S:(imgHeight-offset_K_Height)\n    for c=1:S:(imgWidth-offset_K_Width)\n        % Avoid slide out of the image (Security buffer overflow)\n        if (((c+k_width)-1) <= imgWidth) && (((r+k_height)-1) <= imgHeight)\n            % Select window on input volume\n            if P == 0\n                patch = inputImg(r:(r+k_height)-1,c:(c+k_width)-1,:);\n            else\n                patch = inputImg_Padded(r:(r+k_height)-1,c:(c+k_width)-1,:);\n            end\n\n            % Convert patch to a col vector, the matlab reshape order is\n            % row major while other languages (C/C++, python) are col\n            % major, on this particular case (im2col, then matrix mult with\n            % the kernel) this order will not mather, but it's not allways\n            % true...\n            patchRow = reshape(patch,[],1);\n\n            % Append the transformed patch into the output matrix\n            img_matrix(:,cont) = patchRow;\n            cont = cont+1;\n        end\n    end\nend\n\nend \n```", "```\nfunction [ img_grad ] = im2col_back_ref( dout, dout_H, dout_W, S, HH, WW, C )\n    %IM2COL_BACK_REF Backpropagation of im2col\n    % dout: (\n    % Return\n    % Image gradient (H,W,C)\n\n    % Calculate the spatial dimensions of im_grad\n    % Observe that the result will be \"padded\"\n    H = (dout_H - 1) * S + HH;\n    W = (dout_W - 1) * S + WW;\n\n    img_grad = zeros(H,W,C);\n\n    for ii=1:(dout_H*dout_W)\n        row = dout(ii,:);\n\n        % Create a patch from the row\n        patch = reshape_row_major(row,[HH WW C]);\n        %patch = reshape(row,[HH WW C]);\n\n        % Calculate indexes on dx\n        h_start = floor(((ii-1) / dout_W) * S);    \n        w_start = mod((ii-1),dout_W) * S;\n        h_start = h_start + 1;\n        w_start = w_start + 1;\n\n        img_grad(h_start:h_start+HH-1, w_start:w_start+WW-1, :) = img_grad(h_start:h_start+HH-1, w_start:w_start+WW-1, :) + patch;    \n    end    \nend \n```", "```\ndef conv_forward_naive(x, w, b, conv_param):\n  \"\"\"\n  A naive implementation of the forward pass for a convolutional layer.\n\n  The input consists of N data points, each with C channels, height H and width\n  W. We convolve each input with F different filters, where each filter spans\n  all C channels and has height HH and width HH.\n\n  Input:\n  - x: Input data of shape (N, C, H, W)\n  - w: Filter weights of shape (F, C, HH, WW)\n  - b: Biases, of shape (F,)\n  - conv_param: A dictionary with the following keys:\n    - 'stride': The number of pixels between adjacent receptive fields in the\n      horizontal and vertical directions.\n    - 'pad': The number of pixels that will be used to zero-pad the input.\n\n  Returns a tuple of:\n  - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n    H' = 1 + (H + 2 * pad - HH) / stride\n    W' = 1 + (W + 2 * pad - WW) / stride\n  - cache: (x, w, b, conv_param)\n  \"\"\"\n  out = None\n  pad_num = conv_param['pad']\n  stride = conv_param['stride']\n  N,C,H,W = x.shape\n  F,C,HH,WW = w.shape\n  H_prime = (H+2*pad_num-HH) // stride + 1\n  W_prime = (W+2*pad_num-WW) // stride + 1\n  out = np.zeros([N,F,H_prime,W_prime])\n  #im2col\n  for im_num in range(N):\n      im = x[im_num,:,:,:]\n      im_pad = np.pad(im,((0,0),(pad_num,pad_num),(pad_num,pad_num)),'constant')\n      im_col = im2col(im_pad,HH,WW,stride)\n      filter_col = np.reshape(w,(F,-1))\n      mul = im_col.dot(filter_col.T) + b\n      out[im_num,:,:,:] = col2im(mul,H_prime,W_prime,1)\n  cache = (x, w, b, conv_param)\n  return out, cache \n```", "```\ndef conv_backward_naive(dout, cache):\n  \"\"\"\n  A naive implementation of the backward pass for a convolutional layer.\n\n  Inputs:\n  - dout: Upstream derivatives.\n  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n\n  Returns a tuple of:\n  - dx: Gradient with respect to x\n  - dw: Gradient with respect to w\n  - db: Gradient with respect to b\n  \"\"\"\n  dx, dw, db = None, None, None\n\n  x, w, b, conv_param = cache\n  pad_num = conv_param['pad']\n  stride = conv_param['stride']\n  N,C,H,W = x.shape\n  F,C,HH,WW = w.shape\n  H_prime = (H+2*pad_num-HH) // stride + 1\n  W_prime = (W+2*pad_num-WW) // stride + 1\n\n  dw = np.zeros(w.shape)\n  dx = np.zeros(x.shape)\n  db = np.zeros(b.shape)\n\n  # We could calculate the bias by just summing over the right dimensions\n  # Bias gradient (Sum on dout dimensions (batch, rows, cols)\n  #db = np.sum(dout, axis=(0, 2, 3))\n\n  for i in range(N):\n      im = x[i,:,:,:]\n      im_pad = np.pad(im,((0,0),(pad_num,pad_num),(pad_num,pad_num)),'constant')\n      im_col = im2col(im_pad,HH,WW,stride)\n      filter_col = np.reshape(w,(F,-1)).T\n\n      dout_i = dout[i,:,:,:]\n      dbias_sum = np.reshape(dout_i,(F,-1))\n      dbias_sum = dbias_sum.T\n\n      #bias_sum = mul + b\n      db += np.sum(dbias_sum,axis=0)\n      dmul = dbias_sum\n\n      #mul = im_col * filter_col\n      dfilter_col = (im_col.T).dot(dmul)\n      dim_col = dmul.dot(filter_col.T)\n\n      dx_padded = col2im_back(dim_col,H_prime,W_prime,stride,HH,WW,C)\n      dx[i,:,:,:] = dx_padded[:,pad_num:H+pad_num,pad_num:W+pad_num]\n      dw += np.reshape(dfilter_col.T,(F,C,HH,WW))\n  return dx, dw, db \n```", "```\ndef im2col(x,hh,ww,stride):\n\n    \"\"\"\n    Args:\n      x: image matrix to be translated into columns, (C,H,W)\n      hh: filter height\n      ww: filter width\n      stride: stride\n    Returns:\n      col: (new_h*new_w,hh*ww*C) matrix, each column is a cube that will convolve with a filter\n            new_h = (H-hh) // stride + 1, new_w = (W-ww) // stride + 1\n    \"\"\"\n\n    c,h,w = x.shape\n    new_h = (h-hh) // stride + 1\n    new_w = (w-ww) // stride + 1\n    col = np.zeros([new_h*new_w,c*hh*ww])\n\n    for i in range(new_h):\n       for j in range(new_w):\n           patch = x[...,i*stride:i*stride+hh,j*stride:j*stride+ww]\n           col[i*new_w+j,:] = np.reshape(patch,-1)\n    return col \n```", "```\ndef col2im(mul,h_prime,w_prime,C):\n    \"\"\"\n      Args:\n      mul: (h_prime*w_prime*w,F) matrix, each col should be reshaped to C*h_prime*w_prime when C>0, or h_prime*w_prime when C = 0\n      h_prime: reshaped filter height\n      w_prime: reshaped filter width\n      C: reshaped filter channel, if 0, reshape the filter to 2D, Otherwise reshape it to 3D\n    Returns:\n      if C == 0: (F,h_prime,w_prime) matrix\n      Otherwise: (F,C,h_prime,w_prime) matrix\n    \"\"\"\n    F = mul.shape[1]\n    if(C == 1):\n        out = np.zeros([F,h_prime,w_prime])\n        for i in range(F):\n            col = mul[:,i]\n            out[i,:,:] = np.reshape(col,(h_prime,w_prime))\n    else:\n        out = np.zeros([F,C,h_prime,w_prime])\n        for i in range(F):\n            col = mul[:,i]\n            out[i,:,:] = np.reshape(col,(C,h_prime,w_prime))\n\n    return out \n```", "```\ndef col2im_back(dim_col,h_prime,w_prime,stride,hh,ww,c):\n    \"\"\"\n    Args:\n      dim_col: gradients for im_col,(h_prime*w_prime,hh*ww*c)\n      h_prime,w_prime: height and width for the feature map\n      strid: stride\n      hh,ww,c: size of the filters\n    Returns:\n      dx: Gradients for x, (C,H,W)\n    \"\"\"\n    H = (h_prime - 1) * stride + hh\n    W = (w_prime - 1) * stride + ww\n    dx = np.zeros([c,H,W])\n    for i in range(h_prime*w_prime):\n        row = dim_col[i,:]\n        h_start = (i / w_prime) * stride\n        w_start = (i % w_prime) * stride\n        dx[:,h_start:h_start+hh,w_start:w_start+ww] += np.reshape(row,(c,hh,ww))\n    return dx \n```"]