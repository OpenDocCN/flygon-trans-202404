- en: '[Map](data_mining_map.htm) > [Problem Definition](problem_definition.htm) >
    [Data Preparation](data_preparation.htm) > [Data Exploration](data_exploration.htm)
    > [Modeling](modeling.htm) > Evaluation > [Deployment](model_deployment.htm)'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '[地图](data_mining_map.htm) > [问题定义](problem_definition.htm) > [数据准备](data_preparation.htm)
    > [数据探索](data_exploration.htm) > [建模](modeling.htm) > 评估 > [部署](model_deployment.htm)'
- en: Model Evaluation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Model Evaluation is an integral part of the model development process. It helps
    to find the best model that represents our data and how well the chosen model
    will work in the future. Evaluating model performance with the data used for training
    is not acceptable in data science because it can easily generate overoptimistic
    and overfitted models. There are two methods of evaluating models in data science,
    Hold-Out and Cross-Validation. To avoid overfitting, both methods use a test set
    (not seen by the model) to evaluate model performance.**Hold-Out**In this method,
    the mostly large dataset is *randomly* divided to three subsets:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估是模型开发过程中的一个重要部分。它有助于找到最能代表我们数据的模型，以及选择的模型在未来的工作效果如何。在数据科学中，使用训练数据来评估模型性能是不可接受的，因为这很容易生成过于乐观和过拟合的模型。在数据科学中有两种评估模型的方法，分别是留出法和交叉验证。为了避免过拟合，这两种方法都使用一个测试集（模型未见过的）来评估模型性能。**留出法**在这种方法中，通常大型数据集会被*随机*分成三个子集：
- en: '**Training set** is a subset of the dataset used to build predictive models.'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练集**是用于构建预测模型的数据集的子集。'
- en: '**Validation set** is a subset of the dataset used to assess the performance
    of model built in the training phase. It provides a test platform for fine tuning
    model''s parameters and selecting the best-performing model. Not all modeling
    algorithms need a validation set.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证集**是用于评估在训练阶段构建的模型性能的数据集的子集。它为微调模型参数和选择表现最佳的模型提供了一个测试平台。并非所有建模算法都需要验证集。'
- en: '**Test set** or unseen examples is a subset of the dataset to assess the likely
    future performance of a model. If a model fit to the training set much better
    than it fits the test set, overfitting is probably the cause.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试集**或未见过的示例是用于评估模型未来性能的数据集的子集。如果模型对训练集的拟合要比对测试集的拟合好得多，那么过拟合可能是原因。'
- en: '**Cross-Validation**When only a limited amount of data is available, to achieve
    an unbiased estimate of the model performance we use *k*-fold cross-validation.
    In *k*-fold cross-validation, we divide the data into *k* subsets of equal size.
    We build models *k* times, each time leaving out one of the subsets from training
    and use it as the test set. If *k* equals the sample size, this is called "leave-one-out".Model
    evaluation can be divided to two sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**当只有有限的数据可用时，为了获得模型性能的无偏估计，我们使用*k*折交叉验证。在*k*折交叉验证中，我们将数据分成*k*个大小相等的子集。我们构建模型*k*次，每次将一个子集从训练中排除并将其用作测试集。如果*k*等于样本大小，则称为“留一法”。模型评估可以分为两个部分：'
- en: '[Classification Evaluation](model_evaluation_c.htm)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类评估](model_evaluation_c.htm)'
- en: '[Regression Evaluation](model_evaluation_r.htm)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回归评估](model_evaluation_r.htm)'
