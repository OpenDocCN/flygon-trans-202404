["```\n # https://github.com/pytorch/examples/blob/master/mnist/main.py\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nbatch_size = 64\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/',\n                               train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='./mnist_data/',\n                              train=False,\n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.l1 = nn.Linear(784, 520)\n        self.l2 = nn.Linear(520, 320)\n        self.l3 = nn.Linear(320, 240)\n        self.l4 = nn.Linear(240, 120)\n        self.l5 = nn.Linear(120, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n        x = F.relu(self.l1(x))\n        x = F.relu(self.l2(x))\n        x = F.relu(self.l3(x))\n        x = F.relu(self.l4(x))\n        return self.l5(x)\n\nmodel = Net()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        # sum up batch loss\n        test_loss += criterion(output, target).data[0]\n        # get the index of the max\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\nfor epoch in range(1, 10):\n    train(epoch)\n    test() \n```"]