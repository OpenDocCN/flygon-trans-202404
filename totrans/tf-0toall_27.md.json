["```\n# http://blog.aloni.org/posts/backprop-with-tensorflow/\n# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\n# WIP\nimport tensorflow as tf\n\ntf.set_random_seed(777)  # reproducibility\n\n# tf Graph Input\nx_data = [[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]]\ny_data = [[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]]\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\n# Set wrong model weights\nW = tf.Variable(tf.truncated_normal([3, 1]))\nb = tf.Variable(5.)\n\n# Forward prop\nhypothesis = tf.matmul(X, W) + b\n\nprint(hypothesis.shape, Y.shape)\n\n# diff\nassert hypothesis.shape.as_list() == Y.shape.as_list()\ndiff = (hypothesis - Y)\n\n# Back prop (chain rule)\nd_l1 = diff\nd_b = d_l1\nd_w = tf.matmul(tf.transpose(X), d_l1)\n\nprint(X, d_l1, d_w)\n\n# Updating network using gradients\nlearning_rate = 1e-6\nstep = [\n    tf.assign(W, W - learning_rate * d_w),\n    tf.assign(b, b - learning_rate * tf.reduce_mean(d_b)),\n]\n\n# 7\\. Running and testing the training process\nRMSE = tf.reduce_mean(tf.square((Y - hypothesis)))\n\nsess = tf.InteractiveSession()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(10000):\n    print(i, sess.run([step, RMSE], feed_dict={X: x_data, Y: y_data}))\n\nprint(sess.run(hypothesis, feed_dict={X: x_data})) \n```"]