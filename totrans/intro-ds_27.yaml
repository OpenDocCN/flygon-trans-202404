- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Regression](regression.htm)
    > K Nearest Neighbors'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '[地图](data_mining_map.htm) > [数据科学](data_mining.htm) > [预测未来](predicting_the_future.htm)
    > [建模](modeling.htm) > [回归](regression.htm) > K 最近邻'
- en: K Nearest Neighbors - Regression
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K 最近邻 - 回归
- en: K nearest neighbors is a simple algorithm that stores all available cases and
    predict the numerical target based on a similarity measure (e.g., distance functions).
    KNN has been used in statistical estimation and pattern recognition already in
    the beginning of 1970�s as a non-parametric technique.  **Algorithm** A simple
    implementation of KNN regression is to calculate the average of the numerical
    target of the K nearest neighbors.  Another approach uses an inverse distance
    weighted average of the K nearest neighbors. KNN regression uses the same distance
    functions as KNN classification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: K 最近邻是一个简单的算法，它存储所有可用的情况并根据相似性度量（例如，距离函数）预测数值目标。KNN 已经在 1970 年代初就被用于统计估计和模式识别中，作为一种非参数技术。**算法**
    KNN 回归的一个简单实现是计算 K 最近邻的数值目标的平均值。另一种方法使用 K 最近邻的倒数距离加权平均值。KNN 回归使用与 KNN 分类相同的距离函数。
- en: '![](../Images/823cfbf8638e639628498c708e1847b6.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/823cfbf8638e639628498c708e1847b6.jpg)'
- en: The above three distance measures are only valid for continuous variables. In
    the case of categorical variables you must use the Hamming distance, which is
    a measure of the number of instances in which corresponding symbols are different
    in two strings of equal length.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 上述三种距离度量仅适用于连续变量。对于分类变量的情况，必须使用汉明距离，它是两个等长字符串中对应符号不同的实例数的度量。
- en: '![](../Images/15dce582d6676e4936ea3118e1c80eee.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15dce582d6676e4936ea3118e1c80eee.jpg)'
- en: Choosing the optimal value for K is best done by first inspecting the data.
    In general, a large K value is more precise as it reduces the overall noise; however,
    the compromise is that the distinct boundaries within the feature space are blurred.
    Cross-validation is another way to retrospectively determine a good K value by
    using an independent data set to validate your K value. The optimal K for most
    datasets is 10 or more. That produces much better results than 1-NN. *Example*:Consider
    the following data concerning House Price Index or HPI. Age and Loan are two numerical
    variables (predictors) and HPI is the numerical target.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过首先检查数据来选择最佳的 K 值是最好的方法。一般来说，较大的 K 值更精确，因为它减少了总体噪声；然而，这种妥协是特征空间中的明确边界变得模糊了。交叉验证是另一种通过使用独立数据集来回顾性地确定良好
    K 值的方法。大多数数据集的最佳 K 值为 10 或更多。这比 1-NN 产生了更好的结果。*示例*：考虑以下关于房价指数（HPI）的数据。年龄和贷款是两个数值变量（预测变量），HPI
    是数值目标。
- en: '![](../Images/95caeaf2c5de01519031f0d7581ec7e5.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95caeaf2c5de01519031f0d7581ec7e5.jpg)'
- en: We can now use the training set to classify an unknown case (Age=33 and Loan=$150,000)
    using Euclidean distance. If K=1 then the nearest neighbor is the last case in
    the training set with HPI=264.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用训练集使用欧几里得距离对一个未知情况（年龄=33，贷款=$150,000）进行分类。如果 K=1，则最近的邻居是训练集中 HPI=264
    的最后一个情况。
- en: D = Sqrt[(48-33)^2 + (142000-150000)^2] = 8000.01  >>  HPI = 264
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: D = Sqrt[(48-33)^2 + (142000-150000)^2] = 8000.01 >> HPI = 264
- en: By having K=3, the prediction for HPI is equal to the average of HPI for the
    top three neighbors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当 K=3 时，HPI 的预测值等于前三个邻居的 HPI 的平均值。
- en: HPI = (264+139+139)/3 = 180.7
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: HPI = (264+139+139)/3 = 180.7
- en: '**Standardized Distance**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化距离**'
- en: One major drawback in calculating distance measures directly from the training
    set is in the case where variables have different measurement scales or there
    is a mixture of numerical and categorical variables. For example, if one variable
    is based on annual income in dollars, and the other is based on age in years then
    income will have a much higher influence on the distance calculated. One solution
    is to standardize the training set as shown below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从训练集计算距离度量的一个主要缺点在于变量具有不同的测量尺度或者存在数值和分类变量混合的情况。例如，如果一个变量是以年收入美元为基础的，另一个变量是以年龄年为基础的，则收入将对计算的距离产生更高的影响。一个解决方案是将训练集标准化，如下所示。
- en: '![](../Images/4a7433543400be6e0840f1229da1e884.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a7433543400be6e0840f1229da1e884.jpg)'
- en: As mentioned in KNN Classification using the standardized distance on the same
    training set, the unknown case returned a different neighbor which is not a good
    sign of robustness.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如在同一训练集上使用标准化距离进行 KNN 分类所述，未知情况返回了一个不同的邻居，这不是健壮性的好迹象。
- en: '| [Exercise](knn_reg_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/KnnReg.txt)
    |  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| [练习](knn_reg_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/KnnReg.txt)
    |  |'
