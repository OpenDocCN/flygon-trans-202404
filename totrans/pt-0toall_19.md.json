["```\n # https://github.com/spro/practical-pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom text_loader import TextDataset\n\nhidden_size = 100\nn_layers = 3\nbatch_size = 1\nn_epochs = 100\nn_characters = 128  # ASCII\n\nclass RNN(nn.Module):\n\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    # This runs this one step at a time\n    # It's extremely slow, and please do not use in practice.\n    # We need to use (1) batch and (2) data parallelism\n    def forward(self, input, hidden):\n        embed = self.embedding(input.view(1, -1))  # S(=1) x I\n        embed = embed.view(1, 1, -1)  # S(=1) x B(=1) x I (embedding size)\n        output, hidden = self.gru(embed, hidden)\n        output = self.linear(output.view(1, -1))  # S(=1) x I\n        return output, hidden\n\n    def init_hidden(self):\n        if torch.cuda.is_available():\n            hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\n        else:\n            hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n\n        return Variable(hidden)\n\ndef str2tensor(string):\n    tensor = [ord(c) for c in string]\n    tensor = torch.LongTensor(tensor)\n\n    if torch.cuda.is_available():\n        tensor = tensor.cuda()\n\n    return Variable(tensor)\n\ndef generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n    hidden = decoder.init_hidden()\n    prime_input = str2tensor(prime_str)\n    predicted = prime_str\n\n    # Use priming string to \"build up\" hidden state\n    for p in range(len(prime_str) - 1):\n        _, hidden = decoder(prime_input[p], hidden)\n\n    inp = prime_input[-1]\n\n    for p in range(predict_len):\n        output, hidden = decoder(inp, hidden)\n\n        # Sample from the network as a multinomial distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n\n        # Add predicted character to string and use as next input\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n        inp = str2tensor(predicted_char)\n\n    return predicted\n\n# Train for a given src and target\n# It feeds single string to demonstrate seq2seq\n# It's extremely slow, and we need to use (1) batch and (2) data parallelism\n# http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.\n\ndef train_teacher_forching(line):\n    input = str2tensor(line[:-1])\n    target = str2tensor(line[1:])\n\n    hidden = decoder.init_hidden()\n    loss = 0\n\n    for c in range(len(input)):\n        output, hidden = decoder(input[c], hidden)\n        loss += criterion(output, target[c])\n\n    decoder.zero_grad()\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data[0] / len(input)\n\ndef train(line):\n    input = str2tensor(line[:-1])\n    target = str2tensor(line[1:])\n\n    hidden = decoder.init_hidden()\n    decoder_in = input[0]\n    loss = 0\n\n    for c in range(len(input)):\n        output, hidden = decoder(decoder_in, hidden)\n        loss += criterion(output, target[c])\n        decoder_in = output.max(1)[1]\n\n    decoder.zero_grad()\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data[0] / len(input)\n\nif __name__ == '__main__':\n\n    decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n    if torch.cuda.is_available():\n        decoder.cuda()\n\n    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loader = DataLoader(dataset=TextDataset(),\n                              batch_size=batch_size,\n                              shuffle=True)\n\n    print(\"Training for %d epochs...\" % n_epochs)\n    for epoch in range(1, n_epochs + 1):\n        for i, (lines, _) in enumerate(train_loader):\n            loss = train(lines[0])  # Batch size is 1\n\n            if i % 100 == 0:\n                print('[(%d %d%%) loss: %.4f]' %\n                      (epoch, epoch / n_epochs * 100, loss))\n                print(generate(decoder, 'Wh', 100), '\\n') \n```"]