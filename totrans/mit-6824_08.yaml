- en: Harp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '6.824 2015 Lecture 8: Harp'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Note:** These lecture notes were slightly modified from the ones posted on
    the 6.824 [course website](http://nil.csail.mit.edu/6.824/2015/schedule.html)
    from Spring 2015.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paper: [Replication in the Harp File System](papers/bliskov-harp.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Liskov, Ghemawat, Gruber, Johnson, Shrira, Williams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SOSP 1991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are we reading this paper?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Harp was the first complete primary/backup system that dealt w/ partition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a complete case study of a replicated service (file server)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses Raft-like replication techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How could a 1991 paper still be worth reading?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Harp introduced techniques that are still widely used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are few papers describing complete replicated systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper is a mix of fundamentals and incidentals
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We care a lot about replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We may not care much about NFS specifically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But we care a lot about the challenges faced when integrating a real application
    with a replication protocol.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And we care about where optimization is possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I'm going to focus on parts of Harp that aren't already present in Raft.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: But note that Harp pre-dates Raft by 20+ years.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raft is, to a great extent, a tutorial on ideas pioneered by Harp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though they differ in many details.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What does Harp paper explain that Raft paper does not?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adapting a complex service to state machine abstraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: e.g. possibility of applying an operation twice
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lots of optimizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pipelining of requests to backup
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'witness, to reduce the # of replicas'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: primary-only execution of read-only operations using leases
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient re-integration of re-started server with large state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't want to do things like copying entire disks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Catch-up" for re-joining replicas'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Power failure, including simultaneous failure of all servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient persistence on disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freeing of log
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harp authors had not implemented recovery yet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Earlier paper (1988) describes [View-stamped Replication](http://www.pmg.csail.mit.edu/papers/vr.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Later (2006) paper](http://pmg.csail.mit.edu/papers/vr-revisited.pdf) has
    clearer description, though a bit different:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic setup is familiar
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Clients, primary, backup(s), witness(es).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client -> Primary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary -> Backups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backups -> Primary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary waits for all backups / promoted witnesses in current view
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Commit point
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary -> execute and reply to Client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary -> tell Backups to Commit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2n+1` servers, `n` backups, `n` witnesses, 1 primary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: need a majority of `n+1` servers `=>`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: tolerate up to `n` failures
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: clients send NFS requests to primary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: primary forwards each request to all the backups
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: after all the backups have replied, the primary can execute the op and apply
    it to its FS
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: in a later operation, primary piggybacks an ACK to tell backups the op. commited
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are `2b+1` servers necessary to tolerate `b` failures?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (This is review...)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose we have `N` servers, and execute a write.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can't wait for more than `N-b`, since `b` might be dead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's require waiting for `N-b` for each operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `b` we didn't wait for might be live and in another partition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can prevent them from proceeding if `N-b > b`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I.e. `N > 2b => N = 2b + 1` is enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are Harp's witnesses?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary and backups have FSs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The witnesses don't receive anything from primary and don't have FSs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose we have a `P, B` and a `W`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there's a partition `P | B, W`, the witness acts as a tie breaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: whichever one (P or B) can talk to the witness gets to continue and execute
    client side operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'witness acts as a tie breaker: whoever can talk to it wins and gets to act
    as a primary'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a second use of the witness is to record operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once a witness is part of the partition `B, W`, it records operations so that
    a majority of nodes have the latest operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a final function of the witness is that when the primary comes back to life,
    the witness has been logging every single operation issued since primary disappeared,
    so witness can replay every op to primary and primary will be up to date w.r.t.
    all the operations executed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: efficiently bring primary up to speed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: the backup could do that too, but Harp is designed so that backup dumps the
    op logs to disk and witnesses keep the logs themselves so they can quickly send
    them to primary for reapplying
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: assumption that reapplying witness logs is faster than copying backup disk
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: assumption that witness logs won't get too big
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The witnesses are one significant difference from Raft.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `b` witnesses do not ordinarily hear about operations or keep state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is that OK?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b+1` of `2b+1` do have state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: So any `b` failures leaves at least one live copy of state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are the `b` witnesses needed at all?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `b` replicas with state do fail, witnesses give the required `b+1` majority.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that only one partition operates -- no split brain.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: So, for a 3-server system, the witness is there to break ties about which partition
    is allowed to operate when primary and backup are in different partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partition with the witness wins.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does primary need to send operations to witnesses?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary must collect ACKs from a majority of the `2b+1` for every r/w operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that it is still the primary -- still in the majority partition.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that operation is on enough servers to intersect with any
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: future majority that forms a new view.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If all backups are up, primary+backups are enough for that majority.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `m` backups are down:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary must talk to `m` "promoted" witnesses to get its majority for each op.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Those witnesses must record the op, to ensure overlap with any
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: future majority.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus each "promoted" witness keeps a log.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: So in a `2b+1` system, a view always has `b+1` servers that the primary must
    contact for each op, and that store each op.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: somewhat different from Raft'
  prefs: []
  type: TYPE_NORMAL
- en: Raft keeps sending each op to all servers, proceeds when majority answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So leader must keep full log until failed server re-joins
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Harp eliminates failed server from view, doesn't send to it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only witness has to keep a big log; has special plan (ram, disk, tape).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The bigger issue is that it can take a lot of work to bring a re-joining replica
    up to date; careful design is required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the story about the UPS?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is one of the most interesting aspects of Harp's design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each server's power cord is plugged into a UPS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UPS has enough battery to run server for a few minutes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UPS tells server (via serial port) when main A/C power fails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server writes dirty FS blocks and Harp log to disk, then shuts down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does the UPS buy for Harp?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Efficient protection against A/C power failure of ALL servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For failures of up to b servers, replication is enough
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *all* servers failed and lost state, that's more than b failures,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: so Harp has no guarantee (and indeed no state!)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With UPS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each server can reply *without* writing disk!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: But still guarantees to retain latest state despite simultaneous power fail
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But note:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UPS does not protect against other causes of simultaneous failure
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: e.g. bugs, earthquake
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Harp treats servers that re-start after UPS-protected crash differently
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: than those that re-start with crash that lost in-memory state
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the latter may have forgotten *committed* operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For independent failures Harp has powerful guarantees, for stuff like software
    bugs that will cause a cascade of crashes, it doesn't really have solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Larger point**, faced by every fault-tolerant system'
  prefs: []
  type: TYPE_NORMAL
- en: Every replicated system tends to need to have a commit point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replicas *must* keep persistent state to deal w/ failure of all servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Committed operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Latest view number, proposal number, &c
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Must persist this state before replying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing every commit to disk is very slow!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 ms per disk write, so only 100 ops/second
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So there are a few common patterns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low throughput
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Batching, high delay
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: batch a lot of writes and do them all at the same time to amortize the cost
    of each write
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: but now you need to make clients wait for their write to finish more
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: because they are also waiting for other clients' writes to finish
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lossy or inconsistent recovery from simultaneous failure
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: no guarantees after crashes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Batteries, flash, SSD w/ capacitor, &c
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's talk about Harp's log management and operation execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Primary and backup must apply client operations to their state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State here is a file system -- directories, file, owners, permissions, &c
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harp must mimic an ordinary NFS server to the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: i.e. not forget about ops for which it has sent a reply
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is in a typical log record?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Not just the client-issued op., like `chmod`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Log record stores:'
  prefs: []
  type: TYPE_NORMAL
- en: Client's NFS operation (write, mkdir, chmod, &c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shadow state: modified i-nodes and directory content *after* execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (i.e. the results after executing the operation)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Client RPC request ID, for duplicate detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary might repeat an RPC if it thinks the backup has failed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reply to send to client, for duplicate detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does Harp have so many log pointers?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FP most recent client request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CP commit point (real in primary, latest heard in backup)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AP highest update sent to disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LB disk has finished writing up to here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLB all nodes have completed disk up to here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why the FP-CP gap?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So primary doesn't need to wait for ACKs from each backup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: before sending next operation to backups
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary pipelines ops CP..FP to the backups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher throughput if concurrent client requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why the AP-LB gap?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Allows Harp to issue many ops as disk writes before waiting for disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disk is more efficient if it has lots of writes (e.g. arm scheduling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the LB?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This replica has everything `<= LB` on disk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So it won't need those log records again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why the LB-GLB gap?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GLB is min(all servers' LBs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLB is earliest record that *some* server might need if it loses memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When does Harp execute a client operation?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two answers!
  prefs: []
  type: TYPE_NORMAL
- en: When operation arrives, primary figures out exactly what should happen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produces resulting on-disk bytes for modified i-nodes, directories, &c.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the shadow state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This happens before the CP, so the primary must consult recent operations in
    the log to find the latest file system state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After the operation commits, primary and backup can apply it to their file systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They copy the log entry's shadow state to the file system;
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: they do not really execute the operation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And now the primary can reply to the client's RPC.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does Harp split execution in this way?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a server crashes and reboots, it is brought up to date by replaying log entries
    it might have missed. Harp can't know exactly what the last pre-crash operation
    was, so Harp may repeat some. It's not correct to fully execute some operations
    twice, e.g. file append.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So Harp log entries contain the *resulting* state, which is what's applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Append example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If backup crashes after it writes A1 to disk but before replying to primary,
    when the backup reboots there's no obvious way of telling whether it executed
    A1\. As a result, it has to reexecute it. Thus, these log records have to be "repeatable."
  prefs: []
  type: TYPE_NORMAL
- en: '*Actually,* a lot of replication systems have to cope with this, and this is
    one way to deal with it. It also illustrates how non-straightforward replication
    can be.'
  prefs: []
  type: TYPE_NORMAL
- en: Harp needs to be aware of FS-level inodes for instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The point: multiple replay means replication isn''t transparent to the service.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Service must be modified to generate and accept the state modifications that
    result from client operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, when applying replication to existing services, the service must
    be modified to cope with multiple replay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can Harp primary execute read-only operations w/o replicating to backups?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: e.g. reading a file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Would be faster -- after all, there's no new data to replicate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason we forward read only operations to backup is to make sure we find
    out if we were partitioned and 1000 ops were executed that we don''t know about:
    make sure we don''t reply with an old write of the data we are reading'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the danger?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harp''s idea: leases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backups promise not to form a new view for some time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (i.e. not to process any ops as a primary)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary can execute read-only ops locally for that time minus slop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: because it knows backup won't execute ops as primary during that time (backup
    promised this!)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depends on reasonably *synchronized clocks*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robert Morris: "Not really happy about this."'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Primary and backup must have bounded disagreement on how fast time passes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This really requires a bounded frequency skew
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apparently hardware is really bad at providing this
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What state should primary use when executing a read-only operation?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Does it have to wait for all previously arrived operations to commit?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No! That would be almost as slow as committing the read-only op.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Should it look at state as of operation at FP, i.e. latest r/w operation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No! That operation has not committed; not allowed to reveal its effects.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus Harp executes read-only ops with state as of CP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if client sends a WRITE and (before WRITE finishes) a READ of same data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: READ may see data *before* the WRITE!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is that OK?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The client sent the READ and the WRITE concurrently. It has no right to expect
    one order or the other. So if the READ doesn't see the WRITE's effects, that's
    acceptable -- it's the same answer you'd get if the READ had moved through the
    network faster than the WRITE, which could happen. You can only expect a READ
    to see a WRITE's effect if you issue the WRITE, wait for a reply to the WRITE,
    and then issue the READ.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does failure recovery work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I.e. how does Harp recover replicated state during view change?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setup for the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '5 servers: S1 is usually primary, S2+S3 are backups, S4+S5 witnesses'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Will S2 have every committed operation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Will S2 have every operation S1 received?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No. No, maybe op didn't reach S2 from S1 and then S1 crashed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Will S2's log tail be the same as S3's log tail?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not necessarily.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Maybe op reached S2 but not S3 and then S1 crashed.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Maybe op reached S2, and S3 crashed, so S4 was promoted. Then S3 came back up?
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How far back can S2 and S3 log tail differ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not up to the CP, because committed ops could be committed w/ help of promoted
    witness `=>` backup logs differ
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to cause S2 and S3's log to be the same?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must commit ops that appeared in both S2+S3 logs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What about ops that appear in only one log?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this scenario, can discard since could not have committed
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: But in general committed op might be visible in just one log
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From what point does promoted witness have to start keeping a log?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if S1 crashed just before replying to a client?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Will the client ever get a reply?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After S1 recovers, with intact disk, but lost memory.
  prefs: []
  type: TYPE_NORMAL
- en: It will be primary, but Harp can't immediately use its state or log.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike Raft, where leader only elected if it has the best log.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Harp must replay log from promoted witness (S4)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Could S1 have executed an op just before crashing that the replicas didn't execute
    after taking over?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No, execution up to CP only, and CP is safe on S2+S3.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New scenario: S2 and S3 are partitioned (but still alive)'
  prefs: []
  type: TYPE_NORMAL
- en: Can S1+S4+S5 continue to process operations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, promoted witnesses S4+S5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: S4 moves to S2/S3 partition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can S1+S5 continue?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No, primary S1 doesn't get enough backup ACKs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can S2+S3+S4 continue?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, new view copies log entries S4->S2, S4->S3, now S2 is primary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New primary was missing many committed operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In general some *committed* operations may be on only one server
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New scenario: S2 and S3 are partitioned (but still alive)'
  prefs: []
  type: TYPE_NORMAL
- en: S4 crashes, loses memory contents, reboots in S2/S3 partition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can they continue?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only if there wasn't another view that formed and committed more ops
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depends on what S4''s on-disk view # says.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OK if S4''s disk view # is same as S2+S3''s.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No new views formed.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: S2+S3 must have heard about all committed ops in old view.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Everybody (S1-5) suffers a power failure.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: S4 disk and memory are lost, but it does re-start after repair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S1 and S5 never recover.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S2 and S3 save everything on disk, re-start just fine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can S2+S3+S4 continue?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (harder than it looks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Believe the answer is "No": cannot be sure what state S4 had before failure.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Might have formed a new view with S1+S5, and committed some ops.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can S2 and S3 know about the previous view? Not always.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When can Harp form a new view?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: No other view possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Know view # of most recent view.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Know all ops from most recent view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Details:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) is true if you have n+1 nodes in new view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(2) true if you have n+1 nodes that did not lose view # since last view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'View # stored on disk, so they just have to know disk is OK.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One of them *must* have been in the previous view.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: So just take the highest view number.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And #3?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need a disk image, and a log, that together reflect all operations through the
    end of the previous view.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps from different servers, e.g. log from promoted witness, disk from backup
    that failed multiple views ago.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does Harp have performance benefits?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Fig 5-1, why is Harp *faster* than non-replicated server?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much win would we expect by substituting RPC for disk operations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why graph x=load y=response-time?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Why does this graph make sense?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why not just graph total time to perform X operations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One reason is that systems sometimes get more/less efficient w/ high load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And we care a lot how they perform w/ overload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does response time go up with load?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Why first gradual...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queuing and random bursts?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And some ops more expensive than others, cause temp delays.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then almost straight up?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probably has hard limits, like disk I/Os per second.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Queue length diverges once offered load > capacity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
