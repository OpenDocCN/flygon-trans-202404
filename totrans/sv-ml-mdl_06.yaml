- en: Chapter 6\. Apache Spark Implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章\. Apache Spark 实现
- en: '[Spark](https://spark.apache.org/) is a powerful open source processing engine
    built around speed, ease of use, and sophisticated analytics. It is currently
    the largest open source community in big data, with more than 1,000 contributors
    representing more than 250 organizations. The main characteristics of Spark are
    as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[Spark](https://spark.apache.org/) 是一个围绕速度、易用性和复杂分析构建的强大开源处理引擎。它目前是大数据领域最大的开源社区，拥有超过
    1,000 名贡献者，代表着超过 250 个组织。Spark 的主要特点如下：'
- en: Performance
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 性能
- en: It is engineered from the bottom up for performance and can be very fast by
    exploiting in-memory computing and other optimizations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它从底层开始进行性能优化，通过利用内存计算和其他优化技术可以非常快速。
- en: Ease of use
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用便捷性
- en: It provides easy-to-use APIs for operating on large datasets including a collection
    of more than 100 operators for transforming data and data frame APIs for manipulating
    semi-structured data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了易于使用的 API，用于操作大型数据集，包括超过 100 个用于转换数据的操作符集合以及用于操作半结构化数据的数据框 API。
- en: Unified engine
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 统一引擎
- en: It is packaged with higher-level libraries, including support for SQL queries,
    streaming data, machine learning, and graph processing. These standard libraries
    increase developer productivity and can be seamlessly combined to create complex
    workflows.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它打包了更高级别的库，包括支持 SQL 查询、流数据、机器学习和图处理。这些标准库提高了开发人员的生产力，并可以无缝组合以创建复杂的工作流程。
- en: Spark Streaming is an extension of core Spark API, which makes it easy to build
    fault-tolerant processing of real-time data streams. In the next section, we discuss
    how to use Spark Streaming for implementing our solution.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是核心 Spark API 的扩展，使得构建容错的实时数据流处理变得容易。在下一节中，我们将讨论如何使用 Spark Streaming
    来实现我们的解决方案。
- en: Overall Architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整体架构
- en: Spark Streaming currently uses a minibatch approach to streaming, although a
    true streaming implementation is under way (discussed shortly). Because our problem
    requires state, the only option for our implementation is to use the [`mapWithState`](http://bit.ly/2xzhNCM)
    [operator](http://bit.ly/2xzhNCM), where state is a Resilient Distributed Dataset
    core data collection abstraction in Spark. Each record in this RDD is a key–value
    pair, in which the key is the data type (see [Example 3-2](ch03.html#protobuf_definition_for_the_model_update))
    and the value is the current model object. This approach effectively uses the
    state RDD as a memory for the model, which is saved between minibatch executions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 目前使用一种小批量处理流的方法，尽管真正的流式处理实现正在进行中（稍后讨论）。由于我们的问题需要状态，我们的实现唯一的选择是使用[`mapWithState`](http://bit.ly/2xzhNCM)
    [操作符](http://bit.ly/2xzhNCM)，其中状态是 Spark 中弹性分布式数据集核心数据集合的抽象。这个 RDD 中的每个记录都是一个键-值对，其中键是数据类型（参见[示例
    3-2](ch03.html#protobuf_definition_for_the_model_update)），值是当前的模型对象。这种方法有效地使用状态
    RDD 作为模型的内存，这个内存在小批量执行之间被保存。
- en: 'The second issue for this implementation is the necessity to merge both input
    streams: the data and the model streams. Similar to the Beam case, the only option
    to merge nonwindowed streams in Spark is the [`union`](http://bit.ly/2xzyoeG)
    operator, which requires both streams to have the same data definition. [Figure 6-1](#spark_implementation_approach)
    presents the overall architecture for the Spark implementation.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现的第二个问题是必须合并两个输入流：数据流和模型流。与 Beam 情况类似，Spark 中合并非窗口流的唯一选项是[`union`](http://bit.ly/2xzyoeG)
    操作符，它要求两个流具有相同的数据定义。[图 6-1](#spark_implementation_approach) 展示了 Spark 实现的整体架构。
- en: '![smlt 0601](assets/smlt_0601.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![smlt 0601](assets/smlt_0601.png)'
- en: Figure 6-1\. Spark implementation approach
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. Spark 实现方法
- en: Implementing Model Serving Using Spark Streaming
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark Streaming 实现模型服务
- en: '[Example 6-1](#model_serving_with_spark) shows the overall pipeline implementation
    for model serving using Spark adhering to this architecture ([complete code available
    here](http://bit.ly/2gbSpk9)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-1](#model_serving_with_spark) 展示了使用 Spark 实现模型服务的整体流程，遵循这个架构（[完整代码在此处可用](http://bit.ly/2gbSpk9)）。'
- en: Example 6-1\. Model serving with Spark
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 使用 Spark 进行模型服务
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The actual model serving is done in `mappingFunc` (its functionality is similar
    to the model serving implementation in Flink [[Example 4-1](ch04.html#the_dataprocessor_class)]),
    which is invoked on the combined stream for every minibatch.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的模型服务是在 `mappingFunc` 中完成的（其功能类似于 Flink 中的模型服务实现[[示例 4-1](ch04.html#the_dataprocessor_class)]），它在每个小批量上对合并流进行调用。
- en: Another important point here is setting `KryoSerializer` as the default serializer
    and implementing Kryo serializers for the models. To register these serializers,
    you need to implement a special `KryoRegistrator` class, as shown in [Example 6-2](#model_kryo_serializer_and_registrator)
    ([complete code available here](http://bit.ly/2ygTii6)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里另一个重要的点是将 `KryoSerializer` 设置为默认序列化器，并为模型实现 Kryo 序列化器。要注册这些序列化器，你需要实现一个特殊的
    `KryoRegistrator` 类，如[示例 6-2](#model_kryo_serializer_and_registrator)（[完整代码可在此处找到](http://bit.ly/2ygTii6)）所示。
- en: Example 6-2\. Model Kryo serializer and registrator
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2. 模型 Kryo 序列化器和注册器
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Although this example uses a single model, you can expand it easily to support
    multiple models by using a map of models keyed on the data type.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此示例使用单个模型，但你可以通过使用按数据类型键入的模型映射轻松扩展它以支持多个模型。
- en: When using this Spark implementation, it is necessary to keep in mind that it
    is not a true streaming system, meaning that there will be delay of up to the
    “batch time size,” typically 500 milliseconds or more. If this is acceptable,
    this solution is very scalable.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种 Spark 实现时，有必要牢记它不是真正的流式系统，这意味着会有长达“批处理时间大小”的延迟，通常为 500 毫秒或更长。如果这是可以接受的，那么这个解决方案非常可扩展。
- en: 'Spark provides a new streaming API, called [Structured Streaming](http://bit.ly/2gEAFwG),
    but you can’t use it for the problem that we are solving here because some required
    operations are not yet [supported](http://bit.ly/2yFLOWy) (as of version 2.2);
    for example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个名为[结构化流式处理](http://bit.ly/2gEAFwG)的新流处理 API，但是对于我们在解决的问题，你不能使用它，因为一些必需的操作还没有得到[支持](http://bit.ly/2yFLOWy)（截至版本
    2.2）；例如：
- en: Any kind of joins between two streaming Datasets are not yet supported.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目前尚不支持两个流式数据集之间的任何类型的连接。
- en: Spark Streaming is the last stream-processing engine that I will discuss. In
    Chapters [7](ch07.html#apache_kafka_streams_implementation) and [8](ch08.html#akka_streams_implementation),
    we look at using streaming frameworks, starting with Kafka Streams, for solving
    the same problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 流式处理是我将讨论的最后一个流处理引擎。在[第7章](ch07.html#apache_kafka_streams_implementation)和[第8章](ch08.html#akka_streams_implementation)中，我们将研究使用流式框架解决相同问题的开始，从
    Kafka Streams 开始。
