- en: 'Chapter 13: Solving Equations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We construct four iterative methods for solving an equation f(x) = 0\. They
    are: Newton''s method, in which we go from the old point to the new by finding
    where the linear approximation to f at the old is zero; poor man''s Newton which
    is the same except we approximate the slope in the linear approximation, and two
    interpolative methods.'
  prefs: []
  type: TYPE_NORMAL
- en: We also raise the problem of solving two simultaneous equations in two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 13.0  [Solving an Equation in One Variable](section00.html)
  prefs: []
  type: TYPE_NORMAL
- en: 13.1  [Newton's Method](section01.html)
  prefs: []
  type: TYPE_NORMAL
- en: 13.2  [Poor Man's Newton](section02.html)
  prefs: []
  type: TYPE_NORMAL
- en: 13.3  [Another Linear Method](section03.html)
  prefs: []
  type: TYPE_NORMAL
- en: 13.4  [Divide and Conquer](section04.html)
  prefs: []
  type: TYPE_NORMAL
- en: 13.5  [Solving Two General Equations in Two Variables](section05.html)
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 Newton's Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method, as just noted, consists of iterating the process of setting the
    linear approximation of f to 0 to improve guesses at the solution to f = 0.
  prefs: []
  type: TYPE_NORMAL
- en: The linear approximation to f(x) at argument x[0] which we will call fLx[0](x)
    can be described by the equation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afba12a50ec71feb1cad851bb401cc5e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: if we set fLx[0](x) = 0, and solve for x, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ab38ba8ac2495b0bc3b917221736763.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and so we obtain ![](../Images/b829afb3cfcb13e73c5de68936e4bc55.jpg), and in
    general can define
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cfb2fc6bd37f08ba247a068d8173e56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the applet that follows you can enter a standard function, choose the number
    (nb points) of iterations that can be shown, adjust x[0] with the second slider,
    and look at each iteration with the first slider. You will see the function and
    the effects of the iterations.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that with this method you may arrive at a nearby 0, or a far
    away one, depending a bit on luck.
  prefs: []
  type: TYPE_NORMAL
- en: <applet code="NewtonsMethod" codebase="../applets/" archive="newtonsMethod.jar,mk_lib.jar,parser_math.jar,jcbwt363.jar"
    width="760" height="450"></applet>
  prefs: []
  type: TYPE_NORMAL
- en: In� the old days the tedium of performing the steps of finding the x[j]'s was
    so formidable that it could not be safely inflicted on students.
  prefs: []
  type: TYPE_NORMAL
- en: Now, with a spreadsheet, we can set this up and do it with even a messy function
    f, in approximately a minute.
  prefs: []
  type: TYPE_NORMAL
- en: '**To do so put your initial guess, x[0] in one box say a2, put "= f(a2)" in
    b2, and "= f ''(a2)" in c2\. Then put "= a2-b2/c2" in a3 and copy a3, b2 and c2
    down as far as you like.** (Of course you have to spell out what f and f '' are
    in doing this.)'
  prefs: []
  type: TYPE_NORMAL
- en: That's all there is to it.
  prefs: []
  type: TYPE_NORMAL
- en: If column b goes to zero, the entries in column a will have converged to an
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to change your initial guess you need only enter something else
    in a2; to solve a different equation you need only change b2 and c2 and copy them
    down.
  prefs: []
  type: TYPE_NORMAL
- en: This raises some interesting questions; namely, **can we say anything about
    when this method will work and when it will not?**
  prefs: []
  type: TYPE_NORMAL
- en: First you should realize that many functions have more than one argument for
    which their values are 0\. Thus you may not get the solution you want.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if the function f has no zero, like x² + 1, you will never get anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another problem: if your initial guess, ![](../Images/12100f1af3bef133bce92a6485c249b4.jpg)
    (or any subsequent ![](../Images/543c49b83b1b8e09fff0fe530ea475a9.jpg)) is near
    a critical point of f (at which f '' = 0) the quantity ![](../Images/9b25376f1f0270396ead00afd096ecc8.jpg)
    may become huge at that argument, and you may be led to looking at arguments very
    far from what you are looking for.'
  prefs: []
  type: TYPE_NORMAL
- en: And if f is implicitly defined you may find that some new guess x[j] in which
    f is not even defined, and the iteration will dead end.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can we say anything positive about the use of the method?**'
  prefs: []
  type: TYPE_NORMAL
- en: Yes! **If f goes from negative to positive at the true solution x, and f ' is
    increasing between x and your guess x[0], which is greater than x,** then the
    method will always converge. Similar statements hold when f goes from positive
    to negative, and under many other circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is this?**'
  prefs: []
  type: TYPE_NORMAL
- en: If f ' is increasing, then the tangent line to f at x[0]�will go under the f
    curve at between x and x[0], so that the linear approximation, whose curve it
    is, will hit zero between x and x[0], and the same thing will be true in each
    iteration. Thus the x's will march off toward the true solution without hope of
    escape and will eventually get there.
  prefs: []
  type: TYPE_NORMAL
- en: Another virtue of the method is that as one gets closer to the solution, the
    differentiable function will tend to look more and more like its linear approximation
    between the current guess and the true solution. Thus the method tends to converge
    very rapidly once that current guess is near a solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up a spread sheet to apply it to the following functions:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**13.1     ** **exp(x) - 27**'
  prefs: []
  type: TYPE_NORMAL
- en: '**13.2     ** **sin (x) - 0.1**'
  prefs: []
  type: TYPE_NORMAL
- en: '**13.3     ** **x²**'
  prefs: []
  type: TYPE_NORMAL
- en: '**13.4     ** **tan x**'
  prefs: []
  type: TYPE_NORMAL
- en: '**13.5     ** **x^(1/3)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**13.6     ** **x^(1/3) - 1**'
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 Poor Man's Newton
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To apply Newton's method as just described, it is necessary to differentiate
    the function f at each successive guess. This is not difficult, but requires a
    small amount of effort.
  prefs: []
  type: TYPE_NORMAL
- en: We can instead apply what is essentially the same method, using an approximation
    to ![](../Images/692df5ffd88e9948b550b37b7e5e7e5b.jpg) of the form ![](../Images/da80df7b47af623e9e7d7b6f427f7dc6.jpg)
    for some d. We then have to decide what to use for d, but with a spreadsheet we
    can pick a small value to begin with and let it slowly go to zero as we iterate.
    If we wanted to be fancier, we could use a symmetric approximation to the derivative![](../Images/7fbf6eb2e5aef315062d1b076e291be1.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we do all this?**'
  prefs: []
  type: TYPE_NORMAL
- en: In box e2 we can put our initial value of d, say 10^(-3); then we can put "=
    e2*9/10" in e3 and copy it down, so that d will slowly decline to 0\. (Why slowly
    like this? If you go too fast round-off error might ruin you before you find your
    solution.) The iteration here is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c221d9e90e6e1e384d3fdecb3062ac9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can therefore put our guess in f2, put "= e2+f2"in g2, set h2 to "= f(f2)",
    copy it into i2� and then set f3 to "= f2-e2*h2/(i2-h2)", and copy g2, h2, i2
    and e3 and f3 down, and we are done.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.7 Do this for each of the functions in the exercises of the section
    13.1\. Do you find a difference between the results here and with the regular
    Newton''s method? If so, what?**'
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Another Linear Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another easy method to apply is to pick two arguments, x[0] and x[0'] that you
    hope are near the solution you want, evaluate the function at them and assume
    that the straight line defined by the two values and arguments is reasonable approximation
    to f. You can then find where that straight line meets the x axis, and use it
    to replace one of the two initial values.
  prefs: []
  type: TYPE_NORMAL
- en: In doing this there are two cases; one in which the value of f has the same
    sign at both points. This means that their straight line will not meet the x axis
    between your guesses, and then you might as well use the new point to replace
    the one of the two others furthest from it.
  prefs: []
  type: TYPE_NORMAL
- en: If, on the other hand, the values of f at your two arguments have the opposite
    sign, you should replace the old argument at which f has the same sign as it has
    at the new one, so that the values of f will have the opposite sign on the next
    turn as well.
  prefs: []
  type: TYPE_NORMAL
- en: This method has the advantage over Newton's that once you find arguments whose
    values of f have the opposite sign, you must home in on a solution between those
    arguments. (A solution being a point at which f changes sign; for a continuous
    f this is a point at which f is 0; functions like tan x change signs at arguments
    at which they are infinite (or more properly, undefined) as well as those at which
    they are 0.)
  prefs: []
  type: TYPE_NORMAL
- en: It has the disadvantage that there are functions for which the true derivative
    between your guesses is far from a straight line, and you gain very little in
    each iteration. It can then be painfully slow to converge.
  prefs: []
  type: TYPE_NORMAL
- en: The equation used for finding x[i + 1]�given the two values x[i] and say x[i']
    that you use to compute it is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/331eb50f4f7f38f7849fe2da6c5d1ca5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you compare this with the previous method, this corresponds to approximating
    the derivative in Newton's method by using values at the previous two guesses,
    rather than by the last guess and it plus d, as in Poor Man's Newton.
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation can be solved and we can find a new x[i] easily as before. With
    a small amount of effort we can figure out which of the old arguments to discard;
    the following setup does it:'
  prefs: []
  type: TYPE_NORMAL
- en: In s2 and t2 put your initial guesses. In u2 put "= f(s2)"' and copy that into
    v2.
  prefs: []
  type: TYPE_NORMAL
- en: Now put "= s2 -(s2-t2)*u2/(u2-v2)"in s3, and "= if(abs(s3-t2)<abs(s3-s2),t2,s2)"
    in w2\. (This puts the old argument closer to the new one in w2.)
  prefs: []
  type: TYPE_NORMAL
- en: Copy s3 and u2 down, and now put "= if(u2*v2>0,w2,if(u2*u3>0,t2,s2))" in t3
    and copy t3, w2, and v2 down.
  prefs: []
  type: TYPE_NORMAL
- en: This last horrible instruction makes t3 hold the nearer old argument to s3 if
    u2 and v2 had the same sign, and otherwise puts the old argument whose value of
    f has the opposite sign to u3 in t3\.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.8 How does this scheme compare to the others on your examples?
    Try it on the various examples of the previous exercises.**'
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Divide and Conquer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last previous method uses a straight line approximation to get a new argument
    between old ones, once the values of f at the old arguments have the opposite
    sign. This is usually a smart thing to do, unless the endpoints of the interval
    between which your solution is trapped (in columns s and t above) converge very
    slowly.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid that possibility we can, **once the function f has opposite sign at
    two points, say a and b, evaluate it at the middle point, and replace one end
    by the middle.**
  prefs: []
  type: TYPE_NORMAL
- en: This will cut the size of the interval in which the solution must lie in half.
    This is slow convergence compared to the best of Newton's algorithm or the variants
    discussed above, but it is steady and effective and will always give a definite
    improvement in accuracy in a fixed number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: Since 2 to the tenth power is a bit more than one thousand, (it's 1024) the
    size of the interval goes down by a factor of at least 1000 for every ten iterations,
    and so if it starts at something like 1, after 35 or so steps you will have the
    answer to ten decimal places.
  prefs: []
  type: TYPE_NORMAL
- en: '**How does the algorithm go?**'
  prefs: []
  type: TYPE_NORMAL
- en: We start with two arguments, a and b and suppose we assume a < b. We evaluate
    f(a) and f(b), and also ![](../Images/b1319cf0018c8f6d6007a7f8559604e1.jpg). Then
    if the last of these and the first has the same sign you replace a by ![](../Images/892847552d2b6a7b393feb1ecb53165d.jpg)
    and keep b, while otherwise you keep a and replace b by ![](../Images/892847552d2b6a7b393feb1ecb53165d.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**In a spreadsheet you can put your initial guesses in aa2 and ab2 put "= aa2/2+ab2/2"
    in ac2 and put "= f(aa2)" in ad2 and copy that to ae2 and af2\. You can then put
    "= if(ae2*af2>0, aa2,ac2)" in aa3, and "= if(ae2*af2>0,ac2,ab2)" in ab3, copy
    down and you are done.**'
  prefs: []
  type: TYPE_NORMAL
- en: Unless there is an error lurking somewhere, or you started with ad2 and ae2
    having the same sign, this will shorten the initial a to b by a factor of at least
    10^(-10) interval after 35 or so steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.9 Compare performance of this algorithm with the previous ones
    on the same examples. Any comments?**'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Solving Two General Equations in Two Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One nice feature of Newton's Method (and of Poor Man's Newton as well) is that
    it can easily be generalized to two or even three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: That is, suppose we have two standard functions, f and g of two variables, x
    and y. Each of the equations, f(x, y) = 0, and g(x, y) = 0 will typically be satisfied
    on curves, just as similar linear equations are satisfied on straight lines. And
    suppose we seek simultaneous solutions to both equations, which will then be at
    the intersections, if any, of these curves.
  prefs: []
  type: TYPE_NORMAL
- en: If we could solve either of the equations for x say, in terms of y, we could
    find a parametric representations of the curve solutions to it (with y as parameter),
    and use the divide and conquer method of the last section on the other function,
    cutting the parameter interval in half at each step as in one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: This is a slow and steady method that can be implemented fairly easily. But
    it assumes we can obtain a parametric representation of one or the other curve.
  prefs: []
  type: TYPE_NORMAL
- en: We can always try Newton's method, which is fairly easy to implement in general.
  prefs: []
  type: TYPE_NORMAL
- en: To use Newton's method, we **compute the gradients of f and g at some initial
    point, and find a new point at which the linear approximations to f and g defined
    by the gradients at the initial point are both 0.** We then iterate this step.
  prefs: []
  type: TYPE_NORMAL
- en: To implement this method takes roughly three times the work of using Newton's
    method in one dimension. On the other hand three times almost nothing is still
    small.
  prefs: []
  type: TYPE_NORMAL
- en: This method suffers from the same problems as Newton's method suffers from in
    one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: We may wander far from where we want to be in the xy plane, especially if we
    come to a point at which the gradients are small.
  prefs: []
  type: TYPE_NORMAL
- en: And of course two random curves need not intersect at all, so there may not
    even be a solution, or there could be many of them.
  prefs: []
  type: TYPE_NORMAL
- en: But again if we can start near a solution, under the right circumstances the
    method does very well indeed.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we set it up?**'
  prefs: []
  type: TYPE_NORMAL
- en: First you pick initial guesses of x[0] and y[0]; on the spreadsheet, enter them
    in the first two columns (say). Then you need a column to enter f(x, y) and one
    for g(x, y); and one each for the x and y derivatives of f and g (four derivatives
    all together).
  prefs: []
  type: TYPE_NORMAL
- en: You now have all the information you need to compute x[1] and y[1]. Once you
    have done this you need only copy everything down say 30 rows, and you can see
    what happens. If f and g go to zero, which means that x[i] and y[i] both converge,
    you will have a solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**So how do we iterate?**'
  prefs: []
  type: TYPE_NORMAL
- en: We must solve the two linear equations in x and y that state that the linear
    approximations to f and to g defined at (x[0], y[0]) both are 0\.
  prefs: []
  type: TYPE_NORMAL
- en: What are these equations? They are
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d31d52d15472d751e9db095e3ce32157.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**And what are the solutions?**'
  prefs: []
  type: TYPE_NORMAL
- en: We can use[Cramer's (ratio of determinant) rule](../chapter32/section04.html#CramersRule)
    to tell us that the solutions are
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e742caef3f945ec9f96c10c8e6b20e7e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17a9277af60947f98c2b7d89561974de.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Of course all further iterations are identical to this one with the new guesses.
    Thus after entering these formulae once, copying them down is all that is necessary
    to apply the method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 13.10 Try this method on a spreadsheet with the following function:
    f(x, y) = exp(x * y) - y² , g(x, y) = cos(x + y).'
  prefs: []
  type: TYPE_NORMAL
- en: Find three solutions for which both variables are positive. How many such solutions
    are there?**
  prefs: []
  type: TYPE_NORMAL
- en: The same approach can be implemented in three dimensions, though the amount
    of work required on a spreadsheet starts to be slightly tedious.
  prefs: []
  type: TYPE_NORMAL
- en: You have to enter three variables, three functions, their nine partial derivatives
    and of course Cramer's rule now involves the ratio of two three by three determinants
    each, now three times.
  prefs: []
  type: TYPE_NORMAL
- en: You could do it, though if you ever really wanted to, and actually find the
    solution to three arbitrary non-linear equations in three variables, with reasonable
    luck.
  prefs: []
  type: TYPE_NORMAL
- en: <applet code="NewtonsMethodTwoVariables" codebase="../applets/" archive="newtonsMethodTwoVariables.jar,mk_lib.jar,parser_math.jar,jcbwt363.jar"
    width="760" height="450"></applet>
  prefs: []
  type: TYPE_NORMAL
