["```\n'''\nThis script shows how to predict stock prices using a basic RNN\n'''\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib\nimport os\n\ntf.set_random_seed(777)  # reproducibility\n\nif \"DISPLAY\" not in os.environ:\n    # remove Travis CI Error\n    matplotlib.use('Agg')\n\nimport matplotlib.pyplot as plt\n\ndef MinMaxScaler(data):\n    ''' Min Max Normalization\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        input data to be normalized\n        shape: [Batch size, dimension]\n\n    Returns\n    ----------\n    data : numpy.ndarry\n        normalized data\n        shape: [Batch size, dimension]\n\n    References\n    ----------\n    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n\n    '''\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    # noise term prevents the zero division\n    return numerator / (denominator + 1e-7)\n\n# train Parameters\nseq_length = 7\ndata_dim = 5\nhidden_dim = 10\noutput_dim = 1\nlearning_rate = 0.01\niterations = 500\n\n# Open, High, Low, Volume, Close\nxy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\nxy = xy[::-1]  # reverse order (chronically ordered)\n\n# train/test split\ntrain_size = int(len(xy) * 0.7)\ntrain_set = xy[0:train_size]\ntest_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n\n# Scale each\ntrain_set = MinMaxScaler(train_set)\ntest_set = MinMaxScaler(test_set)\n\n# build datasets\ndef build_dataset(time_series, seq_length):\n    dataX = []\n    dataY = []\n    for i in range(0, len(time_series) - seq_length):\n        _x = time_series[i:i + seq_length, :]\n        _y = time_series[i + seq_length, [-1]]  # Next close price\n        print(_x, \"->\", _y)\n        dataX.append(_x)\n        dataY.append(_y)\n    return np.array(dataX), np.array(dataY)\n\ntrainX, trainY = build_dataset(train_set, seq_length)\ntestX, testY = build_dataset(test_set, seq_length)\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, seq_length, data_dim])\nY = tf.placeholder(tf.float32, [None, 1])\n\n# build a LSTM network\ncell = tf.contrib.rnn.BasicLSTMCell(\n    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\noutputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\nY_pred = tf.contrib.layers.fully_connected(\n    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n\n# cost/loss\nloss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n# optimizer\noptimizer = tf.train.AdamOptimizer(learning_rate)\ntrain = optimizer.minimize(loss)\n\n# RMSE\ntargets = tf.placeholder(tf.float32, [None, 1])\npredictions = tf.placeholder(tf.float32, [None, 1])\nrmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    # Training step\n    for i in range(iterations):\n        _, step_loss = sess.run([train, loss], feed_dict={\n                                X: trainX, Y: trainY})\n        print(\"[step: {}] loss: {}\".format(i, step_loss))\n\n    # Test step\n    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n    rmse_val = sess.run(rmse, feed_dict={\n                    targets: testY, predictions: test_predict})\n    print(\"RMSE: {}\".format(rmse_val))\n\n    # Plot predictions\n    plt.plot(testY)\n    plt.plot(test_predict)\n    plt.xlabel(\"Time Period\")\n    plt.ylabel(\"Stock Price\")\n    plt.show() \n```"]