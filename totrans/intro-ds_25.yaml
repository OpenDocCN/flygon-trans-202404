- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Regression](regression.htm)
    > Decision Tree'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree - Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree builds regression or classification models in the form of a tree
    structure. It brakes down a dataset into smaller and smaller subsets while at
    the same time an associated decision tree is incrementally developed. The final
    result is a tree with **decision nodes** and **leaf nodes**. A decision node (e.g.,
    Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing
    values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision
    on the numerical target. The topmost decision node in a tree which corresponds
    to the best predictor called **root node**. Decision trees can handle both categorical
    and numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b2ade66e7dc5bcd27593b3bb3727d14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Decision Tree Algorithm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core algorithm for building decision trees called **ID3** by J. R. Quinlan
    which employs a top-down, greedy search through the space of possible branches
    with no backtracking. The ID3 algorithm can be used to construct a decision tree
    for regression by replacing Information Gain with *Standard Deviation* *Reduction*.**Standard
    Deviation** A decision tree is built top-down from a root node and involves partitioning
    the data into subsets that contain instances with similar values (homogenous).
    We use standard deviation to calculate the homogeneity of a numerical sample.
    If the numerical sample is completely homogeneous its standard deviation is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'a) Standard deviation for **one** attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05a7da6b8e1ca738e4de49b9f5d977a6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'b) Standard deviation for **two** attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/767cce951a9cb7ad396414faae65b516.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Standard Deviation Reduction**The standard deviation reduction is based on
    the decrease in standard deviation after a dataset is split on an attribute. Constructing
    a decision tree is all about finding attribute that returns the highest standard
    deviation reduction (i.e., the most homogeneous branches). *Step 1*: The standard
    deviation of the target is calculated.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard deviation (Hours Played) = 9.32**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2*: The dataset is then split on the different attributes. The standard
    deviation for each branch is calculated. The resulting standard deviation is subtracted
    from the standard deviation before the split. The result is the standard deviation
    reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/590232f120da09aa428aefdba130b226.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/bdb48a29345b0bf7f16b9bba35d6ff20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 3*: The attribute with the largest standard deviation reduction is chosen
    for the decision node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6617e111fff6ec78b8b9a83facfdf1e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 4a*: Dataset is divided based on the values of the selected attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b794d9423480f843e5cc8f8fe345c1b6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 4b*: A branch set with standard deviation more than 0 needs further splitting.Â 
    In practice, we need some termination criteria. For example, when standard deviation
    for the branch becomes smaller than a certain fraction (e.g., 5%) of standard
    deviation for the full dataset *OR* when too few instances remain in the branch
    (e.g., 3).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1989c677282b1c3f6eab8563f0838da3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 5*: The process is run recursively on the non-leaf branches, until all
    data is processed. When the number of instances is more than one at a leaf node
    we calculate the *average* as the final value for the target.'
  prefs: []
  type: TYPE_NORMAL
- en: '| [Exercise](decision_tree_reg_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/TreeReg.txt)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a new algorithm
    to construct a decision tree from data using [MLR](mlr.htm) instead of average
    at the leaf node.'
  prefs: []
  type: TYPE_NORMAL
