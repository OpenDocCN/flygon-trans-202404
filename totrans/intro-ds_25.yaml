- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Regression](regression.htm)
    > Decision Tree'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '[地图](data_mining_map.htm) > [数据科学](data_mining.htm) > [预测未来](predicting_the_future.htm)
    > [建模](modeling.htm) > [回归](regression.htm) > 决策树'
- en: Decision Tree - Regression
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树 - 回归
- en: Decision tree builds regression or classification models in the form of a tree
    structure. It brakes down a dataset into smaller and smaller subsets while at
    the same time an associated decision tree is incrementally developed. The final
    result is a tree with **decision nodes** and **leaf nodes**. A decision node (e.g.,
    Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing
    values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision
    on the numerical target. The topmost decision node in a tree which corresponds
    to the best predictor called **root node**. Decision trees can handle both categorical
    and numerical data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树以树形结构构建回归或分类模型。它将数据集分解为越来越小的子集，同时逐步开发关联的决策树。最终结果是一个具有**决策节点**和**叶节点**的树。决策节点（例如，Outlook）有两个或更多分支（例如，晴天、阴天和雨天），每个分支代表被测试属性的值。叶节点（例如，游戏时间）表示对数值目标的决策。树中最顶层的决策节点对应于最佳预测器，称为**根节点**。决策树可以处理分类和数值数据。
- en: '![](../Images/8b2ade66e7dc5bcd27593b3bb3727d14.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b2ade66e7dc5bcd27593b3bb3727d14.jpg)'
- en: '**Decision Tree Algorithm**'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**决策树算法**'
- en: The core algorithm for building decision trees called **ID3** by J. R. Quinlan
    which employs a top-down, greedy search through the space of possible branches
    with no backtracking. The ID3 algorithm can be used to construct a decision tree
    for regression by replacing Information Gain with *Standard Deviation* *Reduction*.**Standard
    Deviation** A decision tree is built top-down from a root node and involves partitioning
    the data into subsets that contain instances with similar values (homogenous).
    We use standard deviation to calculate the homogeneity of a numerical sample.
    If the numerical sample is completely homogeneous its standard deviation is zero.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的核心算法称为**ID3**，由J.R. Quinlan提出，采用自顶向下、贪婪搜索的方式在可能的分支空间中进行搜索，没有回溯。ID3算法可用于构建回归决策树，将信息增益替换为*标准差减少*。**标准差**：决策树自顶向下构建，涉及将数据分成包含具有相似值的实例的子集（同质）。我们使用标准差来计算数值样本的同质性。如果数值样本完全同质，则其标准差为零。
- en: 'a) Standard deviation for **one** attribute:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: a) 一个属性的标准差：
- en: '![](../Images/05a7da6b8e1ca738e4de49b9f5d977a6.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05a7da6b8e1ca738e4de49b9f5d977a6.jpg)'
- en: 'b) Standard deviation for **two** attributes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: b) 两个属性的标准差：
- en: '![](../Images/767cce951a9cb7ad396414faae65b516.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/767cce951a9cb7ad396414faae65b516.jpg)'
- en: '**Standard Deviation Reduction**The standard deviation reduction is based on
    the decrease in standard deviation after a dataset is split on an attribute. Constructing
    a decision tree is all about finding attribute that returns the highest standard
    deviation reduction (i.e., the most homogeneous branches). *Step 1*: The standard
    deviation of the target is calculated.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准差减少**：标准差减少是基于数据集在属性上分裂后标准差的减少。构建决策树的关键在于找到返回最高标准差减少（即最一致的分支）的属性。*步骤1*：计算目标的标准差。'
- en: '**Standard deviation (Hours Played) = 9.32**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准差（游戏时间）= 9.32**'
- en: '*Step 2*: The dataset is then split on the different attributes. The standard
    deviation for each branch is calculated. The resulting standard deviation is subtracted
    from the standard deviation before the split. The result is the standard deviation
    reduction.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*：然后根据不同属性对数据集进行分裂。计算每个分支的标准差。将得到的标准差减去分裂前的标准差。结果就是标准差减少。'
- en: '![](../Images/590232f120da09aa428aefdba130b226.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/590232f120da09aa428aefdba130b226.jpg)'
- en: '![](../Images/bdb48a29345b0bf7f16b9bba35d6ff20.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdb48a29345b0bf7f16b9bba35d6ff20.jpg)'
- en: '*Step 3*: The attribute with the largest standard deviation reduction is chosen
    for the decision node.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤3*：选择具有最大标准差减少的属性作为决策节点。'
- en: '![](../Images/b6617e111fff6ec78b8b9a83facfdf1e.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6617e111fff6ec78b8b9a83facfdf1e.jpg)'
- en: '*Step 4a*: Dataset is divided based on the values of the selected attribute.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤4a*：根据所选属性的值对数据集进行划分。'
- en: '![](../Images/b794d9423480f843e5cc8f8fe345c1b6.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b794d9423480f843e5cc8f8fe345c1b6.jpg)'
- en: '*Step 4b*: A branch set with standard deviation more than 0 needs further splitting. 
    In practice, we need some termination criteria. For example, when standard deviation
    for the branch becomes smaller than a certain fraction (e.g., 5%) of standard
    deviation for the full dataset *OR* when too few instances remain in the branch
    (e.g., 3).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*第四步b*：标准差大于0的分支集需要进一步分割。实际操作中，我们需要一些终止准则。例如，当分支的标准差变得比整个数据集的标准差的某个分数（例如，5%）还要小，*或者*当分支中剩余的实例太少（例如，3）时。'
- en: '![](../Images/1989c677282b1c3f6eab8563f0838da3.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1989c677282b1c3f6eab8563f0838da3.jpg)'
- en: '*Step 5*: The process is run recursively on the non-leaf branches, until all
    data is processed. When the number of instances is more than one at a leaf node
    we calculate the *average* as the final value for the target.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*第五步*：该过程在非叶子节点上递归运行，直到所有数据被处理。当叶子节点上的实例数大于一个时，我们将*平均值*计算为目标的最终值。'
- en: '| [Exercise](decision_tree_reg_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/TreeReg.txt)
    |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [练习](decision_tree_reg_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/TreeReg.txt)
    |  |'
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a new algorithm
    to construct a decision tree from data using [MLR](mlr.htm) instead of average
    at the leaf node.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) 尝试发明一种新的算法，使用[MLR](mlr.htm)而不是在叶子节点上使用平均值来构建决策树。'
