["```\n # Original code is from https://github.com/spro/practical-pytorch\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom name_dataset import NameDataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Parameters and DataLoaders\nHIDDEN_SIZE = 100\nN_LAYERS = 2\nBATCH_SIZE = 256\nN_EPOCHS = 100\n\ntest_dataset = NameDataset(is_train_set=False)\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=BATCH_SIZE, shuffle=True)\n\ntrain_dataset = NameDataset(is_train_set=True)\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=BATCH_SIZE, shuffle=True)\n\nN_COUNTRIES = len(train_dataset.get_countries())\nprint(N_COUNTRIES, \"countries\")\nN_CHARS = 128  # ASCII\n\n# Some utility functions\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef create_variable(tensor):\n    # Do cuda() before wrapping with variable\n    if torch.cuda.is_available():\n        return Variable(tensor.cuda())\n    else:\n        return Variable(tensor)\n\n# pad sequences and sort the tensor\ndef pad_sequences(vectorized_seqs, seq_lengths, countries):\n    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n\n    # Sort tensors by their length\n    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n    seq_tensor = seq_tensor[perm_idx]\n\n    # Also sort the target (countries) in the same order\n    target = countries2tensor(countries)\n    if len(countries):\n        target = target[perm_idx]\n\n    # Return variables\n    # DataParallel requires everything to be a Variable\n    return create_variable(seq_tensor), \\\n        create_variable(seq_lengths), \\\n        create_variable(target)\n\n# Create necessary variables, lengths, and target\ndef make_variables(names, countries):\n    sequence_and_length = [str2ascii_arr(name) for name in names]\n    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n\ndef str2ascii_arr(msg):\n    arr = [ord(c) for c in msg]\n    return arr, len(arr)\n\ndef countries2tensor(countries):\n    country_ids = [train_dataset.get_country_id(\n        country) for country in countries]\n    return torch.LongTensor(country_ids)\n\nclass RNNClassifier(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.n_directions = int(bidirectional) + 1\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          bidirectional=bidirectional)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input, seq_lengths):\n        # Note: we run this all at once (over the whole input sequence)\n        # input shape: B x S (input size)\n        # transpose to make S(sequence) x B (batch)\n        input = input.t()\n        batch_size = input.size(1)\n\n        # Make a hidden\n        hidden = self._init_hidden(batch_size)\n\n        # Embedding S x B -> S x B x I (embedding size)\n        embedded = self.embedding(input)\n\n        # Pack them up nicely\n        gru_input = pack_padded_sequence(\n            embedded, seq_lengths.data.cpu().numpy())\n\n        # To compact weights again call flatten_parameters().\n        self.gru.flatten_parameters()\n        output, hidden = self.gru(gru_input, hidden)\n\n        # Use the last layer output as FC's input\n        # No need to unpack, since we are going to use hidden\n        fc_output = self.fc(hidden[-1])\n        return fc_output\n\n    def _init_hidden(self, batch_size):\n        hidden = torch.zeros(self.n_layers * self.n_directions,\n                             batch_size, self.hidden_size)\n        return create_variable(hidden)\n\n# Train cycle\ndef train():\n    total_loss = 0\n\n    for i, (names, countries) in enumerate(train_loader, 1):\n        input, seq_lengths, target = make_variables(names, countries)\n        output = classifier(input, seq_lengths)\n\n        loss = criterion(output, target)\n        total_loss += loss.data[0]\n\n        classifier.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 10 == 0:\n            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n                time_since(start), epoch,  i *\n                len(names), len(train_loader.dataset),\n                100. * i * len(names) / len(train_loader.dataset),\n                total_loss / i * len(names)))\n\n    return total_loss\n\n# Testing cycle\ndef test(name=None):\n    # Predict for a given name\n    if name:\n        input, seq_lengths, target = make_variables([name], [])\n        output = classifier(input, seq_lengths)\n        pred = output.data.max(1, keepdim=True)[1]\n        country_id = pred.cpu().numpy()[0][0]\n        print(name, \"is\", train_dataset.get_country(country_id))\n        return\n\n    print(\"evaluating trained model ...\")\n    correct = 0\n    train_data_size = len(test_loader.dataset)\n\n    for names, countries in test_loader:\n        input, seq_lengths, target = make_variables(names, countries)\n        output = classifier(input, seq_lengths)\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, train_data_size, 100. * correct / train_data_size))\n\nif __name__ == '__main__':\n\n    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n        classifier = nn.DataParallel(classifier)\n\n    if torch.cuda.is_available():\n        classifier.cuda()\n\n    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    start = time.time()\n    print(\"Training for %d epochs...\" % N_EPOCHS)\n    for epoch in range(1, N_EPOCHS + 1):\n        # Train cycle\n        train()\n\n        # Testing\n        test()\n\n        # Testing several samples\n        test(\"Sung\")\n        test(\"Jungwoo\")\n        test(\"Soojin\")\n        test(\"Nako\") \n```"]