- en: Twitter Streaming Language Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter Streaming Language Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this reference application, we show how you can use Apache Spark for training
    a language classifier - replacing a whole suite of tools you may be currently
    using.
  prefs: []
  type: TYPE_NORMAL
- en: 'This reference application was demo-ed at a meetup which is taped here - the
    link skips straight to demo time, but the talk before that is useful too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Aaron Davidson doing his magic](aaron-yaaay.png)](https://www.youtube.com/watch?v=FjhRkfAuU7I#t=2035)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are 5 typical stages for creating a production-ready classifier. Often,
    each stage is done with a different set of tools and even by different engineering
    teams:'
  prefs: []
  type: TYPE_NORMAL
- en: Scrape/collect a dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean and explore the data, doing feature extraction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a model on the data and iterate/improve it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improve the model using more and more data, perhaps upgrading your infrastructure
    to support building larger models. (Such as migrating over to Hadoop.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the model in real time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Spark can be used for all of the above and simple to use for all these purposes.
    We''ve chosen to break up the language classifier into 3 parts with one simple
    Spark program to accomplish each part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Collect a Dataset of Tweets](collect.html) - Spark Streaming is used to collect
    a dataset of tweets and write them out to files.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Examine the Tweets and Train a Model](examine_and_train.html) - Spark SQL
    is used to examine the dataset of Tweets. Then Spark MLLib is used to apply the
    K-Means algorithm to train a model on the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Apply the Model in Real-time](predict.html) - Spark Streaming and Spark MLLib
    are used to filter a live stream of Tweets for those that match the specified
    cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect a Dataset of Tweets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Part 1: Collect a Dataset of Tweets'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming is used to collect tweets as the dataset. The tweets are written
    out in JSON format, one tweet per line. A file of tweets is written every time
    interval until at least the desired number of tweets is collected.
  prefs: []
  type: TYPE_NORMAL
- en: See [Collect.scala](Collect.scala) for the full code. We'll walk through some
    of the interesting bits now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collect.scala takes in the following argument list:'
  prefs: []
  type: TYPE_NORMAL
- en: '*outputDirectory* - the output directory for writing the tweets. The files
    will be named ''part-%05d'''
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*numTweetsToCollect* - this is the minimum number of tweets to collect before
    the program exits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*intervalInSeconds* - write out a new set of tweets every interval.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*partitionsEachInterval* - this is used to control the number of output files
    written for each interval'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect.scala will also require [Twitter API Credentials](https://apps.twitter.com/).
    If you have never signed up for Twitter Api Credentials, follow these steps [here](https://databricks-training.s3.amazonaws.com/realtime-processing-with-spark-streaming.html#twitter-credential-setup).
    The Twitter credentials are passed in through command line flags.
  prefs: []
  type: TYPE_NORMAL
- en: Below is a snippet of the actual code in Collect.scala. The code calls TwitterUtils
    in the Spark Streaming Twitter library to get a DStream of tweets. Then, map is
    called to convert the tweets to JSON format. Finally, call for each RDD on the
    DStream. This example repartitions the RDD to write out so that you can control
    the number of output files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Run [Collect.scala](Collect.scala) yourself to collect a dataset of tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Examine the Tweets and Train a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Part 2: Examine Tweets and Train a Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second program examines the data found in tweets and trains a language
    classifier using K-Means clustering on the tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Examine](examine.html) - Spark SQL is used to gather data about the tweets
    -- to look at a few of them, and to count the total number of tweets for the most
    common languages of the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Train](train.html) - Spark MLLib is used for applying the K-Means algorithm
    for clustering the tweets. The number of clusters and the number of iterations
    of algorithm are configurable. After training the model, some sample tweets from
    the different clusters are shown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See [here](run_part2.html) for the command to run part 2.
  prefs: []
  type: TYPE_NORMAL
- en: Examine with Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Examine with Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL can be used to examine data based on the tweets. Below are some relevant
    code snippets from [ExamineAndTrain.scala](ExamineAndTrain.scala).
  prefs: []
  type: TYPE_NORMAL
- en: First, here is code to pretty print 5 sample tweets so that they are more human-readable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Spark SQL can load JSON files and infer the schema based on that data. Here
    is the code to load the json files, register the data in the temp table called
    "tweetTable" and print out the schema based on that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, look at the text of 10 sample tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: View the user language, user name, and text for 10 sample tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, show the count of tweets by user language. This can help determine
    the number of clusters that is ideal for this dataset of tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Train with Spark MLLib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Train with Spark MLLib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers how to train a language classifier using the text in the
    Tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to featurize the Tweet text. MLLib has a HashingTF class that
    does that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is the code that actually grabs the tweet text from the tweetTable and
    featurizes it. K-Means is called to create the number of clusters and the algorithm
    is run for the specified number of iterations. Finally, the trained model is persisted
    so it can be loaded later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Last, here is some code to take a sample set of tweets and print them out by
    cluster, so that we can see what language clusters our model contains. Pick your
    favorite to use for part 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Run Examine And Train
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Run Examine and Train
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run this program, the following argument list is required:'
  prefs: []
  type: TYPE_NORMAL
- en: YOUR_TWEET_INPUT - This is the file pattern for input tweets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OUTPUT_MODEL_DIR - This is the directory to persist the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NUM_CLUSTERS - The number of clusters the algorithm should create.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NUM_ITERATIONS - The number of iterations the algorithm should run for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is an example command to run [ExamineAndTrain.scala](ExamineAndTrain.scala):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Apply the Model in Real-time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Part 3: Apply the Model in Real Time'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark Streaming is used to filter live tweets coming in, only accepting those
    that are classified as the specified cluster (type) of tweets. It takes the following
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: modelDirectory - This the directory where the model that was trained in part
    2 was persisted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: clusterNumber - This is the cluster you want to select from part 2\. Only tweets
    that match this language cluster will be printed out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This program is very simple - this is the bulk of the code below. First, load
    up a Spark Streaming Context. Second, create a Twitter DStream and map it to grab
    the text. Third, load up the K-Means model that was trained in step 2\. Finally,
    apply the model on the tweets, filtering out only those that match the specified
    cluster, and print the matching tweets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run [Predict.scala](Predict.scala):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
