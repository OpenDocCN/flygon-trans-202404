- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Classification](classification.htm)
    > Decision Tree'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '[地图](data_mining_map.htm) > [数据科学](data_mining.htm) > [预测未来](predicting_the_future.htm)
    > [建模](modeling.htm) > [分类](classification.htm) > 决策树'
- en: Decision Tree - Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树 - 分类
- en: Decision tree builds classification or regression models in the form of a tree
    structure. It breaks down a dataset into smaller and smaller subsets while at
    the same time an associated decision tree is incrementally developed. The final
    result is a tree with **decision nodes** and **leaf nodes**. A decision node (e.g.,
    Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). Leaf node
    (e.g., Play) represents a classification or decision. The topmost decision node
    in a tree which corresponds to the best predictor called **root node**. Decision
    trees can handle both categorical and numerical data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树以树结构的形式构建分类或回归模型。它将数据集分解为越来越小的子集，同时逐步开发关联的决策树。最终结果是一个具有**决策节点**和**叶节点**的树。决策节点（例如，Outlook）有两个或更多分支（例如，晴天、阴天和雨天）。叶节点（例如，Play）表示分类或决策。树中最顶层的决策节点对应于最佳预测器，称为**根节点**。决策树可以处理分类和数值数据。
- en: '![](../Images/ef25d3a41270affc6b6f53f87eaec13a.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef25d3a41270affc6b6f53f87eaec13a.jpg)'
- en: '**Algorithm**'
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**算法**'
- en: The core algorithm for building decision trees called **ID3** by J. R. Quinlan
    which employs a top-down, greedy search through the space of possible branches
    with no backtracking. ID3 uses *Entropy* and *Information Gain* to construct a
    decision tree.**Entropy** A decision tree is built top-down from a root node and
    involves partitioning the data into subsets that contain instances with similar
    values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of
    a sample. If the sample is completely homogeneous the entropy is zero and if the
    sample is an equally divided it has entropy of one.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的核心算法称为**ID3**，由J.R. Quinlan提出，采用自顶向下、贪婪搜索的方式在可能的分支空间中进行搜索，没有回溯。ID3使用*熵*和*信息增益*构建决策树。**熵**决策树是从根节点自顶向下构建的，涉及将数据分成包含具有相似值的实例的子集（同质）。ID3算法使用熵来计算样本的同质性。如果样本完全同质，则熵为零，如果样本均匀分布，则熵为一。
- en: '![](../Images/546383720bd2bab50b9d4731680d1b2c.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/546383720bd2bab50b9d4731680d1b2c.jpg)'
- en: 'To build a decision tree, we need to calculate two types of entropy using frequency
    tables as follows:a) Entropy using the frequency table of one attribute:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建决策树，我们需要使用频率表计算两种类型的熵，如下所示：a) 使用一个属性的频率表计算熵：
- en: '![](../Images/27aad700b991c1155d60d0cffd1f4a2a.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27aad700b991c1155d60d0cffd1f4a2a.jpg)'
- en: 'b) Entropy using the frequency table of two attributes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: b) 使用两个属性的频率表计算熵：
- en: '![](../Images/36cea26858ed2db6dc5fb19370618f8a.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36cea26858ed2db6dc5fb19370618f8a.jpg)'
- en: '**Information Gain**The information gain is based on the decrease in entropy
    after a dataset is split on an attribute. Constructing a decision tree is all
    about finding attribute that returns the highest information gain (i.e., the most
    homogeneous branches). *Step 1*: Calculate entropy of the target.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息增益**信息增益是基于数据集在属性分裂后熵的减少。构建决策树的关键在于找到返回最高信息增益（即最同质分支）的属性。*第一步*：计算目标的熵。'
- en: '![](../Images/5f887af353cec512033d17101594f278.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f887af353cec512033d17101594f278.jpg)'
- en: '*Step 2*: The dataset is then split on the different attributes. The entropy
    for each branch is calculated. Then it is added proportionally, to get total entropy
    for the split. The resulting entropy is subtracted from the entropy before the
    split. The result is the Information Gain, or decrease in entropy.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二步*：然后在不同的属性上对数据集进行分裂。计算每个分支的熵。然后按比例添加，以获得分裂的总熵。然后从分裂前的熵中减去结果。结果就是信息增益，或熵的减少。'
- en: '![](../Images/b6a3e985ec57c450327c203016a51af5.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6a3e985ec57c450327c203016a51af5.jpg)'
- en: '![](../Images/083fcb954130077562edf6383c63b866.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/083fcb954130077562edf6383c63b866.jpg)'
- en: '*Step 3*: Choose attribute with the largest information gain as the decision
    node, divide the dataset by its branches and repeat the same process on every
    branch.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*第三步*：选择具有最大信息增益的属性作为决策节点，通过其分支划分数据集，并在每个分支上重复相同的过程。'
- en: '![](../Images/bafab4a2065b6287b5f0718cca3e7b04.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bafab4a2065b6287b5f0718cca3e7b04.jpg)'
- en: '![](../Images/2fda9792dbebef1a8c9bf8e585a523e5.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fda9792dbebef1a8c9bf8e585a523e5.jpg)'
- en: '*Step 4a*: A branch with entropy of 0 is a leaf node.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*第四步a*：熵为0的分支是叶节点。'
- en: '![](../Images/1de4ae9da982d4896fd7389ad6d7dbe3.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1de4ae9da982d4896fd7389ad6d7dbe3.jpg)'
- en: '*Step 4b*: A branch with entropy more than 0 needs further splitting.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4b*：熵大于 0 的分支需要进一步分割。'
- en: '![](../Images/52fb6e381be1f65efd92371f63365fe3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52fb6e381be1f65efd92371f63365fe3.jpg)'
- en: '*Step 5*: The ID3 algorithm is run recursively on the non-leaf branches, until
    all data is classified.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 5*：在非叶子分支上递归运行 ID3 算法，直到所有数据被分类。'
- en: '**Decision Tree to Decision Rules**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**决策树转换为决策规则**'
- en: A decision tree can easily be transformed to a set of rules by mapping from
    the root node to the leaf nodes one by one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从根节点到叶节点逐一映射，可以轻松将决策树转换为一组规则。
- en: '![](../Images/f35bad0201fa364b359bcf14aa1402e1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f35bad0201fa364b359bcf14aa1402e1.jpg)'
- en: '**Decision Trees - Issues**'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**决策树 - 问题**'
- en: Working with continuous attributes ([binning](binning.htm))
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理连续属性（[分箱](binning.htm)）
- en: '[Avoiding overfitting](decision_tree_overfitting.htm)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[避免过拟合](decision_tree_overfitting.htm)'
- en: '[Super Attributes](decision_tree_super.htm) (attributes with many values)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超级属性](decision_tree_super.htm)（具有多个值的属性）'
- en: Working with [missing values](missing_values.htm)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理[缺失值](missing_values.htm)
- en: '| [Exercise](decision_tree_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Tree.txt)
    |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [练习](decision_tree_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Tree.txt)
    |  |'
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a new algorithm
    to construct a decision tree from data using [Chi² test](categorical_categorical.htm).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) 尝试发明一种使用[卡方检验](categorical_categorical.htm)从数据构建决策树的新算法。'
