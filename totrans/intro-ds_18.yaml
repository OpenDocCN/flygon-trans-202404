- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Classification](classification.htm)
    > Decision Tree'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree - Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree builds classification or regression models in the form of a tree
    structure. It breaks down a dataset into smaller and smaller subsets while at
    the same time an associated decision tree is incrementally developed. The final
    result is a tree with **decision nodes** and **leaf nodes**. A decision node (e.g.,
    Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). Leaf node
    (e.g., Play) represents a classification or decision. The topmost decision node
    in a tree which corresponds to the best predictor called **root node**. Decision
    trees can handle both categorical and numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef25d3a41270affc6b6f53f87eaec13a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Algorithm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core algorithm for building decision trees called **ID3** by J. R. Quinlan
    which employs a top-down, greedy search through the space of possible branches
    with no backtracking. ID3 uses *Entropy* and *Information Gain* to construct a
    decision tree.**Entropy** A decision tree is built top-down from a root node and
    involves partitioning the data into subsets that contain instances with similar
    values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of
    a sample. If the sample is completely homogeneous the entropy is zero and if the
    sample is an equally divided it has entropy of one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/546383720bd2bab50b9d4731680d1b2c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To build a decision tree, we need to calculate two types of entropy using frequency
    tables as follows:a) Entropy using the frequency table of one attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27aad700b991c1155d60d0cffd1f4a2a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'b) Entropy using the frequency table of two attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36cea26858ed2db6dc5fb19370618f8a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Information Gain**The information gain is based on the decrease in entropy
    after a dataset is split on an attribute. Constructing a decision tree is all
    about finding attribute that returns the highest information gain (i.e., the most
    homogeneous branches). *Step 1*: Calculate entropy of the target.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f887af353cec512033d17101594f278.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 2*: The dataset is then split on the different attributes. The entropy
    for each branch is calculated. Then it is added proportionally, to get total entropy
    for the split. The resulting entropy is subtracted from the entropy before the
    split. The result is the Information Gain, or decrease in entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6a3e985ec57c450327c203016a51af5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/083fcb954130077562edf6383c63b866.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 3*: Choose attribute with the largest information gain as the decision
    node, divide the dataset by its branches and repeat the same process on every
    branch.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bafab4a2065b6287b5f0718cca3e7b04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/2fda9792dbebef1a8c9bf8e585a523e5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 4a*: A branch with entropy of 0 is a leaf node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1de4ae9da982d4896fd7389ad6d7dbe3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 4b*: A branch with entropy more than 0 needs further splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52fb6e381be1f65efd92371f63365fe3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Step 5*: The ID3 algorithm is run recursively on the non-leaf branches, until
    all data is classified.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Tree to Decision Rules**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree can easily be transformed to a set of rules by mapping from
    the root node to the leaf nodes one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f35bad0201fa364b359bcf14aa1402e1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Decision Trees - Issues**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with continuous attributes ([binning](binning.htm))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Avoiding overfitting](decision_tree_overfitting.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Super Attributes](decision_tree_super.htm) (attributes with many values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with [missing values](missing_values.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [Exercise](decision_tree_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Tree.txt)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a new algorithm
    to construct a decision tree from data using [ChiÂ² test](categorical_categorical.htm).'
  prefs: []
  type: TYPE_NORMAL
