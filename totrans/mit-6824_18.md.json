["```\nDatacenter\n\n    Frontend            Dynamo server\n    server\n                        Dynamo server\n    Frontend\n    server              Dynamo server\n\n                        Dynamo server \n```", "```\nDynamo: Amazon's Highly Available Key-value Store\nDeCandia et al, SOSP 2007\n\nWhy are we reading this paper?\n  Database, eventually consistent, write any replica\n    Like Ficus -- but a database! A surprising design.\n  A real system: used for e.g. shopping cart at Amazon\n  More available than PNUTS, Spanner, &c\n  Less consistent than PNUTS, Spanner, &c\n  Influential design; inspired e.g. Cassandra\n  2007: before PNUTS, before Spanner\n\nTheir Obsessions\n  SLA, e.g. 99.9th percentile of delay < 300 ms\n  constant failures\n  \"data centers being destroyed by tornadoes\"\n  \"always writeable\"\n\nBig picture\n  [lots of data centers, Dynamo nodes]\n  each item replicated at a few random nodes, by key hash\n\nWhy replicas at just a few sites? Why not replica at every site?\n  with two data centers, site failure takes down 1/2 of nodes\n    so need to be careful that *everything* replicated at *both* sites\n  with 10 data centers, site failure affects small fraction of nodes\n    so just need copies at a few sites\n\nConsequences of mostly remote access (since no guaranteed local copy)\n  most puts/gets may involve WAN traffic -- high delays\n    maybe distinct Dynamo instances with limited geographical scope?\n    paper quotes low average delays in graphs but does not explain\n  more vulnerable to network failure than PNUTS\n    again since no local copy\n\nConsequences of \"always writeable\"\n  always writeable => no master! must be able to write locally.\n  always writeable + failures = conflicting versions\n\nIdea #1: eventual consistency\n  accept writes at any replica\n  allow divergent replicas\n  allow reads to see stale or conflicting data\n  resolve multiple versions when failures go away\n    latest version if no conflicting updates\n    if conflicts, reader must merge and then write\n  like Bayou and Ficus -- but in a DB\n\nUnhappy consequences of eventual consistency\n  May be no unique \"latest version\"\n  Read can yield multiple conflicting versions\n  Application must merge and resolve conflicts\n  No atomic operations (e.g. no PNUTS test-and-set-write)\n\nIdea #2: sloppy quorum\n  try to get consistency benefits of single master if no failures\n    but allows progress even if coordinator fails, which PNUTS does not\n  when no failures, send reads/writes through single node\n    the coordinator\n    causes reads to see writes in the usual case\n  but don't insist! allow reads/writes to any replica if failures\n\nWhere to place data -- consistent hashing\n  [ring, and physical view of servers]\n  node ID = random\n  key ID = hash(key)\n  coordinator: successor of key\n    clients send puts/gets to coordinator\n  replicas at successors -- \"preference list\"\n  coordinator forwards puts (and gets...) to nodes on preference list\n\nWhy consistent hashing?\n  Pro\n    naturally somewhat balanced\n    decentralized -- both lookup and join/leave\n  Con (section 6.2)\n    not really balanced (why not?), need virtual nodes\n    hard to control placement (balancing popular keys, spread over sites)\n    join/leave changes partition, requires data to shift\n\nFailures\n  Tension: temporary or permanent failure?\n    node unreachable -- what to do?\n    if temporary, store new puts elsewhere until node is available\n    if permanent, need to make new replica of all content\n  Dynamo itself treats all failures as temporary\n\nTemporary failure handling: quorum\n  goal: do not block waiting for unreachable nodes\n  goal: put should always succeed\n  goal: get should have high prob of seeing most recent put(s)\n  quorum: R + W > N\n    never wait for all N\n    but R and W will overlap\n    cuts tail off delay distribution and tolerates some failures\n  N is first N *reachable* nodes in preference list\n    each node pings successors to keep rough estimate of up/down\n    \"sloppy\" quorum, since nodes may disagree on reachable\n  sloppy quorum means R/W overlap *not guaranteed*\n\ncoordinator handling of put/get:\n  sends put/get to first N reachable nodes, in parallel\n  put: waits for W replies\n  get: waits for R replies\n  if failures aren't too crazy, get will see all recent put versions\n\nWhen might this quorum scheme *not* provide R/W intersection?\n\nWhat if a put() leaves data far down the ring?\n  after failures repaired, new data is beyond N?\n  that server remembers a \"hint\" about where data really belongs\n  forwards once real home is reachable\n  also -- periodic \"merkle tree\" sync of key range\n\nHow can multiple versions arise?\n  Maybe a node missed the latest write due to network problem\n  So it has old data, should be superseded by newer put()s\n  get() consults R, will likely see newer version as well as old\n\nHow can *conflicting* versions arise?\n  N=3 R=2 W=2\n  shopping cart, starts out empty \"\"\n  preference list n1, n2, n3, n4\n  client 1 wants to add item X\n    get(cart) from n1, n2, yields \"\"\n    n1 and n2 fail\n    put(cart, \"X\") goes to n3, n4\n  client 2 wants to delete X\n    get(cart) from n3, n4, yields \"X\"\n    put(cart, \"\") to n3, n4\n  n1, n2 revive\n  client 3 wants to add Y\n    get(cart) from n1, n2 yields \"\"\n    put(cart, \"Y\") to n1, n2\n  client 3 wants to display cart\n    get(cart) from n1, n3 yields two values!\n      \"X\" and \"Y\"\n      neither supersedes the other -- the put()s conflicted\n\nHow should clients resolve conflicts on read?\n  Depends on the application\n  Shopping basket: merge by taking union?\n    Would un-delete item X\n    Weaker than Bayou (which gets deletion right), but simpler\n  Some apps probably can use latest wall-clock time\n    E.g. if I'm updating my password\n    Simpler for apps than merging\n  Write the merged result back to Dynamo\n\nHow to detect whether two versions conflict?\n  As opposed to a newer version superseding an older one\n  If they are not bit-wise identical, must client always merge+write?\n  We have seen this problem before...\n\nVersion vectors\n  Example tree of versions:\n    [a:1]\n           [a:1,b:2]\n    VVs indicate v1 supersedes v2\n    Dynamo nodes automatically drop [a:1] in favor of [a:1,b:2]\n  Example:\n    [a:1]\n           [a:1,b:2]\n    [a:2]\n    Client must merge\n\nget(k) may return multiple versions, along with \"context\"\n  and put(k, v, context)\n  put context tells coordinator which versions this put supersedes/merges\n\nWon't the VVs get big?\n  Yes, but slowly, since key mostly served from same N nodes\n  Dynamo deletes least-recently-updated entry if VV has > 10 elements\n\nImpact of deleting a VV entry?\n  won't realize one version subsumes another, will merge when not needed:\n    put@b: [b:4]\n    put@a: [a:3, b:4]\n    forget b:4: [a:3]\n    now, if you sync w/ [b:4], looks like a merge is required\n  forgetting the oldest is clever\n    since that's the element most likely to be present in other branches\n    so if it's missing, forces a merge\n    forgetting *newest* would erase evidence of recent difference\n\nIs client merge of conflicting versions always possible?\n  Suppose we're keeping a counter, x\n  x starts out 0\n  incremented twice\n  but failures prevent clients from seeing each others' writes\n  After heal, client sees two versions, both x=1\n  What's the correct merge result?\n  Can the client figure it out?\n\nWhat if two clients concurrently write w/o failure?\n  e.g. two clients add diff items to same cart at same time\n  Each does get-modify-put\n  They both see the same initial version\n  And they both send put() to same coordinator\n  Will coordinator create two versions with conflicting VVs?\n    We want that outcome, otherwise one was thrown away\n    Paper doesn't say, but coordinator could detect problem via put() context\n\nPermanent server failures / additions?\n  Admin manually modifies the list of servers\n  System shuffles data around -- this takes a long time!\n\nThe Question:\n  It takes a while for notice of added/deleted server to become known\n    to all other servers. Does this cause trouble?\n  Deleted server might get put()s meant for its replacement.\n  Deleted server might receive get()s after missing some put()s.\n  Added server might miss some put()s b/c not known to coordinator.\n  Added server might serve get()s before fully initialized.\n  Dynamo probably will do the right thing:\n    Quorum likely causes get() to see fresh data as well as stale.\n    Replica sync (4.7) will fix missed get()s.\n\nIs the design inherently low delay?\n  No: client may be forced to contact distant coordinator\n  No: some of the R/W nodes may be distant, coordinator must wait\n\nWhat parts of design are likely to help limit 99.9th pctile delay?\n  This is a question about variance, not mean\n  Bad news: waiting for multiple servers takes *max* of delays, not e.g. avg\n  Good news: Dynamo only waits for W or R out of N\n    cuts off tail of delay distribution\n    e.g. if nodes have 1% chance of being busy with something else\n    or if a few nodes are broken, network overloaded, &c\n\nNo real Eval section, only Experience\n\nHow does Amazon use Dynamo?\n  shopping cart (merge)\n  session info (maybe Recently Visited &c?) (most recent TS)\n  product list (mostly r/o, replication for high read throughput)\n\nThey claim main advantage of Dynamo is flexible N, R, W\n  What do you get by varying them?\n  N-R-W\n  3-2-2 : default, reasonable fast R/W, reasonable durability\n  3-3-1 : fast W, slow R, not very durable, not useful?\n  3-1-3 : fast R, slow W, durable\n  3-3-3 : ??? reduce chance of R missing W?\n  3-1-1 : not useful?\n\nThey had to fiddle with the partitioning / placement / load balance (6.2)\n  Old scheme:\n    Random choice of node ID meant new node had to split old nodes' ranges\n    Which required expensive scans of on-disk DBs\n  New scheme:\n    Pre-determined set of Q evenly divided ranges\n    Each node is coordinator for a few of them\n    New node takes over a few entire ranges\n    Store each range in a file, can xfer whole file\n\nHow useful is ability to have multiple versions? (6.3)\n  I.e. how useful is eventual consistency\n  This is a Big Question for them\n  6.3 claims 0.001% of reads see divergent versions\n    I believe they mean conflicting versions (not benign multiple versions)\n    Is that a lot, or a little?\n  So perhaps 0.001% of writes benefitted from always-writeable?\n    I.e. would have blocked in primary/backup scheme?\n  Very hard to guess:\n    They hint that the problem was concurrent writers, for which\n      better solution is single master\n    But also maybe their measurement doesn't count situations where\n      availability would have been worse if single master\n\nPerformance / throughput (Figure 4, 6.1)\n  Figure 4 says average 10ms read, 20 ms writes\n    the 20 ms must include a disk write\n    10 ms probably includes waiting for R/W of N\n  Figure 4 says 99.9th pctil is about 100 or 200 ms\n    Why?\n    \"request load, object sizes, locality patterns\"\n    does this mean sometimes they had to wait for coast-coast msg? \n\nPuzzle: why are the average delays in Figure 4 and Table 2 so low?\n  Implies they rarely wait for WAN delays\n  But Section 6 says \"multiple datacenters\"\n    you'd expect *most* coordinators and most nodes to be remote!\n    Maybe all datacenters are near Seattle?\n\nWrap-up\n  Big ideas:\n    eventual consistency\n    always writeable despite failures\n    allow conflicting writes, client merges\n  Awkward model for some applications (stale reads, merges)\n    this is hard for us to tell from paper\n  Maybe a good way to get high availability + no blocking on WAN\n    but PNUTS master scheme implies Yahoo thinks not a problem\n  No agreement on whether eventual consistency is good for storage systems \n```"]