- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Classification](classification.htm)
    > Naive Bayesian'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayesian
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayesian classifier is based on Bayes� theorem with independence assumptions
    between predictors. A Naive Bayesian model is easy to build, with no complicated
    iterative parameter estimation which makes it particularly useful for very large
    datasets. Despite its simplicity, the Naive Bayesian classifier often does surprisingly
    well and is widely used because it often outperforms more sophisticated classification
    methods.    **Algorithm** Bayes theorem provides a way of calculating the posterior
    probability, *P*(*c|x*), from *P*(*c*), *P*(*x*), and *P*(*x|c*). Naive Bayes
    classifier assume that the effect of the value of a predictor (*x*) on a given
    class (*c*) is independent of the values of other predictors. This assumption
    is called class conditional independence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4678df9e3bf0b0fc0f8b371177eee1c7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*P*(*c|x*) is the posterior probability of *class* (*target*) given *predictor*
    (*attribute*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*c*) is the prior probability of *class*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*x|c*) is the likelihood which is the probability of *predictor* given
    *class*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*x*) is the prior probability of *predictor*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example*: The posterior probability can be calculated by first, constructing
    a frequency table for each attribute against the target. Then, transforming the
    frequency tables to likelihood tables and finally use the Naive Bayesian equation
    to calculate the posterior probability for each class. The class with the highest
    posterior probability is the outcome of prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6228bafabe2053dd51ce726c37db0382.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**The zero-frequency problem**Add 1 to the count for every attribute value-class
    combination (*Laplace estimator*) when an attribute value (*Outlook=Overcast*)
    doesn�t occur with every class value (*Play Golf=no*).  **Numerical Predictors**Numerical
    variables need to be transformed to their categorical counterparts ([binning](binning.htm))
    before constructing their frequency tables. The other option we have is using
    the distribution of the numerical variable to have a good guess of the frequency.
    For example, one common practice is to assume normal distributions for numerical
    variables.The probability density function for the normal distribution is defined
    by two parameters (mean and standard deviation).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1982d4d8246fb5335b82b07ce1c6675a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Example*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | **Humidity** |  | *Mean* | *StDev* |'
  prefs: []
  type: TYPE_TB
- en: '| **Play Golf** | yes | 86 | 96 | 80 | 65 | 70 | 80 | 70 | 90 | 75 | 79.1 |
    10.2 |'
  prefs: []
  type: TYPE_TB
- en: '| no | 85 | 90 | 70 | 95 | 91 |  |  |  |  | 86.2 | 9.7 |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/11442024bd51c64b4bd1ed787fea8bae.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Predictors Contribution**Kononenko''s *information gain* as a sum of information
    contributed by each attribute can offer an explanation on how values of the predictors
    influence the class probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f7c85e88a9694fc9eec8ff1046a13c1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The contribution of predictors can also be visualized by plotting [*nomograms*](further_readings.htm).
    Nomogram plots log odds ratios for each value of each predictor. Lengths of the
    lines correspond to spans of odds ratios, suggesting importance of the related
    predictor. It also shows impacts of individual values of the predictor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87e7c61d498c388e195721875318eb2b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '| [Exercise](naive_bayesian_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Bayes.txt)
    | ![](../Images/dc9f5f2d562c6ce8cb7def0d0596abff.jpg)[Naive Bayesian Interactive](flash/Bayesian.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a real time
    Bayesian classifier. You should be able to add or remove data and variables (predictors
    and classes) on the fly.'
  prefs: []
  type: TYPE_NORMAL
