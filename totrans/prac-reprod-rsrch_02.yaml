- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preface: Nullius in Verba'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: P.B. Stark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The origins of the scientific method, epitomized by Sir Francis Bacon''s work
    in the early 1600s, amount to insistence on direct evidence. This is reflected
    in the motto of The Royal Society, founded in 1660: *Nullius in verba*, which
    roughly means "take nobody''s word for it" (The Royal Society, 2016). Fellows
    of the Royal Society did not consider a claim to be scientifically established
    unless it had been demonstrated experimentally in front of a group of observers
    (other fellows), who could see with their own eyes what happened (Shapin & Schaffer,
    2011). Over time, Robert Boyle and others developed conventions for documenting
    experiments in sufficient detail, using prose and illustrations of the apparatus
    and experimental set up, that the reader could imagine being in the room, observing
    the experiment and its outcome.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Such observability---visibility into the process of generating results---provides
    the evidence that the scientific claim is true. It helps ensure we are not fooling
    ourselves or each other, accidentally or deliberately. It is a safeguard against
    error and fraud, and a springboard for progress, enabling others to replicate
    the experiment, to refine or improve the experiment, and to leverage the techniques
    to answer new questions. It generates and promulgates scientific knowledge *and*
    the means of generating scientific knowledge.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: However, science has largely abandoned that transparency and observability,
    resulting in a devolution from *show me* to *trust me.* Scientific publications
    simply do not contain the information needed to know what was done, nor to try
    to replicate the experiment and data analysis. Peer reviewers and journal editors,
    the gatekeepers we rely upon to ensure the correctness of published results, cannot
    possibly vet submissions well, because they are not provided enough information
    to do the job. There are many reasons for this regression, among them, the rise
    of Big Science, the size of many modern data sets, the complexity of modern data
    analysis and the software tools used for data analysis, and draconian limits on
    the length of articles and even on electronic supplemental material. But as a
    consequence, most scientific publications provide little scientific evidence for
    the results they report.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'It is impractial or impossible to repeat some experiments from scratch: who
    can afford to replicate CERN, the Hubble Space Telescope, or the National Health
    and Nutrition Examination Survey? Some data sets are too large to move efficiently,
    or contain information restricted by law or ethics. Lack of access to the underlying
    data obviously makes it impossible to replicate data analysis. But even when the
    data are available, reliance on proprietary software or point-and-click tools
    and failure to publish code make it impossible to know exactly what was done to
    the data to generate the figures and tables in most scientific publications.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The (unfortunately rare) attempts to replicate experiments or data analyses
    often fail to support the original claims (Lehrer, 2010; Open Science Collaboration,
    2015) Why?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: One reason is the interaction between scientific publishing and statistics.
    Because journals are generally uninterested in publishing negative results or
    replications of positive results, the emphasis is on "discoveries." Selecting
    data, hypotheses, data analyses, and results to produce (apparently) positive
    results inflates the apparent signal-to-noise ratio and overstates statistical
    significance. The ability to automate many aspects of data analysis, such as feature
    selection and model selection, combined with the large number of variables measured
    in many modern studies and experiments, including "omics," high-energy physics,
    and sensor networks, make it essentially inevitable that many "discoveries" will
    be wrong (Ioannidis, 2005). A primary defense against being misled by this selection
    process, which includes *p*-hacking and the "file-drawer effect" (Nuzzo, 2015;
    Rosenthal, 1979), is to insist that researchers disclose what they tried before
    arriving at the analysis they chose to report or to emphasize.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'I would argue that if a paper does not provide enough information to assess
    whether its results are correct, it is something other than science. Consequently,
    I think scientific journals and the peer review system must change radically:
    referees and editors should not "bless" work they cannot check because the authors
    did not provide enough information, including making available the software used
    to analyze the data. And scientific journals should not publish such work.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: A crucial component of the chain of evidence is the software used to process
    and analyze the data. Modern data analysis typically involves dozens, if not hundreds
    of steps, each of which can be performed by numerous algorithms that are nominally
    identical but differ in detail, and each of which involves at least some ad hoc
    choices. If researchers do not make their code available, there is little hope
    of ever knowing what was done to the data, much less assessing whether it was
    the right thing to do.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And most software has bugs. For instance, a 2014 study by Coverity, based on
    code-scanning algorithms, found 0.61 errors per 1,000 lines of source code in
    open-source projects and 0.76 errors per 1,000 lines of source code in commercial
    software (Synopsys, 2015). Scientific software is not an exception, and few scientists
    use sound software engineering practices, such as rigorous testing---or even version
    control (Merali, 2010; Soergel, 2015). Using point-and-click tools, rather than
    scripted analyses, makes it easier to commit errors and harder to find them. One
    recent calamity attributable in part to poor computational practice is the work
    of Reinhart and Rogoff (2010), which was used to justify economic austerity measures
    in southern Europe. Errors in their Excel spreadsheet led to the wrong conclusion
    (Herndon & Pollin, 2014). If they had scripted their analysis and tested the code
    instead of using spreadsheet software, their errors might have been avoided, discovered,
    or corrected before harm was done.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数软件都存在错误。例如，Coverity在2014年进行的一项研究，基于代码扫描算法，在开源项目中每1,000行源代码中发现0.61个错误，在商业软件中每1,000行源代码中发现0.76个错误（Synopsys,
    2015）。科学软件并非例外，很少有科学家使用严格的软件工程实践，如严格测试——甚至版本控制（Merali, 2010; Soergel, 2015）。使用点击工具而不是脚本分析，使得犯错误更容易，发现错误更困难。部分归因于糟糕的计算实践的最近灾难之一是Reinhart和Rogoff（2010）的工作，该工作被用来证明南欧实施经济紧缩措施的合理性。他们Excel电子表格中的错误导致了错误的结论（Herndon
    & Pollin, 2014）。如果他们脚本化他们的分析并测试代码，而不是使用电子表格软件，他们的错误可能已经被避免、发现或纠正，从而避免造成伤害。
- en: Working reproducibly makes it easier to get correct results and enables others
    to check whether results are correct. This volume focuses on how researchers in
    a broad spectrum of scientific applications document and reveal what they did
    to their data to arrive at their figures, tables, and scientific conclusions;
    that is, how they make the computational portion of their work more transparent
    and reproducible. This enables others to assess crucial aspects of the evidence
    that their scientific claims are correct, and to repeat, improve, and repurpose
    analyses and intellectual contributions embodied in software artifacts. Infrastructure
    to make code and data available in useful forms needs more development, but much
    is possible already, as these vignettes show. The contributors share how their
    workflows and tools enable them to work more transparently and reproducibly, and
    call out "pain points" where new tools and processes might make things easier.
    Whether you are an astrophysicist, an ecologist, a sociologist, a statistician,
    or a nuclear engineer, there is likely something between these covers that will
    interest you, and something you will find useful to make your own work more transparent
    and replicable.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 可复现的工作使得获得正确结果更容易，并使他人能够检查结果是否正确。本卷集中讨论了广泛领域的科学应用研究人员如何记录和展示他们对数据的处理过程，以得出图表和科学结论；也就是说，他们如何使他们工作的计算部分更透明和可复现。这使得他人能够评估他们科学主张正确性的关键方面，并重复、改进和重新利用嵌入在软件工件中的分析和知识贡献。使代码和数据以有用形式可用的基础设施需要更多的发展，但已经有很多可能性，正如这些小插曲所展示的那样。贡献者分享了他们的工作流程和工具如何使他们更透明和可复现，并指出了可能使事情变得更容易的“痛点”。无论您是天体物理学家、生态学家、社会学家、统计学家还是核工程师，这些内容中都可能有您感兴趣的内容，也有一些对您有用的内容，可以使您的工作更透明和可复制。
- en: References
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: Herndon, M. A., T., & Pollin, R. (2014). Does high public debt consistently
    stifle economic growth? A critique of reinhart and rogoff. *Cambridge Journal
    of Economics*, *38*, 257–279.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Herndon, M. A., T., & Pollin, R. (2014). 高公共债务是否一直会扼杀经济增长？对莱因哈特和罗戈夫的批评。*剑桥经济学杂志*,
    *38*, 257–279.
- en: Ioannidis, J. (2005). Why most published research findings are false. *PLoS
    Medicine*, *2*(8), e124.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Ioannidis, J. (2005). 为什么大多数发表的研究结果都是错误的。*PLoS Medicine*, *2*(8), e124.
- en: Lehrer, J. (2010). The truth wears off. *The New Yorker*. Retrieved from [http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off](http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Lehrer, J. (2010). 真相逐渐消失。*纽约客*。检索自[http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off](http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off)
- en: 'Merali, Z. (2010). Computational science: . . . Error . . . why scientific
    programming does not compute. *Nature*, *467*, 775–777.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Merali, Z. (2010). 计算科学：……错误……为什么科学编程无法计算。*自然*, *467*, 775–777.
- en: Nuzzo, R. (2015). How scientists fool themselves – and how they can stop. *Nature*,
    *526*, 182–185.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Nuzzo, R. (2015). 科学家如何欺骗自己——以及他们如何停止。*自然*, *526*, 182–185.
- en: Open Science Collaboration. (2015). Estimating the reproducibility of psychological
    science. *Science*, *349*, 943.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 开放科学合作. (2015). 估计心理科学的可重复性。*科学*, *349*, 943.
- en: Reinhart, C., & Rogoff, K. (2010). Growth in a time of debt. *American Economic
    Review*, *100*, 573–578.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Reinhart, C., & Rogoff, K. (2010). 债务时代的增长。*美国经济评论*, *100*, 573–578.
- en: Rosenthal, R. (1979). The “file drawer problem” and tolerance for null results.
    *Psychological Bulletin*, *86*(3), 638–641.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenthal, R. (1979). “文件抽屉问题”和对零结果的容忍度。*心理学公报*, *86*(3), 638–641.
- en: 'Shapin, S., & Schaffer, S. (2011). *Leviathan and the air-pump: Hobbes, boyle,
    and the experimental life*. Princeton, NJ: Princeton University Press.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Shapin, S., & Schaffer, S. (2011). *利维坦与气泵：霍布斯、博伊尔和实验生活*. 新泽西州普林斯顿：普林斯顿大学出版社。
- en: Soergel, D. (2015). Rampant software errors may undermine scientific results.
    *F1000Research*, *3*, 303.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Soergel, D. (2015). 肆虐的软件错误可能会削弱科学结果。*F1000Research*, *3*, 303.
- en: Synopsys. (2015). Coverity scan open source report 2014\. Retrieved from [http://go.coverity.com/rs/157-LQW-289/images/2014-Coverity-Scan-Report.pdf](http://go.coverity.com/rs/157-LQW-289/images/2014-Coverity-Scan-Report.pdf)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Synopsys. (2015). Coverity scan开源报告2014\. 检索自 [http://go.coverity.com/rs/157-LQW-289/images/2014-Coverity-Scan-Report.pdf](http://go.coverity.com/rs/157-LQW-289/images/2014-Coverity-Scan-Report.pdf)
- en: The Royal Society. (2016). The royal society | history. Retrieved from [https://royalsociety.org/about-us/history/](https://royalsociety.org/about-us/history/)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 皇家学会. (2016). 皇家学会 | 历史。检索自 [https://royalsociety.org/about-us/history/](https://royalsociety.org/about-us/history/)
