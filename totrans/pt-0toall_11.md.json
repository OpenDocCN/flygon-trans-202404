["```\n # https://github.com/pytorch/examples/blob/master/mnist/main.py\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nbatch_size = 64\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./data/',\n                               train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='./data/',\n                              train=False,\n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.mp = nn.MaxPool2d(2)\n        self.fc = nn.Linear(320, 10)\n\n    def forward(self, x):\n        in_size = x.size(0)\n        x = F.relu(self.mp(self.conv1(x)))\n        x = F.relu(self.mp(self.conv2(x)))\n        x = x.view(in_size, -1)  # flatten the tensor\n        x = self.fc(x)\n        return F.log_softmax(x)\n\nmodel = Net()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        # sum up batch loss\n        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n        # get the index of the max log-probability\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\nfor epoch in range(1, 10):\n    train(epoch)\n    test() \n```"]