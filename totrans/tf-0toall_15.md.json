["```\n# Lab 6 Softmax Classifier\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [[1, 2, 1, 1],\n          [2, 1, 3, 2],\n          [3, 1, 3, 4],\n          [4, 1, 5, 5],\n          [1, 7, 5, 5],\n          [1, 2, 5, 6],\n          [1, 6, 6, 6],\n          [1, 7, 7, 7]]\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\nX = tf.placeholder(\"float\", [None, 4])\nY = tf.placeholder(\"float\", [None, 3])\nnb_classes = 3\n\nW = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\nb = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n\n# tf.nn.softmax computes softmax activations\n# softmax = exp(logits) / reduce_sum(exp(logits), dim)\nhypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n\n# Cross entropy cost/loss\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Launch graph\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(2001):\n        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n        if step % 200 == 0:\n            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n\n    print('--------------')\n\n    # Testing & One-hot encoding\n    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n    print(a, sess.run(tf.argmax(a, 1)))\n\n    print('--------------')\n\n    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n    print(b, sess.run(tf.argmax(b, 1)))\n\n    print('--------------')\n\n    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n    print(c, sess.run(tf.argmax(c, 1)))\n\n    print('--------------')\n\n    all = sess.run(hypothesis, feed_dict={\n                   X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n    print(all, sess.run(tf.argmax(all, 1)))\n\n'''\n--------------\n[[  1.38904958e-03   9.98601854e-01   9.06129117e-06]] [1]\n--------------\n[[ 0.93119204  0.06290206  0.0059059 ]] [0]\n--------------\n[[  1.27327668e-08   3.34112905e-04   9.99665856e-01]] [2]\n--------------\n[[  1.38904958e-03   9.98601854e-01   9.06129117e-06]\n [  9.31192040e-01   6.29020557e-02   5.90589503e-03]\n [  1.27327668e-08   3.34112905e-04   9.99665856e-01]] [1 0 2]\n''' \n```"]