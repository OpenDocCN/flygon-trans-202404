- en: Feature Extraction and Data Wrangling for Predictive Models of the Brain in
    Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 Python 中大脑预测模型的特征提取和数据整理
- en: Feature Extraction and Data Wrangling for Predictive Models of the Brain in
    Python
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 Python 中大脑预测模型的特征提取和数据整理
- en: Chris Holdgraf
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克里斯·霍德格拉夫
- en: My name is Chris Holdgraf, I am a senior graduate student with the Helen Wills
    Neuroscience Institute at UC Berkeley. My thesis work involves using predictive
    models to understand how auditory regions of the brain respond to acoustic features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是克里斯·霍德格拉夫，我是加州大学伯克利分校海伦·威尔斯神经科学研究所的高年级研究生。我的论文工作涉及使用预测模型来理解大脑听觉区对声学特征的反应方式。
- en: I am interested in how experience, learning, and assumptions about the world
    shape the way that we interact with low level features of sound. This involves
    a lot of computational work, signal processing, and data cleaning, utilizing a
    number of packages in the Python scientific ecosystem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我对经验、学习和对世界的假设如何塑造我们与声音低级特征互动的方式感兴趣。这涉及大量的计算工作、信号处理和数据清理，利用了 Python 科学生态系统中的许多软件包。
- en: Workflow
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作流程
- en: '![Diagram](choldgraf.png) My workflow involves taking raw data from a variety
    of sources, doing a few steps of munging on each one separately, combining them
    into a common structured format, and then doing further processing on this data.
    Here''s a general breakdown:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示](choldgraf.png) 我的工作流程涉及从各种来源获取原始数据，对每个数据进行几个步骤的处理，将它们合并为一种通用的结构化格式，然后对这些数据进行进一步处理。以下是一般的分解：'
- en: Wrangling raw data
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 整理原始数据
- en: The raw data for my work involves electrophysiology signals collected from the
    brains of surgical patients in several sites across the country. This is challenging
    because the nature of the data is quite different from one location to another.
    I often get the neural data in many different formats, and a loose collection
    of metadata (e.g., sensor locations, names, sampling rates) that can be anything
    from a PDF of hand-written notes to a structured text file. As such, the first
    step in my workflow is to wrangle all of this data into some kind of structured
    format.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我的工作原始数据涉及从全国各地手术患者的大脑中收集的电生理信号。这很具有挑战性，因为数据的性质在不同地点之间有很大不同。我经常以许多不同的格式获取神经数据，并且松散地收集了一些元数据（例如，传感器位置、名称、采样率），这些元数据可以是从手写笔记的
    PDF 到结构化文本文件的任何东西。因此，我工作流程的第一步是将所有这些数据整理成某种结构化格式。
- en: First, I put all data into subject-specific folders. Each of these folders has
    sub-folders for different kinds of data (e.g., "raw", "munged", "meta"). The sub-folders
    will eventually be populated with data during processing, and the structure is
    consistent across all subjects so that I can easily parse them with scripts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将所有数据放入特定于主题的文件夹中。每个文件夹都有不同种类数据的子文件夹（例如，“原始”，“处理过的”，“元”）。子文件夹最终将在处理过程中填充数据，并且结构在所有主题中保持一致，以便我可以轻松地使用脚本解析它们。
- en: Next, I have a Jupyter notebook that is unique for each person, and is designed
    to take whatever raw format the data is in and turn it into a standardized version.
    This is called `munge_{subject_name}.ipynb`, and will output a file that I can
    use for the rest of my analyses. Jupyter notebooks are useful here because each
    subject is different, and will require a different set of steps to get the data
    ready. For this reason, I like to have lots of plots that go along with the analysis
    process, and a record of exactly what code was run to create the munged data for
    that subject.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我有一个独特于每个人的 Jupyter 笔记本，旨在将数据处于任何原始格式并将其转换为标准化版本。这称为 `munge_{subject_name}.ipynb`，并将输出一个文件，我可以用于我的其他分析。Jupyter
    笔记本在这里很有用，因为每个主题都不同，并且需要不同的步骤来准备数据。出于这个原因，我喜欢有很多与分析过程相关的图形，以及创建该主题的处理过的数据的确切代码记录。
- en: Because the data often comes in different formats, I make the output of this
    step the same format for everyone. I use a Python package for neuroscience electrophysiology
    called `MNE-python`. This provides a common way of structuring data in order to
    streamline I/O, processing, and data analysis. I convert all of my raw data into
    the `.fif` format, which is a standard format for storing electrophysiology data.
    This means that I can read the data into other platforms (e.g., R or Matlab) fairly
    easily. The output files are stored in the folder `subject_name/munged`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据通常以不同的格式提供，我让此步骤的输出对所有人都是相同的格式。我使用了一个名为`MNE-python`的神经科学电生理Python包。这提供了一种通用的数据结构方式，以便简化输入/输出、处理和数据分析。我将所有的原始数据转换成`.fif`格式，这是一种存储电生理数据的标准格式。这意味着我可以相当容易地将数据读入其他平台（例如，R或Matlab）。输出文件存储在文件夹`subject_name/munged`中。
- en: The final thing I do in this step is look at the metadata files for this subject,
    double check that all the values inside are correct (this is done with the munging
    notebook), and then insert them into a "combined" csv file of metadata. I have
    a Python script called `create_combined_meta.py` that will look through all the
    subject folders, find the metadata files that my munging notebook outputs, and
    turn them all into a single CSV file.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我要做的最后一件事情是查看这个主题的元数据文件，仔细检查里面的所有值是否正确（这是通过数据处理笔记本完成的），然后将它们插入到一个元数据的“合并”CSV文件中。我有一个名为`create_combined_meta.py`的Python脚本，它会遍历所有主题文件夹，找到我的数据处理笔记本输出的元数据文件，并将它们全部转换为单个CSV文件。
- en: This aggregated CSV has data for all subjects that I have, and makes it much
    easier to quickly look at information across datasets. To do so, I use `Pandas`.
    This is a package that lets you represent tabular-style data in memory, and also
    gives you "database-style" functionality with their objects (e.g., joining two
    `Pandas` objects with partially-overlapping fields). Doing this necessitates that
    all of my data is in "tidy" format. This simplifies things, because it means that
    while I have a separate file for each dataset, I have a single combined file across
    all subjects for their metadata.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个汇总的CSV文件包含了我拥有的所有主题的数据，并且使得快速查看跨数据集的信息变得更加容易。为了这样做，我使用`Pandas`。这是一个让你在内存中表示类似表格的数据的包，并且还通过它们的对象提供了“数据库风格”的功能（例如，将两个部分重叠字段的`Pandas`对象连接在一起）。这样做需要所有的数据都是“整洁”格式的。这简化了事情，因为这意味着虽然我对每个数据集有一个单独的文件，但我对它们的元数据有一个跨所有主题的单一组合文件。
- en: I should note that this is the point where somebody usually suggests using a
    "proper" database like SQL instead of keeping my data in CSV / FIFF formats. I've
    found that the overhead added by reformatting my data for something like SQL isn't
    worth the benefit it would give. If I were to start a new project - particularly
    with larger datasets - then I would likely consider a more robust data storage
    solution like SQL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该指出，这是有人通常建议使用“适当的”数据库，如SQL，而不是将我的数据保留在CSV / FIFF格式中的时候。我发现，为了将我的数据重新格式化为类似SQL的东西所增加的开销不值得它所带来的好处。如果我要开始一个新项目
    - 尤其是使用更大的数据集 - 那么我可能会考虑一个更健壮的数据存储解决方案，如SQL。
- en: Cleaning the data
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 清理数据
- en: Once I've created my munged data, I can now use a single script for processing/analyzing
    all datasets. In the field of cognitive neuroscience, there are common preprocessing
    techniques that are carried out in order to improve the quality of the data (e.g.,
    a few filtering steps and rejecting channels that are too noisy). I have a `clean_data.py`
    file that will look through the "munged" folder of each subject, load the data,
    run the cleaning, and then output the results to the folder `{subject_name}/clean`.
    This way, I know that any data in the clean folder is ready for further analysis.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我创建了我的数据处理结果，现在我可以使用一个单独的脚本来处理/分析所有数据集。在认知神经科学领域，有一些常见的预处理技术，旨在提高数据的质量（例如，一些滤波步骤和拒绝过于嘈杂的通道）。我有一个名为`clean_data.py`的文件，它将遍历每个主题的“数据处理结果”文件夹，加载数据，运行清理，并将结果输出到文件夹`{subject_name}/clean`。这样，我就知道干净文件夹中的任何数据都已准备好进行进一步分析。
- en: At this point, I have cleaned data in each subject's folder, I also have that
    subject's metadata inserted into a CSV file with all subject information. From
    now on, I can just load a subject's data file, then load the metadata CSV file
    for everybody, and query only the rows that belong to the subject that I care
    about.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我在每个主题文件夹中都有干净的数据，我还将该主题的元数据插入到一个包含所有主题信息的CSV文件中。从现在开始，我只需加载一个主题的数据文件，然后加载所有人的元数据CSV文件，并仅查询属于我关心的主题的行。
- en: Defining a project
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义项目
- en: When I begin analyzing my data to answer a specific question, I create a new
    project-specific folder that exists alongside my "data" folder. Each project generally
    entails a number of analyses, and this is a way to keep them all in the same place.
    The project folder is structured similarly to the "data" folder. It has a sub-folder
    for "scripts", for "data", for "results", and for "documents" and any information
    necessary for publications that come out of this project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始分析我的数据以回答特定问题时，我会创建一个新的项目特定文件夹，该文件夹与我的“data”文件夹并列存在。每个项目通常包括多个分析，这是将它们都放在同一个地方的方法。项目文件夹的结构类似于“data”文件夹。它有一个用于“scripts”的子文件夹，一个用于“data”的子文件夹，一个用于“results”的子文件夹，一个用于“documents”的子文件夹以及用于出版这个项目的任何信息。
- en: For example, the first thing I might do is create some Python script to extract
    features of interest. I will put it in `project_name/script/feature_name/extract_feature.py`.
    The script assumes that there is data for each subject in the "clean" folder.
    It will pull the data from "clean," extract whatever feature I'm interested in,
    and then save the result to the project-specific folder, something like `project_name/data/my_feature/{subject_name}_feature.fif`.
    I parse all subject folders and save files in the same manner.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我可能首先会创建一些Python脚本来提取感兴趣的特征。我会将它放在`project_name/script/feature_name/extract_feature.py`中。该脚本假设每个主题都有数据在“clean”文件夹中。它将从“clean”中提取数据，提取我感兴趣的任何特征，然后将结果保存到项目特定的文件夹中，类似于`project_name/data/my_feature/{subject_name}_feature.fif`。我解析所有主题文件夹，并以相同的方式保存文件。
- en: Storing the extracted features for all subjects in a single project-specific
    data folder makes it much easier to develop scripts/notebooks to further analyze
    the results, since I don't need to keep track of which features have been extracted
    for which subjects. I also develop feature extraction scripts using the Sun-Grid
    engine (a platform for dividing computation between a cluster of computers) for
    speeding up my analyses. I can do this relatively easily because the folder structure
    for each subject is the same.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有主题的提取特征存储在单个项目特定数据文件夹中，使得进一步分析结果的脚本/笔记本的开发变得更加容易，因为我不需要追踪为哪些主题提取了哪些特征。我还使用Sun-Grid引擎（用于在一组计算机集群之间分配计算的平台）来加速我的分析。我可以相对容易地做到这一点，因为每个主题的文件夹结构都是相同的。
- en: Running analyses
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行分析
- en: Now that I have a set of features created for each subject, it is time to run
    analyses and answer questions. These scripts also exist in the project-specific
    folder, and assume that there is data in the "project_name/data/my_feature/" folder.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我为每个主题创建了一组特征，是时候运行分析并回答问题了。这些脚本也存在于项目特定的文件夹中，并假定“project_name/data/my_feature/”文件夹中有数据。
- en: A difficulty that I've had is knowing when to keep your analyses in interactive
    notebooks vs. Python scripts. I generally pilot my analysis interactively - this
    lets me do sanity checks and on-the-fly calculations that help me develop the
    final analysis. Once I have code that does a specific analysis, I will put it
    in a `.py` script.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的一个困难是如何判断何时将分析保留在交互式笔记本中，而不是Python脚本中。我通常会先以交互方式试验我的分析 - 这让我可以进行健全性检查和即时计算，有助于我制定最终的分析。一旦我有了执行特定分析的代码，我就会将其放入`.py`脚本中。
- en: The output of this script then goes into a `project_name/results/my_analysis`
    folder. They may be in the form of PDFs and SVGs for further inspection, or data
    files (e.g., CSV) representing model results (such as regression coefficients).
    For anything consisting of lists, numpy arrays, or simple dataframes, I use the
    `h5io` package, which provides a fast way to read/write collections of data to
    `hdf5` files (another standardized data storage format).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，此脚本的输出进入`project_name/results/my_analysis`文件夹。它们可能是PDF和SVG形式，供进一步检查，或者数据文件（例如，CSV）表示模型结果（如回归系数）。对于由列表、numpy数组或简单数据帧组成的任何内容，我使用`h5io`包，它提供了一种快速的方法来读取/写入`hdf5`文件（另一种标准化的数据存储格式）的集合。
- en: Finally, I use the outputs of the analysis script to create visualizations and
    decide whether or not my analysis actually worked. I use another set of notebooks
    to read in the results, perform last-second wrangling, statistics, etc, and output
    visualizations. If I have a publication-ready figure in mind, I will create a
    notebook specifically for that figure in another folder called `project_name/fig_{analysis_name}`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我使用分析脚本的输出来创建可视化，并决定我的分析是否真的有效。我使用另一组笔记本来读取结果，执行最后一刻的整理、统计等，并输出可视化效果。如果我有一个出版准备好的图形在脑海中，我会专门为该图形创建一个笔记本，放在另一个名为`project_name/fig_{analysis_name}`的文件夹中。
- en: When creating actual figures for papers, I like to use software like Adobe Illustrator
    to make sure that my fonts are the proper size, well-spaced, etc. I use visualization
    notebooks to create high-res PNG versions of my data that have most formatting
    stripped away (except for the data). These plots are then linked to an Illustrator
    file, so that they are updated automatically when a new plot is created. This
    way I can easily arrange my plots and standardize fonts without doing a lot of
    manual tweaking.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在为论文创建实际图形时，我喜欢使用像 Adobe Illustrator 这样的软件，以确保我的字体大小合适，间距良好等等。我使用可视化笔记本创建我的数据的高分辨率
    PNG 版本，除了数据之外，大多数格式都被剥离了。然后，这些图表链接到一个Illustrator文件，这样当创建新的图表时，它们会自动更新。这样，我可以轻松地排列我的图表并标准化字体，而无需进行大量手动调整。
- en: Finally, I use Microsoft Word to write drafts which I put in the "doc" folder.
    These pull from the figures I've created in the "fig" folder. Ideally I would
    use text files here with LaTeX, but the team that I work with makes this prohibitive.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我使用 Microsoft Word 写草稿，将其放在“doc”文件夹中。这些草稿来自我在“fig”文件夹中创建的图形。理想情况下，我会在这里使用
    LaTeX 的文本文件，但我工作的团队使这成为了不可行的。
- en: A general note
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一般说明
- en: 'This process has been refined many times over the past year, and the original
    structure looked very different than this. My original file system had code and
    data living in totally different places. Moreover, it had project-specific scripts
    and more general utility scripts living in the same place. The goal of this file
    structure is to keep data and scripts together when they have a natural pairing,
    and to separate out my more production-ready functions/modules from "hackier"
    project-specific scripts. Below is a list of some things that I''ve learned along
    the way, and that have guided the development of this system:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在过去的一年里经过了多次完善，原始结构看起来与现在非常不同。我的原始文件系统中的代码和数据位于完全不同的位置。此外，项目特定的脚本和更通用的实用脚本位于同一位置。这个文件结构的目标是在数据和脚本有自然配对关系时将它们放在一起，并从“更生产就绪”的函数/模块中分离出“更加垃圾”的项目特定脚本。以下是我在这个过程中学到的一些东西，也指导了这个系统的发展：
- en: For any data/code that are project-specific, keep these together in the same
    general file hierarchy.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于任何项目特定的数据/代码，请将其放在相同的通用文件层次结构中。
- en: Rather than creating metadata structures that live next to the data, come up
    with a “master” metadata template, then store entries from every subject in a
    CSV file that follows this template. Rather than storing data in separate subject
    files, include an entry with “subject id” in the metadata file so that you can
    pull out individual subjects in this way.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 而不是创建与数据相邻的元数据结构，设计一个“主”元数据模板，然后将每个受试者的条目存储在遵循此模板的 CSV 文件中。而不是将数据存储在单独的受试者文件中，将带有“受试者
    ID”的条目包含在元数据文件中，以便以此方式提取单个受试者。
- en: Utilize other packages whenever possible, particularly with the annoying task
    of data I/O. In my case, I store all my raw data as ‘.fif’ files, which can be
    opened easily in Python or Matlab with well-supported third party packages. I
    also use `pandas` and `h5io` to read / write metadata files, which makes it very
    easy to slice and dice these files for particular entries that I want.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽可能利用其他包，特别是在数据 I/O 的烦人任务中。在我的情况下，我将所有原始数据存储为‘.fif’文件，这些文件可以在Python或Matlab中轻松打开，并且有着良好支持的第三方包。我还使用`pandas`和`h5io`来读取/写入元数据文件，这样可以很容易地对这些文件进行切片和切块，以获取我想要的特定条目。
- en: More generally, take a “database” approach to how you store any data. Even though
    I’m not storing data in a MySQL database per-se, I can still draw inspiration
    for how this data is organized. Treat data entries as rows, and data features
    as columns, and then combine / split up the data using pandas database-style syntax
    (e.g., joins and merges). A great guideline for this is the "tidy" data specification
    described by Hadley Wickham.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更一般地说，以数据库的方式处理数据的存储方式。即使我并不是在 MySQL 数据库中存储数据，我仍然可以借鉴数据的组织方式。将数据条目视为行，数据特征视为列，然后使用
    pandas 数据库风格的语法（例如，连接和合并）组合/拆分数据。一个很好的指导原则是 Hadley Wickham 描述的“整洁”数据规范。
- en: Put ad-hoc code in a project-specific folder. Be much pickier about code that
    you expose in public Python modules for any project. If you think a function is
    worth generalizing, then move it out of the project folder and to its proper module,
    and document it extensively.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将临时代码放在项目特定的文件夹中。对于任何项目，对于公共 Python 模块中暴露的代码要更加挑剔。如果你认为一个函数值得概括，那么将其移出项目文件夹并移到其正确的模块中，并对其进行详细记录。
- en: Do all coding with automatic PEP8 and PEP257 checkers. PEP8 is a set of standards
    for naming conventions, code syntax, using white space, etc to ensure that code
    is clean and readable. PEP257 is a set of similar guidelines for docstrings. Many
    "fully-featured" text editors have plugins that automatically check code using
    these guidelines and highlight errors, which is useful for quickly writing clean
    code.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有编码都要使用自动 PEP8 和 PEP257 检查器。PEP8 是一组用于命名约定、代码语法、使用空格等的标准，以确保代码整洁可读。PEP257 是一组类似的文档字符串指南。许多“功能齐全”的文本编辑器都有插件，可以根据这些指南自动检查代码并突出显示错误，这对快速编写整洁代码很有用。
- en: Make a conscious effort to structure Python scripts differently from Jupyter
    notebooks. Structure code (and data) such that it lends itself well to scripting,
    rather than assuming interactive sessions for everything.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请有意识地将 Python 脚本的结构与 Jupyter 笔记本区分开来。结构化代码（和数据），使其更适合脚本编写，而不是假设一切都是交互式会话。
- en: Use Jupyter notebooks to glance at the data and preliminary results, but move
    code into scripts as it becomes more refined or complex. This avoids creating
    a mega notebook with tons of different analyses in it.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Jupyter 笔记本查看数据和初步结果，但随着代码变得更加精细或复杂，将其移入脚本中。这样可以避免创建一个包含大量不同分析的大型笔记本。
- en: Structure code so that some scripts live with the data that they operate on.
    E.g., if you’ve got a script that only operates on a specific collection of data,
    renames specific columns in that data, and always saves it to another location
    relative to the original data, then create another folder “script” right next
    to that data folder. Put all data-specific scripts into this folder. This way,
    you know that scripts operate with relevant data nearby.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结构化代码，使一些脚本与它们操作的数据一起。例如，如果你有一个只操作特定数据集的脚本，重命名该数据集中的特定列，并始终将其保存到原始数据的另一个位置，那么在数据文件夹旁边创建另一个名为“script”的文件夹。将所有特定数据的脚本放入此文件夹。这样，你就知道脚本与相关数据一起操作。
- en: Finally, this is not specific to this project but has been very useful to me.
    Find opportunities to contribute to other open-source projects. Open pull requests
    and learn about how to use the code. The back-and-forths and input you get will
    make you a much better coder, and your codebase / research will greatly benefit
    from it.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这不是特定于这个项目，但对我非常有用。寻找机会为其他开源项目做贡献。提交拉取请求并学习如何使用代码。你得到的来回反馈和输入将使你成为一个更好的编码人员，你的代码库/研究将极大受益。
- en: Pain points
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 痛点
- en: The hardest part of my workflow has been deciding how to balance flexibility
    and control. On the one hand, you don't want your scripts to be so specific to
    data that they break as soon as anything changes, but on the other hand creating
    code that generalizes well and isn't terribly confusing is really difficult to
    do. In my case, I initially made errors on both of these fronts, but the current
    structure of my data seems to be more intuitive, easy to maintain, and easy to
    grow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我的工作流程中最困难的部分是如何平衡灵活性和控制。一方面，你不希望你的脚本对数据过于特定，以至于一旦有任何变化就会出错，但另一方面，编写能够很好泛化且不会令人困惑的代码确实非常困难。在我的情况下，我最初在这两个方面都犯了错误，但目前我的数据结构似乎更直观，易于维护和扩展。
- en: 'Another big issue I''ve had lies in dealing with different formats of data
    and information. For example, I want to version control all of the code that I
    write, but:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的另一个大问题在于处理不同格式的数据和信息。例如，我想对我编写的所有代码进行版本控制，但：
- en: Does that mean that I should create one big repository for all of the code described
    above? What about a separate repository for each project?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是否意味着我应该为上述所有代码创建一个大型存储库？每个项目是否应该有一个单独的存储库？
- en: How should I split up the general modules and functions vs. the code that only
    lives with a particular project?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何区分一般模块和函数与仅与特定项目一起存在的代码？
- en: Finally, how should I deal with the fact that there are lots of other "non-code"
    files living with this file structure (e.g., images)? Should they be version-controlled,
    or should the code just assume that the data lives in particular folders? What
    would happen if somebody else copied the code and didn't get the data?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我应该如何处理与这种文件结构一起存在的许多其他“非代码”文件（例如，图像）？它们应该进行版本控制，还是代码应该假设数据位于特定文件夹中？如果其他人复制了代码但没有获取数据，会发生什么？
- en: I don't necessarily have good answers for these issues, and I'm still coming
    up with a solution that makes me happy, but these are some things that I'm thinking
    about.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我并没有对这些问题有很好的答案，我仍在寻找一个让我满意的解决方案，但这些是我正在思考的一些事情。
- en: Key benefits
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主要好处
- en: The biggest benefit of this system is the fact that messy, subject-specific
    data is quickly turned into a standardized format that is consistent across all
    of my subjects. This is useful because it means you can write scripts that analyze
    the entire dataset without doing a lot of extra customization. Moreover, because
    the structure of the filesystem is the same for everybody (including things like
    naming conventions), it is easy to find what you want from each dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统最大的好处在于，混乱的、特定主题的数据很快被转换成了一个标准化的格式，这个格式在我所有的学科中都是一致的。这很有用，因为这意味着你可以编写脚本来分析整个数据集，而不需要做很多额外的定制。此外，由于文件系统的结构对每个人都是相同的（包括命名约定等），所以很容易从每个数据集中找到你想要的东西。
- en: Another benefit is the fact that I am storing code along with the data that
    it operates on. Some people feel strongly that this is a bad idea, but I've found
    it to be useful so long as the within-project folder structure still makes sense.
    In previous workflows, I had all of my code in one folder, and all of my data
    in another folder. This often led to confusions where I was unsure which code
    operated on what data. It also made it more difficult to connect the steps of
    preprocessing and feature extraction chains. Now, if I want to know all of the
    things that have been done to a collection of data files, I just need to look
    into its corresponding "script" folder.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好处是，我将代码与其操作的数据一起存储。有些人坚决认为这是一个坏主意，但我发现只要项目内的文件夹结构仍然合理，这是有用的。在以前的工作流程中，我把所有的代码放在一个文件夹里，把所有的数据放在另一个文件夹里。这经常导致混乱，我不确定哪些代码操作哪些数据。这也使得连接预处理和特征提取链的步骤更加困难。现在，如果我想知道对一组数据文件做了什么事情，我只需要查看其相应的“脚本”文件夹。
- en: Finally, by separating out operations that are true for all projects (e.g.,
    data munging and cleaning) and those that are project-specific, the scope of individual
    projects becomes more clear and easy to follow. I think of the data pipeline as
    a single tree trunk, where projects branch out from this trunk and do extra things
    to the data, on top of the base workflow of preprocessing. Now, my file structure
    more naturally follows this concept.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过将适用于所有项目的操作（例如数据整理和清理）与项目特定的操作分开，个别项目的范围变得更加清晰和易于跟踪。我把数据管道想象成一棵单一的树干，项目从这个树干上分支出来，在基础预处理工作流之上对数据进行额外处理。现在，我的文件结构更自然地遵循这个概念。
- en: Key tools
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键工具
- en: The two most useful tools that I have found are `Pandas` and `MNE-python`. Pandas
    made it much easier to embed metadata with the signals that I analyze. It allowed
    me to store information from lots of subjects in a single CSV file, and treat
    it as a "database" by using queries on it. `MNE-python` is a package for electrophysiology
    in neuroscience written in Python. When I discovered it, I found that it duplicated
    many of the functions I had already written, and in general did this much better
    than I had. Moreover, it has a lot of convenience functions for doing I/O, which
    up until then was a pain to maintain. By using these two packages, I was able
    to significantly cut down on the amount of custom-written functions that I used
    to wrangle my data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现的两个最有用的工具是`Pandas`和`MNE-python`。Pandas使得将元数据嵌入到我分析的信号中变得更加容易。它允许我将来自许多受试者的信息存储在一个单独的CSV文件中，并通过对其进行查询来将其视为一个“数据库”。`MNE-python`是一个用Python编写的神经科学中的电生理学包。当我发现它时，我发现它复制了我已经编写的许多函数，并且总体上做得比我好得多。此外，它有许多方便的函数用于进行I/O操作，这在那时是一个难以维护的痛点。通过使用这两个包，我能够大大减少我用来处理数据的自定义函数的数量。
- en: Questions
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: What does "reproducibility" mean to you?
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对你来说，“可重现性”是什么意思？
- en: The discussion in this writeup covers the first 6 months of the project. To
    that extent, my definition of "reproducibility" means actually being able to reproduce
    my own results (aka, coding for my future self). I ran into a lot of issues to
    keep things streamlined and understandable in my own head, which made it difficult
    to interpret my findings. Obviously this would generalize to other scientists
    trying to reproduce my analyses and work as well.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的讨论涵盖了项目的前6个月。在这个范围内，我的“可复现性”的定义是实际上能够重现我的结果（也就是为了我的未来编写代码）。我遇到了许多问题，使得难以在自己的脑海中保持事情的简洁和可理解性，这使得解释我的发现变得困难。显然，这也会泛化到其他试图重现我的分析和工作的科学家。
- en: Why do you think that reproducibility in your domain is important?
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您认为在您的领域中可复现性的重要性在于什么？
- en: Because it's a guiding principle that will make my code more understandable,
    maintainable, and extendable for others and for myself.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它是一个指导原则，将使我的代码更易于理解，更易于维护，也更易于为他人和自己扩展。
- en: How or where did you learn about reproducibility?
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您是如何或从何处了解到可复现性的？
- en: At first it came from teaching a few Software Carpentry classes and reading
    things online. Lately, I have gotten a lot of help by contributing to the `MNE-python`
    project, as I've found that going through the pull request process for a well-maintained
    project is a great way to learn a lot about coding well.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，这来自于教授几个软件工程课程和在线阅读的一些内容。最近，我通过为`MNE-python`项目做出贡献获得了很多帮助，因为我发现为一个维护良好的项目进行拉取请求过程是学习良好编码的一种很好的方法。
- en: What do you see as the major challenges to doing reproducible research in your
    domain, and do you have any suggestions?
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在您领域进行可复现研究的主要挑战是什么，您有什么建议吗？
- en: In my case, legality is a big problem because I'm dealing with medical data
    that cannot be shared publicly. Another big problem is simply a matter of training
    and incentive. Right now there is little opportunity to learn how to code well
    or how to make reproducible science. To make matters worse, I see very little
    incentive for anyone to actually do so (if they want to be a tenured faculty).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，法律性是一个大问题，因为我处理的医疗数据不能公开共享。另一个大问题仅仅是培训和激励问题。现在几乎没有机会学习如何编写良好的代码或如何进行可复现的科学研究。更糟糕的是，我几乎看不到任何人真正去做这样的事情的动机（如果他们想成为终身教师）。
- en: What do you view as the major incentives for doing reproducible research?
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您认为进行可复现研究的主要动机是什么？
- en: Other than the warm fuzzy feeling, I think the biggest advantage is that when
    you code and organize for other people, you also code and organize for yourself
    in the future. This makes your life much easier in the long run.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了温暖的愉悦感之外，我认为最大的优势在于当你为他人编写和组织代码时，也在为将来的自己编写和组织代码。这会使你的生活在长远来看变得更加轻松。
- en: Are there any best practices that you'd recommend for researchers in your field?
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你会向同行在你领域的研究者推荐哪些最佳实践？
- en: Front-load a lot of thinking/planning before you just start creating scripts
    and functions. Spend a good chunk of time thinking "big picture" early on, then
    zoom in and build some stuff, then zoom back out and decide if it was a good idea
    or not. Don't get lost in the weeds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始创建脚本和函数之前，预先进行大量的思考/规划。在早期花费大量时间思考“大局”，然后放大并构建一些东西，然后放大并决定是否是一个好主意。不要迷失在细节中。
- en: Would you recommend any specific resources for learning more about reproducibility?
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您会推荐哪些特定资源来了解更多关于可复现性的信息？
- en: Software Carpentry is a great one, but most other stuff is just scattered on
    stack overflow unfortunately. I think that finding a good package that has a sweet-spot
    of contributors (aka, not so few that you don't get feedback, not so many that
    it's a huge pain to do anything). Try to contribute something via a pull request
    and learn from the other people in the community. It will be a great way to learn
    good coding principles. Finally, find a community around you (at the university,
    at local companies/hackathons, etc) that shares your interests. Spend time learning
    and teaching with these people.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程是一个很好的例子，但大多数其他内容都散布在堆栈溢出上。不幸的是，我认为找到一个具有贡献者甜蜜点（即，贡献者不太多以至于您得不到反馈，也不太多以至于做任何事情都是巨大痛苦的）的好软件包是很困难的。试着通过拉取请求贡献一些东西，并从社区中的其他人那里学习。这将是学习良好编码原则的好方法。最后，在您周围找到一个与您分享兴趣的社区（在大学，本地公司/编程马拉松等）。花时间与这些人学习和教学。
