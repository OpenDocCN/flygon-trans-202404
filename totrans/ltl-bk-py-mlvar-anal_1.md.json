["```\nfrom __future__ import print_function, division  # for compatibility with python 3.x\nimport warnings\nwarnings.filterwarnings('ignore')  # don't print out warnings\n\n%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark.py\n%load_ext watermark\n%watermark -v -m -p python,pandas,numpy,matplotlib,seaborn,scikit-learn,scipy -g\n\n```", "```\nInstalled watermark.py. To use it, type:\n  %load_ext watermark\nCPython 2.7.11\nIPython 4.0.3\n\npython 2.7.11\npandas 0.17.1\nnumpy 1.10.4\nmatplotlib 1.5.1\nseaborn 0.7.0\nscikit-learn 0.17\nscipy 0.17.0\n\ncompiler   : GCC 4.2.1 (Apple Inc. build 5577)\nsystem     : Darwin\nrelease    : 13.4.0\nmachine    : x86_64\nprocessor  : i386\nCPU cores  : 4\ninterpreter: 64bit\nGit hash   : b584574b9a5080bac2e592d4432f9c17c1845c18\n\n```", "```\nfrom pydoc import help  # can type in the python console `help(name of function)` to get the documentation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom scipy import stats\nfrom IPython.display import display, HTML\n\n# figures inline in notebook\n%matplotlib inline\n\nnp.set_printoptions(suppress=True)\n\nDISPLAY_MAX_ROWS = 20  # number of max rows to print for a DataFrame\npd.set_option('display.max_rows', DISPLAY_MAX_ROWS)\n\n```", "```\n# %qtconsole\n\n```", "```\n1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065\n1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050\n1,13.16,2.36,2.67,18.6,101,2.8,3.24,.3,2.81,5.68,1.03,3.17,1185\n1,14.37,1.95,2.5,16.8,113,3.85,3.49,.24,2.18,7.8,.86,3.45,1480\n1,13.24,2.59,2.87,21,118,2.8,2.69,.39,1.82,4.32,1.04,2.93,735\n...\n\n```", "```\ndata = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", header=None)\ndata.columns = [\"V\"+str(i) for i in range(1, len(data.columns)+1)]  # rename column names to be similar to R naming convention\ndata.V1 = data.V1.astype(str)\nX = data.loc[:, \"V2\":]  # independent variables data\ny = data.V1  # dependednt variable data\ndata\n\n```", "```\ndata.loc[:, \"V2\":\"V6\"]\n\n```", "```\npd.tools.plotting.scatter_matrix(data.loc[:, \"V2\":\"V6\"], diagonal=\"kde\")\nplt.tight_layout()\nplt.show()\n\n```", "```\nsns.lmplot(\"V4\", \"V5\", data, hue=\"V1\", fit_reg=False);\n\n```", "```\nax = data[[\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]].plot()\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5));\n\n```", "```\nX.apply(np.mean)\n\n```", "```\nV2      13.000618\nV3       2.336348\nV4       2.366517\nV5      19.494944\nV6      99.741573\nV7       2.295112\nV8       2.029270\nV9       0.361854\nV10      1.590899\nV11      5.058090\nV12      0.957449\nV13      2.611685\nV14    746.893258\ndtype: float64\n\n```", "```\nX.apply(np.std)\n\n```", "```\nV2       0.809543\nV3       1.114004\nV4       0.273572\nV5       3.330170\nV6      14.242308\nV7       0.624091\nV8       0.996049\nV9       0.124103\nV10      0.570749\nV11      2.311765\nV12      0.227929\nV13      0.707993\nV14    314.021657\ndtype: float64\n\n```", "```\nclass2data = data[y==\"2\"]\n\n```", "```\nclass2data.loc[:, \"V2\":].apply(np.mean)\n\n```", "```\nV2      12.278732\nV3       1.932676\nV4       2.244789\nV5      20.238028\nV6      94.549296\nV7       2.258873\nV8       2.080845\nV9       0.363662\nV10      1.630282\nV11      3.086620\nV12      1.056282\nV13      2.785352\nV14    519.507042\ndtype: float64\n\n```", "```\nclass2data.loc[:, \"V2\":].apply(np.std)\n\n```", "```\nV2       0.534162\nV3       1.008391\nV4       0.313238\nV5       3.326097\nV6      16.635097\nV7       0.541507\nV8       0.700713\nV9       0.123085\nV10      0.597813\nV11      0.918393\nV12      0.201503\nV13      0.493064\nV14    156.100173\ndtype: float64\n\n```", "```\ndef printMeanAndSdByGroup(variables, groupvariable):\n    data_groupby = variables.groupby(groupvariable)\n    print(\"## Means:\")\n    display(data_groupby.apply(np.mean))\n    print(\"\\n## Standard deviations:\")\n    display(data_groupby.apply(np.std))\n    print(\"\\n## Sample sizes:\")\n    display(pd.DataFrame(data_groupby.apply(len)))\n\n```", "```\nprintMeanAndSdByGroup(X, y)\n\n```", "```\n## Means:\n\n```", "```\n## Standard deviations:\n\n```", "```\n## Sample sizes:\n\n```", "```\ndef calcWithinGroupsVariance(variable, groupvariable):\n    # find out how many values the group variable can take\n    levels = sorted(set(groupvariable))\n    numlevels = len(levels)\n    # get the mean and standard deviation for each group:\n    numtotal = 0\n    denomtotal = 0\n    for leveli in levels:\n        levelidata = variable[groupvariable==leveli]\n        levelilength = len(levelidata)\n        # get the standard deviation for group i:\n        sdi = np.std(levelidata)\n        numi = (levelilength)*sdi**2\n        denomi = levelilength\n        numtotal = numtotal + numi\n        denomtotal = denomtotal + denomi\n    # calculate the within-groups variance\n    Vw = numtotal / (denomtotal - numlevels)\n    return Vw\n\n```", "```\ncalcWithinGroupsVariance(X.V2, y)\n\n```", "```\n0.2620524691539065\n\n```", "```\ndef calcBetweenGroupsVariance(variable, groupvariable):\n    # find out how many values the group variable can take\n    levels = sorted(set((groupvariable)))\n    numlevels = len(levels)\n    # calculate the overall grand mean:\n    grandmean = np.mean(variable)\n    # get the mean and standard deviation for each group:\n    numtotal = 0\n    denomtotal = 0\n    for leveli in levels:\n        levelidata = variable[groupvariable==leveli]\n        levelilength = len(levelidata)\n        # get the mean and standard deviation for group i:\n        meani = np.mean(levelidata)\n        sdi = np.std(levelidata)\n        numi = levelilength * ((meani - grandmean)**2)\n        denomi = levelilength\n        numtotal = numtotal + numi\n        denomtotal = denomtotal + denomi\n    # calculate the between-groups variance\n    Vb = numtotal / (numlevels - 1)\n    return(Vb)\n\n```", "```\ncalcBetweenGroupsVariance(X.V2, y)\n\n```", "```\n35.397424960269106\n\n```", "```\n# 35.397424960269106 / 0.2620524691539065\ncalcBetweenGroupsVariance(X.V2, y) / calcWithinGroupsVariance(X.V2, y)\n\n```", "```\n135.07762424279917\n\n```", "```\ndef calcSeparations(variables, groupvariable):\n    # calculate the separation for each variable\n    for variablename in variables:\n        variablei = variables[variablename]\n        Vw = calcWithinGroupsVariance(variablei, groupvariable)\n        Vb = calcBetweenGroupsVariance(variablei, groupvariable)\n        sep = Vb/Vw\n        print(\"variable\", variablename, \"Vw=\", Vw, \"Vb=\", Vb, \"separation=\", sep)\n\n```", "```\ncalcSeparations(X, y)\n\n```", "```\nvariable V2 Vw= 0.262052469154 Vb= 35.3974249603 separation= 135.077624243\nvariable V3 Vw= 0.887546796747 Vb= 32.7890184869 separation= 36.9434249632\nvariable V4 Vw= 0.0660721013425 Vb= 0.879611357249 separation= 13.3129012\nvariable V5 Vw= 8.00681118121 Vb= 286.416746363 separation= 35.7716374073\nvariable V6 Vw= 180.657773164 Vb= 2245.50102789 separation= 12.4295843381\nvariable V7 Vw= 0.191270475224 Vb= 17.9283572943 separation= 93.7330096204\nvariable V8 Vw= 0.274707514337 Vb= 64.2611950236 separation= 233.925872682\nvariable V9 Vw= 0.0119117022133 Vb= 0.328470157462 separation= 27.575417147\nvariable V10 Vw= 0.246172943796 Vb= 7.45199550778 separation= 30.2713831702\nvariable V11 Vw= 2.28492308133 Vb= 275.708000822 separation= 120.664018441\nvariable V12 Vw= 0.0244876469432 Vb= 2.48100991494 separation= 101.31679539\nvariable V13 Vw= 0.160778729561 Vb= 30.5435083544 separation= 189.972320579\nvariable V14 Vw= 29707.6818705 Vb= 6176832.32228 separation= 207.920373902\n\n```", "```\ndef calcWithinGroupsCovariance(variable1, variable2, groupvariable):\n    levels = sorted(set(groupvariable))\n    numlevels = len(levels)\n    Covw = 0.0\n    # get the covariance of variable 1 and variable 2 for each group:\n    for leveli in levels:\n        levelidata1 = variable1[groupvariable==leveli]\n        levelidata2 = variable2[groupvariable==leveli]\n        mean1 = np.mean(levelidata1)\n        mean2 = np.mean(levelidata2)\n        levelilength = len(levelidata1)\n        # get the covariance for this group:\n        term1 = 0.0\n        for levelidata1j, levelidata2j in zip(levelidata1, levelidata2):\n            term1 += (levelidata1j - mean1)*(levelidata2j - mean2)\n        Cov_groupi = term1 # covariance for this group\n        Covw += Cov_groupi\n    totallength = len(variable1)\n    Covw /= totallength - numlevels\n    return Covw\n\n```", "```\ncalcWithinGroupsCovariance(X.V8, X.V11, y)\n\n```", "```\n0.28667830215140183\n\n```", "```\ndef calcBetweenGroupsCovariance(variable1, variable2, groupvariable):\n    # find out how many values the group variable can take\n    levels = sorted(set(groupvariable))\n    numlevels = len(levels)\n    # calculate the grand means\n    variable1mean = np.mean(variable1)\n    variable2mean = np.mean(variable2)\n    # calculate the between-groups covariance\n    Covb = 0.0\n    for leveli in levels:\n        levelidata1 = variable1[groupvariable==leveli]\n        levelidata2 = variable2[groupvariable==leveli]\n        mean1 = np.mean(levelidata1)\n        mean2 = np.mean(levelidata2)\n        levelilength = len(levelidata1)\n        term1 = (mean1 - variable1mean) * (mean2 - variable2mean) * levelilength\n        Covb += term1\n    Covb /= numlevels - 1\n    return Covb\n\n```", "```\ncalcBetweenGroupsCovariance(X.V8, X.V11, y)\n\n```", "```\n-60.4107748359163\n\n```", "```\ncorr = stats.pearsonr(X.V2, X.V3)\nprint(\"p-value:\\t\", corr[1])\nprint(\"cor:\\t\\t\", corr[0])\n\n```", "```\np-value:     0.210081985971\ncor:         0.0943969409104\n\n```", "```\ncorrmat = X.corr()\ncorrmat\n\n```", "```\nsns.heatmap(corrmat, vmax=1., square=False).xaxis.tick_top()\n\n```", "```\n# adapted from http://matplotlib.org/examples/specialty_plots/hinton_demo.html\ndef hinton(matrix, max_weight=None, ax=None):\n    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n    ax = ax if ax is not None else plt.gca()\n\n    if not max_weight:\n        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n\n    ax.patch.set_facecolor('lightgray')\n    ax.set_aspect('equal', 'box')\n    ax.xaxis.set_major_locator(plt.NullLocator())\n    ax.yaxis.set_major_locator(plt.NullLocator())\n\n    for (x, y), w in np.ndenumerate(matrix):\n        color = 'red' if w > 0 else 'blue'\n        size = np.sqrt(np.abs(w))\n        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n                             facecolor=color, edgecolor=color)\n        ax.add_patch(rect)\n\n    nticks = matrix.shape[0]\n    ax.xaxis.tick_top()\n    ax.set_xticks(range(nticks))\n    ax.set_xticklabels(list(matrix.columns), rotation=90)\n    ax.set_yticks(range(nticks))\n    ax.set_yticklabels(matrix.columns)\n    ax.grid(False)\n\n    ax.autoscale_view()\n    ax.invert_yaxis()\n\nhinton(corrmat)\n\n```", "```\ndef mosthighlycorrelated(mydataframe, numtoreport):\n    # find the correlations\n    cormatrix = mydataframe.corr()\n    # set the correlations on the diagonal or lower triangle to zero,\n    # so they will not be reported as the highest ones:\n    cormatrix *= np.tri(*cormatrix.values.shape, k=-1).T\n    # find the top n correlations\n    cormatrix = cormatrix.stack()\n    cormatrix = cormatrix.reindex(cormatrix.abs().sort_values(ascending=False).index).reset_index()\n    # assign human-friendly names\n    cormatrix.columns = [\"FirstVariable\", \"SecondVariable\", \"Correlation\"]\n    return cormatrix.head(numtoreport)\n\n```", "```\nmosthighlycorrelated(X, 10)\n\n```", "```\nstandardisedX = scale(X)\nstandardisedX = pd.DataFrame(standardisedX, index=X.index, columns=X.columns)\n\n```", "```\nstandardisedX.apply(np.mean)\n\n```", "```\nV2    -8.619821e-16\nV3    -8.357859e-17\nV4    -8.657245e-16\nV5    -1.160121e-16\nV6    -1.995907e-17\nV7    -2.972030e-16\nV8    -4.016762e-16\nV9     4.079134e-16\nV10   -1.699639e-16\nV11   -1.247442e-18\nV12    3.717376e-16\nV13    2.919013e-16\nV14   -7.484650e-18\ndtype: float64\n\n```", "```\nstandardisedX.apply(np.std)\n\n```", "```\nV2     1\nV3     1\nV4     1\nV5     1\nV6     1\nV7     1\nV8     1\nV9     1\nV10    1\nV11    1\nV12    1\nV13    1\nV14    1\ndtype: float64\n\n```", "```\npca = PCA().fit(standardisedX)\n\n```", "```\ndef pca_summary(pca, standardised_data, out=True):\n    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    a = list(np.std(pca.transform(standardised_data), axis=0))\n    b = list(pca.explained_variance_ratio_)\n    c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n    summary = pd.DataFrame(zip(a, b, c), index=names, columns=columns)\n    if out:\n        print(\"Importance of components:\")\n        display(summary)\n    return summary\n\n```", "```\nsummary = pca_summary(pca, standardisedX)\n\n```", "```\nImportance of components:\n\n```", "```\nsummary.sdev\n\n```", "```\nnp.sum(summary.sdev**2)\n\n```", "```\nStandard deviation    13\ndtype: float64\n\n```", "```\ndef screeplot(pca, standardised_values):\n    y = np.std(pca.transform(standardised_values), axis=0)**2\n    x = np.arange(len(y)) + 1\n    plt.plot(x, y, \"o-\")\n    plt.xticks(x, [\"Comp.\"+str(i) for i in x], rotation=60)\n    plt.ylabel(\"Variance\")\n    plt.show()\n\nscreeplot(pca, standardisedX)\n\n```", "```\nsummary.sdev**2\n\n```", "```\npca.components_[0]\n\n```", "```\narray([-0.1443294 ,  0.24518758,  0.00205106,  0.23932041, -0.14199204,\n       -0.39466085, -0.4229343 ,  0.2985331 , -0.31342949,  0.0886167 ,\n       -0.29671456, -0.37616741, -0.28675223])\n\n```", "```\n-0.144*Z2 + 0.245*Z3 + 0.002*Z4 + 0.239*Z5 - 0.142*Z6 - 0.395*Z7 - 0.423*Z8 + 0.299*Z9 -0.313*Z10 + 0.089*Z11 - 0.297*Z12 - 0.376*Z13 - 0.287*Z14\n\n```", "```\nnp.sum(pca.components_[0]**2)\n\n```", "```\n1.0000000000000004\n\n```", "```\ndef calcpc(variables, loadings):\n    # find the number of samples in the data set and the number of variables\n    numsamples, numvariables = variables.shape\n    # make a vector to store the component\n    pc = np.zeros(numsamples)\n    # calculate the value of the component for each sample\n    for i in range(numsamples):\n        valuei = 0\n        for j in range(numvariables):\n            valueij = variables.iloc[i, j]\n            loadingj = loadings[j]\n            valuei = valuei + (valueij * loadingj)\n        pc[i] = valuei\n    return pc\n\n```", "```\ncalcpc(standardisedX, pca.components_[0])\n\n```", "```\narray([-3.31675081, -2.20946492, -2.51674015, -3.75706561, -1.00890849,\n       -3.05025392, -2.44908967, -2.05943687, -2.5108743 , -2.75362819,\n       -3.47973668, -1.7547529 , -2.11346234, -3.45815682, -4.31278391,\n       -2.3051882 , -2.17195527, -1.89897118, -3.54198508, -2.0845222 ,\n       -3.12440254, -1.08657007, -2.53522408, -1.64498834, -1.76157587,\n       -0.9900791 , -1.77527763, -1.23542396, -2.18840633, -2.25610898,\n       -2.50022003, -2.67741105, -1.62857912, -1.90269086, -1.41038853,\n       -1.90382623, -1.38486223, -1.12220741, -1.5021945 , -2.52980109,\n       -2.58809543, -0.66848199, -3.07080699, -0.46220914, -2.10135193,\n       -1.13616618, -2.72660096, -2.82133927, -2.00985085, -2.7074913 ,\n       -3.21491747, -2.85895983, -3.50560436, -2.22479138, -2.14698782,\n       -2.46932948, -2.74151791, -2.17374092, -3.13938015,  0.92858197,\n        1.54248014,  1.83624976, -0.03060683, -2.05026161,  0.60968083,\n       -0.90022784, -2.24850719, -0.18338403,  0.81280503, -1.9756205 ,\n        1.57221622, -1.65768181,  0.72537239, -2.56222717, -1.83256757,\n        0.8679929 , -0.3700144 ,  1.45737704, -1.26293085, -0.37615037,\n       -0.7620639 , -1.03457797,  0.49487676,  2.53897708, -0.83532015,\n       -0.78790461,  0.80683216,  0.55804262,  1.11511104,  0.55572283,\n        1.34928528,  1.56448261,  1.93255561, -0.74666594, -0.95745536,\n       -2.54386518,  0.54395259, -1.03104975, -2.25190942, -1.41021602,\n       -0.79771979,  0.54953173,  0.16117374,  0.65979494, -0.39235441,\n        1.77249908,  0.36626736,  1.62067257, -0.08253578, -1.57827507,\n       -1.42056925,  0.27870275,  1.30314497,  0.45707187,  0.49418585,\n       -0.48207441,  0.25288888,  0.10722764,  2.4330126 ,  0.55108954,\n       -0.73962193, -1.33632173,  1.177087  ,  0.46233501, -0.97847408,\n        0.09680973, -0.03848715,  1.5971585 ,  0.47956492,  1.79283347,\n        1.32710166,  2.38450083,  2.9369401 ,  2.14681113,  2.36986949,\n        3.06384157,  3.91575378,  3.93646339,  3.09427612,  2.37447163,\n        2.77881295,  2.28656128,  2.98563349,  2.3751947 ,  2.20986553,\n        2.625621  ,  4.28063878,  3.58264137,  2.80706372,  2.89965933,\n        2.32073698,  2.54983095,  1.81254128,  2.76014464,  2.7371505 ,\n        3.60486887,  2.889826  ,  3.39215608,  1.0481819 ,  1.60991228,\n        3.14313097,  2.2401569 ,  2.84767378,  2.59749706,  2.94929937,\n        3.53003227,  2.40611054,  2.92908473,  2.18141278,  2.38092779,\n        3.21161722,  3.67791872,  2.4655558 ,  3.37052415,  2.60195585,\n        2.67783946,  2.38701709,  3.20875816])\n\n```", "```\npca.transform(standardisedX)[:, 0]\n\n```", "```\narray([-3.31675081, -2.20946492, -2.51674015, -3.75706561, -1.00890849,\n       -3.05025392, -2.44908967, -2.05943687, -2.5108743 , -2.75362819,\n       -3.47973668, -1.7547529 , -2.11346234, -3.45815682, -4.31278391,\n       -2.3051882 , -2.17195527, -1.89897118, -3.54198508, -2.0845222 ,\n       -3.12440254, -1.08657007, -2.53522408, -1.64498834, -1.76157587,\n       -0.9900791 , -1.77527763, -1.23542396, -2.18840633, -2.25610898,\n       -2.50022003, -2.67741105, -1.62857912, -1.90269086, -1.41038853,\n       -1.90382623, -1.38486223, -1.12220741, -1.5021945 , -2.52980109,\n       -2.58809543, -0.66848199, -3.07080699, -0.46220914, -2.10135193,\n       -1.13616618, -2.72660096, -2.82133927, -2.00985085, -2.7074913 ,\n       -3.21491747, -2.85895983, -3.50560436, -2.22479138, -2.14698782,\n       -2.46932948, -2.74151791, -2.17374092, -3.13938015,  0.92858197,\n        1.54248014,  1.83624976, -0.03060683, -2.05026161,  0.60968083,\n       -0.90022784, -2.24850719, -0.18338403,  0.81280503, -1.9756205 ,\n        1.57221622, -1.65768181,  0.72537239, -2.56222717, -1.83256757,\n        0.8679929 , -0.3700144 ,  1.45737704, -1.26293085, -0.37615037,\n       -0.7620639 , -1.03457797,  0.49487676,  2.53897708, -0.83532015,\n       -0.78790461,  0.80683216,  0.55804262,  1.11511104,  0.55572283,\n        1.34928528,  1.56448261,  1.93255561, -0.74666594, -0.95745536,\n       -2.54386518,  0.54395259, -1.03104975, -2.25190942, -1.41021602,\n       -0.79771979,  0.54953173,  0.16117374,  0.65979494, -0.39235441,\n        1.77249908,  0.36626736,  1.62067257, -0.08253578, -1.57827507,\n       -1.42056925,  0.27870275,  1.30314497,  0.45707187,  0.49418585,\n       -0.48207441,  0.25288888,  0.10722764,  2.4330126 ,  0.55108954,\n       -0.73962193, -1.33632173,  1.177087  ,  0.46233501, -0.97847408,\n        0.09680973, -0.03848715,  1.5971585 ,  0.47956492,  1.79283347,\n        1.32710166,  2.38450083,  2.9369401 ,  2.14681113,  2.36986949,\n        3.06384157,  3.91575378,  3.93646339,  3.09427612,  2.37447163,\n        2.77881295,  2.28656128,  2.98563349,  2.3751947 ,  2.20986553,\n        2.625621  ,  4.28063878,  3.58264137,  2.80706372,  2.89965933,\n        2.32073698,  2.54983095,  1.81254128,  2.76014464,  2.7371505 ,\n        3.60486887,  2.889826  ,  3.39215608,  1.0481819 ,  1.60991228,\n        3.14313097,  2.2401569 ,  2.84767378,  2.59749706,  2.94929937,\n        3.53003227,  2.40611054,  2.92908473,  2.18141278,  2.38092779,\n        3.21161722,  3.67791872,  2.4655558 ,  3.37052415,  2.60195585,\n        2.67783946,  2.38701709,  3.20875816])\n\n```", "```\npca.components_[1]\n\n```", "```\narray([ 0.48365155,  0.22493093,  0.31606881, -0.0105905 ,  0.299634  ,\n        0.06503951, -0.00335981,  0.02877949,  0.03930172,  0.52999567,\n       -0.27923515, -0.16449619,  0.36490283])\n\n```", "```\n0.484*Z2 + 0.225*Z3 + 0.316*Z4 - 0.011*Z5 + 0.300*Z6 + 0.065*Z7 - 0.003*Z8 + 0.029*Z9 + 0.039*Z10 + 0.530*Z11 - 0.279*Z12 - 0.164*Z13 + 0.365*Z14\n\n```", "```\nnp.sum(pca.components_[1]**2)\n\n```", "```\n1.0000000000000011\n\n```", "```\ndef pca_scatter(pca, standardised_values, classifs):\n    foo = pca.transform(standardised_values)\n    bar = pd.DataFrame(zip(foo[:, 0], foo[:, 1], classifs), columns=[\"PC1\", \"PC2\", \"Class\"])\n    sns.lmplot(\"PC1\", \"PC2\", bar, hue=\"Class\", fit_reg=False)\n\npca_scatter(pca, standardisedX, y)\n\n```", "```\nprintMeanAndSdByGroup(standardisedX, y);\n\n```", "```\n## Means:\n\n```", "```\n## Standard deviations:\n\n```", "```\n## Sample sizes:\n\n```", "```\nlda = LinearDiscriminantAnalysis().fit(X, y)\n\n```", "```\ndef pretty_scalings(lda, X, out=False):\n    ret = pd.DataFrame(lda.scalings_, index=X.columns, columns=[\"LD\"+str(i+1) for i in range(lda.scalings_.shape[1])])\n    if out:\n        print(\"Coefficients of linear discriminants:\")\n        display(ret)\n    return ret\n\npretty_scalings_ = pretty_scalings(lda, X, out=True)\n\n```", "```\nCoefficients of linear discriminants:\n\n```", "```\n-0.403*V2 + 0.165*V3 - 0.369*V4 + 0.155*V5 - 0.002*V6 + 0.618*V7 - 1.661*V8 - 1.496*V9 + 0.134*V10 + 0.355*V11 - 0.818*V12 - 1.158*V13 - 0.003*V14\n\n```", "```\nlda.scalings_[:, 0]\n\n```", "```\narray([-0.40339978,  0.1652546 , -0.36907526,  0.15479789, -0.0021635 ,\n        0.61805207, -1.66119123, -1.49581844,  0.13409263,  0.35505571,\n       -0.81803607, -1.15755938, -0.00269121])\n\n```", "```\npretty_scalings_.LD1\n\n```", "```\nV2    -0.403400\nV3     0.165255\nV4    -0.369075\nV5     0.154798\nV6    -0.002163\nV7     0.618052\nV8    -1.661191\nV9    -1.495818\nV10    0.134093\nV11    0.355056\nV12   -0.818036\nV13   -1.157559\nV14   -0.002691\nName: LD1, dtype: float64\n\n```", "```\ndef calclda(variables, loadings):\n    # find the number of samples in the data set and the number of variables\n    numsamples, numvariables = variables.shape\n    # make a vector to store the discriminant function\n    ld = np.zeros(numsamples)\n    # calculate the value of the discriminant function for each sample\n    for i in range(numsamples):\n        valuei = 0\n        for j in range(numvariables):\n            valueij = variables.iloc[i, j]\n            loadingj = loadings[j]\n            valuei = valuei + (valueij * loadingj)\n        ld[i] = valuei\n    # standardise the discriminant function so that its mean value is 0:\n    ld = scale(ld, with_std=False)\n    return ld\n\n```", "```\n-0.403*V2 - 0.165*V3 - 0.369*V4 + 0.155*V5 - 0.002*V6 + 0.618*V7 - 1.661*V8 - 1.496*V9 + 0.134*V10 + 0.355*V11 - 0.818*V12 - 1.158*V13 - 0.003*V14\n\n```", "```\ncalclda(X, lda.scalings_[:, 0])\n\n```", "```\narray([-4.70024401, -4.30195811, -3.42071952, -4.20575366, -1.50998168,\n       -4.51868934, -4.52737794, -4.14834781, -3.86082876, -3.36662444,\n       -4.80587907, -3.42807646, -3.66610246, -5.58824635, -5.50131449,\n       -3.18475189, -3.28936988, -2.99809262, -5.24640372, -3.13653106,\n       -3.57747791, -1.69077135, -4.83515033, -3.09588961, -3.32164716,\n       -2.14482223, -3.9824285 , -2.68591432, -3.56309464, -3.17301573,\n       -2.99626797, -3.56866244, -3.38506383, -3.5275375 , -2.85190852,\n       -2.79411996, -2.75808511, -2.17734477, -3.02926382, -3.27105228,\n       -2.92065533, -2.23721062, -4.69972568, -1.23036133, -2.58203904,\n       -2.58312049, -3.88887889, -3.44975356, -2.34223331, -3.52062596,\n       -3.21840912, -4.38214896, -4.36311727, -3.51917293, -3.12277475,\n       -1.8024054 , -2.87378754, -3.61690518, -3.73868551,  1.58618749,\n        0.79967216,  2.38015446, -0.45917726, -0.50726885,  0.39398359,\n       -0.92256616, -1.95549377, -0.34732815,  0.20371212, -0.24831914,\n        1.17987999, -1.07718925,  0.64100179, -1.74684421, -0.34721117,\n        1.14274222,  0.18665882,  0.900525  , -0.70709551, -0.59562833,\n       -0.55761818, -1.80430417,  0.23077079,  2.03482711, -0.62113021,\n       -1.03372742,  0.76598781,  0.35042568,  0.15324508, -0.14962842,\n        0.48079504,  1.39689016,  0.91972331, -0.59102937,  0.49411386,\n       -1.62614426,  2.00044562, -1.00534818, -2.07121314, -1.6381589 ,\n       -1.0589434 ,  0.02594549, -0.21887407,  1.3643764 , -1.12901245,\n       -0.21263094, -0.77946884,  0.61546732,  0.22550192, -2.03869851,\n        0.79274716,  0.30229545, -0.50664882,  0.99837397, -0.21954922,\n       -0.37131517,  0.05545894, -0.09137874,  1.79755252, -0.17405009,\n       -1.17870281, -3.2105439 ,  0.62605202,  0.03366613, -0.6993008 ,\n       -0.72061079, -0.51933512,  1.17030045,  0.10824791,  1.12319783,\n        2.24632419,  3.28527755,  4.07236441,  3.86691235,  3.45088333,\n        3.71583899,  3.9222051 ,  4.8516102 ,  3.54993389,  3.76889174,\n        2.6694225 ,  2.32491492,  3.17712883,  2.88964418,  3.78325562,\n        3.04411324,  4.70697017,  4.85021393,  4.98359184,  4.86968293,\n        4.5986919 ,  5.67447884,  5.32986123,  5.03401031,  4.52080087,\n        5.0978371 ,  5.04368277,  4.86980829,  5.61316558,  5.67046737,\n        5.37413513,  3.09975377,  3.35888137,  3.04007194,  4.94861303,\n        4.54504458,  5.27255844,  5.13016117,  4.30468082,  5.08336782,\n        4.06743571,  5.74212961,  4.4820514 ,  4.29150758,  4.50329623,\n        5.04747033,  4.27615505,  5.5380861 ])\n\n```", "```\n# Try either, they produce the same result, use help() for more info\n# lda.transform(X)[:, 0]\nlda.fit_transform(X, y)[:, 0]\n\n```", "```\narray([-4.70024401, -4.30195811, -3.42071952, -4.20575366, -1.50998168,\n       -4.51868934, -4.52737794, -4.14834781, -3.86082876, -3.36662444,\n       -4.80587907, -3.42807646, -3.66610246, -5.58824635, -5.50131449,\n       -3.18475189, -3.28936988, -2.99809262, -5.24640372, -3.13653106,\n       -3.57747791, -1.69077135, -4.83515033, -3.09588961, -3.32164716,\n       -2.14482223, -3.9824285 , -2.68591432, -3.56309464, -3.17301573,\n       -2.99626797, -3.56866244, -3.38506383, -3.5275375 , -2.85190852,\n       -2.79411996, -2.75808511, -2.17734477, -3.02926382, -3.27105228,\n       -2.92065533, -2.23721062, -4.69972568, -1.23036133, -2.58203904,\n       -2.58312049, -3.88887889, -3.44975356, -2.34223331, -3.52062596,\n       -3.21840912, -4.38214896, -4.36311727, -3.51917293, -3.12277475,\n       -1.8024054 , -2.87378754, -3.61690518, -3.73868551,  1.58618749,\n        0.79967216,  2.38015446, -0.45917726, -0.50726885,  0.39398359,\n       -0.92256616, -1.95549377, -0.34732815,  0.20371212, -0.24831914,\n        1.17987999, -1.07718925,  0.64100179, -1.74684421, -0.34721117,\n        1.14274222,  0.18665882,  0.900525  , -0.70709551, -0.59562833,\n       -0.55761818, -1.80430417,  0.23077079,  2.03482711, -0.62113021,\n       -1.03372742,  0.76598781,  0.35042568,  0.15324508, -0.14962842,\n        0.48079504,  1.39689016,  0.91972331, -0.59102937,  0.49411386,\n       -1.62614426,  2.00044562, -1.00534818, -2.07121314, -1.6381589 ,\n       -1.0589434 ,  0.02594549, -0.21887407,  1.3643764 , -1.12901245,\n       -0.21263094, -0.77946884,  0.61546732,  0.22550192, -2.03869851,\n        0.79274716,  0.30229545, -0.50664882,  0.99837397, -0.21954922,\n       -0.37131517,  0.05545894, -0.09137874,  1.79755252, -0.17405009,\n       -1.17870281, -3.2105439 ,  0.62605202,  0.03366613, -0.6993008 ,\n       -0.72061079, -0.51933512,  1.17030045,  0.10824791,  1.12319783,\n        2.24632419,  3.28527755,  4.07236441,  3.86691235,  3.45088333,\n        3.71583899,  3.9222051 ,  4.8516102 ,  3.54993389,  3.76889174,\n        2.6694225 ,  2.32491492,  3.17712883,  2.88964418,  3.78325562,\n        3.04411324,  4.70697017,  4.85021393,  4.98359184,  4.86968293,\n        4.5986919 ,  5.67447884,  5.32986123,  5.03401031,  4.52080087,\n        5.0978371 ,  5.04368277,  4.86980829,  5.61316558,  5.67046737,\n        5.37413513,  3.09975377,  3.35888137,  3.04007194,  4.94861303,\n        4.54504458,  5.27255844,  5.13016117,  4.30468082,  5.08336782,\n        4.06743571,  5.74212961,  4.4820514 ,  4.29150758,  4.50329623,\n        5.04747033,  4.27615505,  5.5380861 ])\n\n```", "```\ndef groupStandardise(variables, groupvariable):\n    # find the number of samples in the data set and the number of variables\n    numsamples, numvariables = variables.shape\n    # find the variable names\n    variablenames = variables.columns\n    # calculate the group-standardised version of each variable\n    variables_new = pd.DataFrame()\n    for i in range(numvariables):\n        variable_name = variablenames[i]\n        variablei = variables[variable_name]\n        variablei_Vw = calcWithinGroupsVariance(variablei, groupvariable)\n        variablei_mean = np.mean(variablei)\n        variablei_new = (variablei - variablei_mean)/(np.sqrt(variablei_Vw))\n        variables_new[variable_name] = variablei_new\n    return variables_new\n\n```", "```\ngroupstandardisedX = groupStandardise(X, y)\n\n```", "```\nlda2 = LinearDiscriminantAnalysis().fit(groupstandardisedX, y)\npretty_scalings(lda2, groupstandardisedX)\n\n```", "```\nlda.fit_transform(X, y)[:, 0]\n\n```", "```\narray([-4.70024401, -4.30195811, -3.42071952, -4.20575366, -1.50998168,\n       -4.51868934, -4.52737794, -4.14834781, -3.86082876, -3.36662444,\n       -4.80587907, -3.42807646, -3.66610246, -5.58824635, -5.50131449,\n       -3.18475189, -3.28936988, -2.99809262, -5.24640372, -3.13653106,\n       -3.57747791, -1.69077135, -4.83515033, -3.09588961, -3.32164716,\n       -2.14482223, -3.9824285 , -2.68591432, -3.56309464, -3.17301573,\n       -2.99626797, -3.56866244, -3.38506383, -3.5275375 , -2.85190852,\n       -2.79411996, -2.75808511, -2.17734477, -3.02926382, -3.27105228,\n       -2.92065533, -2.23721062, -4.69972568, -1.23036133, -2.58203904,\n       -2.58312049, -3.88887889, -3.44975356, -2.34223331, -3.52062596,\n       -3.21840912, -4.38214896, -4.36311727, -3.51917293, -3.12277475,\n       -1.8024054 , -2.87378754, -3.61690518, -3.73868551,  1.58618749,\n        0.79967216,  2.38015446, -0.45917726, -0.50726885,  0.39398359,\n       -0.92256616, -1.95549377, -0.34732815,  0.20371212, -0.24831914,\n        1.17987999, -1.07718925,  0.64100179, -1.74684421, -0.34721117,\n        1.14274222,  0.18665882,  0.900525  , -0.70709551, -0.59562833,\n       -0.55761818, -1.80430417,  0.23077079,  2.03482711, -0.62113021,\n       -1.03372742,  0.76598781,  0.35042568,  0.15324508, -0.14962842,\n        0.48079504,  1.39689016,  0.91972331, -0.59102937,  0.49411386,\n       -1.62614426,  2.00044562, -1.00534818, -2.07121314, -1.6381589 ,\n       -1.0589434 ,  0.02594549, -0.21887407,  1.3643764 , -1.12901245,\n       -0.21263094, -0.77946884,  0.61546732,  0.22550192, -2.03869851,\n        0.79274716,  0.30229545, -0.50664882,  0.99837397, -0.21954922,\n       -0.37131517,  0.05545894, -0.09137874,  1.79755252, -0.17405009,\n       -1.17870281, -3.2105439 ,  0.62605202,  0.03366613, -0.6993008 ,\n       -0.72061079, -0.51933512,  1.17030045,  0.10824791,  1.12319783,\n        2.24632419,  3.28527755,  4.07236441,  3.86691235,  3.45088333,\n        3.71583899,  3.9222051 ,  4.8516102 ,  3.54993389,  3.76889174,\n        2.6694225 ,  2.32491492,  3.17712883,  2.88964418,  3.78325562,\n        3.04411324,  4.70697017,  4.85021393,  4.98359184,  4.86968293,\n        4.5986919 ,  5.67447884,  5.32986123,  5.03401031,  4.52080087,\n        5.0978371 ,  5.04368277,  4.86980829,  5.61316558,  5.67046737,\n        5.37413513,  3.09975377,  3.35888137,  3.04007194,  4.94861303,\n        4.54504458,  5.27255844,  5.13016117,  4.30468082,  5.08336782,\n        4.06743571,  5.74212961,  4.4820514 ,  4.29150758,  4.50329623,\n        5.04747033,  4.27615505,  5.5380861 ])\n\n```", "```\nlda2.fit_transform(groupstandardisedX, y)[:, 0]\n\n```", "```\narray([-4.70024401, -4.30195811, -3.42071952, -4.20575366, -1.50998168,\n       -4.51868934, -4.52737794, -4.14834781, -3.86082876, -3.36662444,\n       -4.80587907, -3.42807646, -3.66610246, -5.58824635, -5.50131449,\n       -3.18475189, -3.28936988, -2.99809262, -5.24640372, -3.13653106,\n       -3.57747791, -1.69077135, -4.83515033, -3.09588961, -3.32164716,\n       -2.14482223, -3.9824285 , -2.68591432, -3.56309464, -3.17301573,\n       -2.99626797, -3.56866244, -3.38506383, -3.5275375 , -2.85190852,\n       -2.79411996, -2.75808511, -2.17734477, -3.02926382, -3.27105228,\n       -2.92065533, -2.23721062, -4.69972568, -1.23036133, -2.58203904,\n       -2.58312049, -3.88887889, -3.44975356, -2.34223331, -3.52062596,\n       -3.21840912, -4.38214896, -4.36311727, -3.51917293, -3.12277475,\n       -1.8024054 , -2.87378754, -3.61690518, -3.73868551,  1.58618749,\n        0.79967216,  2.38015446, -0.45917726, -0.50726885,  0.39398359,\n       -0.92256616, -1.95549377, -0.34732815,  0.20371212, -0.24831914,\n        1.17987999, -1.07718925,  0.64100179, -1.74684421, -0.34721117,\n        1.14274222,  0.18665882,  0.900525  , -0.70709551, -0.59562833,\n       -0.55761818, -1.80430417,  0.23077079,  2.03482711, -0.62113021,\n       -1.03372742,  0.76598781,  0.35042568,  0.15324508, -0.14962842,\n        0.48079504,  1.39689016,  0.91972331, -0.59102937,  0.49411386,\n       -1.62614426,  2.00044562, -1.00534818, -2.07121314, -1.6381589 ,\n       -1.0589434 ,  0.02594549, -0.21887407,  1.3643764 , -1.12901245,\n       -0.21263094, -0.77946884,  0.61546732,  0.22550192, -2.03869851,\n        0.79274716,  0.30229545, -0.50664882,  0.99837397, -0.21954922,\n       -0.37131517,  0.05545894, -0.09137874,  1.79755252, -0.17405009,\n       -1.17870281, -3.2105439 ,  0.62605202,  0.03366613, -0.6993008 ,\n       -0.72061079, -0.51933512,  1.17030045,  0.10824791,  1.12319783,\n        2.24632419,  3.28527755,  4.07236441,  3.86691235,  3.45088333,\n        3.71583899,  3.9222051 ,  4.8516102 ,  3.54993389,  3.76889174,\n        2.6694225 ,  2.32491492,  3.17712883,  2.88964418,  3.78325562,\n        3.04411324,  4.70697017,  4.85021393,  4.98359184,  4.86968293,\n        4.5986919 ,  5.67447884,  5.32986123,  5.03401031,  4.52080087,\n        5.0978371 ,  5.04368277,  4.86980829,  5.61316558,  5.67046737,\n        5.37413513,  3.09975377,  3.35888137,  3.04007194,  4.94861303,\n        4.54504458,  5.27255844,  5.13016117,  4.30468082,  5.08336782,\n        4.06743571,  5.74212961,  4.4820514 ,  4.29150758,  4.50329623,\n        5.04747033,  4.27615505,  5.5380861 ])\n\n```", "```\ndef rpredict(lda, X, y, out=False):\n    ret = {\"class\": lda.predict(X),\n           \"posterior\": pd.DataFrame(lda.predict_proba(X), columns=lda.classes_)}\n    ret[\"x\"] = pd.DataFrame(lda.fit_transform(X, y))\n    ret[\"x\"].columns = [\"LD\"+str(i+1) for i in range(ret[\"x\"].shape[1])]\n    if out:\n        print(\"class\")\n        print(ret[\"class\"])\n        print()\n        print(\"posterior\")\n        print(ret[\"posterior\"])\n        print()\n        print(\"x\")\n        print(ret[\"x\"])\n    return ret\n\nlda_values = rpredict(lda, standardisedX, y, True)\n\n```", "```\nclass\n['2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '2' '2' '3' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '2' '2' '2' '2' '3' '3' '3' '3' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n '3' '2' '3' '3' '3' '3' '3' '3' '3' '2' '2' '2' '3' '2' '3' '3' '2' '2'\n '2' '2' '3' '2' '3' '3' '3' '3' '2' '2' '3' '3' '3' '3' '2' '3']\n\nposterior\n                1         2         3\n0    1.344367e-22  0.999236  0.000764\n1    4.489007e-27  0.983392  0.016608\n2    2.228888e-24  0.791616  0.208384\n3    1.026755e-24  0.500161  0.499839\n4    6.371860e-23  0.790657  0.209343\n5    1.552082e-24  0.981986  0.018014\n6    3.354960e-23  0.951823  0.048177\n7    3.417899e-22  0.925154  0.074846\n8    4.041139e-26  0.978998  0.021002\n9    3.718868e-26  0.619841  0.380159\n..            ...       ...       ...\n168  7.463695e-30  0.500000  0.500000\n169  1.389203e-29  0.499927  0.500073\n170  1.356187e-33  0.500000  0.500000\n171  1.007615e-33  0.500000  0.500000\n172  1.524219e-30  0.500000  0.500000\n173  1.317492e-30  0.500000  0.500000\n174  2.664128e-32  0.500000  0.500000\n175  2.873436e-34  0.500000  0.500000\n176  1.479166e-32  0.500000  0.500000\n177  1.209888e-28  0.500000  0.500000\n\n[178 rows x 3 columns]\n\nx\n          LD1       LD2\n0   -4.700244  1.979138\n1   -4.301958  1.170413\n2   -3.420720  1.429101\n3   -4.205754  4.002871\n4   -1.509982  0.451224\n5   -4.518689  3.213138\n6   -4.527378  3.269122\n7   -4.148348  3.104118\n8   -3.860829  1.953383\n9   -3.366624  1.678643\n..        ...       ...\n168  4.304681  2.391125\n169  5.083368  3.157667\n170  4.067436  0.318922\n171  5.742130  1.467082\n172  4.482051  3.307084\n173  4.291508  3.390332\n174  4.503296  2.083546\n175  5.047470  3.196231\n176  4.276155  2.431388\n177  5.538086  3.042057\n\n[178 rows x 2 columns]\n\n```", "```\ncalcSeparations(lda_values[\"x\"], y)\n\n```", "```\nvariable LD1 Vw= 1.0 Vb= 794.652200566 separation= 794.652200566\nvariable LD2 Vw= 1.0 Vb= 361.241041493 separation= 361.241041493\n\n```", "```\ndef proportion_of_trace(lda):\n    ret = pd.DataFrame([round(i, 4) for i in lda.explained_variance_ratio_ if round(i, 4) > 0], columns=[\"ExplainedVariance\"])\n    ret.index = [\"LD\"+str(i+1) for i in range(ret.shape[0])]\n    ret = ret.transpose()\n    print(\"Proportion of trace:\")\n    print(ret.to_string(index=False))\n    return ret\n\nproportion_of_trace(LinearDiscriminantAnalysis(solver=\"eigen\").fit(X, y));\n\n```", "```\nProportion of trace:\n    LD1     LD2\n 0.6875  0.3125\n\n```", "```\ndef ldahist(data, g, sep=False):\n    xmin = np.trunc(np.min(data)) - 1\n    xmax = np.trunc(np.max(data)) + 1\n    ncol = len(set(g))\n    binwidth = 0.5\n    bins=np.arange(xmin, xmax + binwidth, binwidth)\n    if sep:\n        fig, axl = plt.subplots(ncol, 1, sharey=True, sharex=True)\n    else:\n        fig, axl = plt.subplots(1, 1, sharey=True, sharex=True)\n        axl = [axl]*ncol\n    for ax, (group, gdata) in zip(axl, data.groupby(g)):\n        sns.distplot(gdata.values, bins, ax=ax, label=\"group \"+str(group))\n        ax.set_xlim([xmin, xmax])\n        if sep:\n            ax.set_xlabel(\"group\"+str(group))\n        else:\n            ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    plt.tight_layout()\n\n```", "```\nldahist(lda_values[\"x\"].LD1, y)\n\n```", "```\nldahist(lda_values[\"x\"].LD2, y)\n\n```", "```\nsns.lmplot(\"LD1\", \"LD2\", lda_values[\"x\"].join(y), hue=\"V1\", fit_reg=False);\n\n```", "```\nprintMeanAndSdByGroup(lda_values[\"x\"], y);\n\n```", "```\n## Means:\n\n```", "```\n## Standard deviations:\n\n```", "```\n## Sample sizes:\n\n```", "```\ndef calcAllocationRuleAccuracy(ldavalue, groupvariable, cutoffpoints):\n    # find out how many values the group variable can take\n    levels = sorted(set((groupvariable)))\n    numlevels = len(levels)\n    confusion_matrix = []\n    # calculate the number of true positives and false negatives for each group\n    for i, leveli in enumerate(levels):\n        levelidata = ldavalue[groupvariable==leveli]\n        row = []\n        # see how many of the samples from this group are classified in each group\n        for j, levelj in enumerate(levels):\n            if j == 0:\n                cutoff1 = cutoffpoints[0]\n                cutoff2 = \"NA\"\n                results = (levelidata <= cutoff1).value_counts()\n            elif j == numlevels-1:\n                cutoff1 = cutoffpoints[numlevels-2]\n                cutoff2 = \"NA\"\n                results = (levelidata > cutoff1).value_counts()\n            else:\n                cutoff1 = cutoffpoints[j-1]\n                cutoff2 = cutoffpoints[j]\n                results = ((levelidata > cutoff1) & (levelidata <= cutoff2)).value_counts()\n            try:\n                trues = results[True]\n            except KeyError:\n                trues = 0\n            print(\"Number of samples of group\", leveli, \"classified as group\", levelj, \":\", trues, \"(cutoffs:\", cutoff1, \",\", cutoff2, \")\")\n            row.append(trues)\n        confusion_matrix.append(row)\n    return confusion_matrix\n\n```", "```\nconfusion_matrix = calcAllocationRuleAccuracy(lda_values[\"x\"].iloc[:, 0], y, [-1.751107, 2.122505])\n\n```", "```\nNumber of samples of group 1 classified as group 1 : 56 (cutoffs: -1.751107 , NA )\nNumber of samples of group 1 classified as group 2 : 3 (cutoffs: -1.751107 , 2.122505 )\nNumber of samples of group 1 classified as group 3 : 0 (cutoffs: 2.122505 , NA )\nNumber of samples of group 2 classified as group 1 : 5 (cutoffs: -1.751107 , NA )\nNumber of samples of group 2 classified as group 2 : 65 (cutoffs: -1.751107 , 2.122505 )\nNumber of samples of group 2 classified as group 3 : 1 (cutoffs: 2.122505 , NA )\nNumber of samples of group 3 classified as group 1 : 0 (cutoffs: -1.751107 , NA )\nNumber of samples of group 3 classified as group 2 : 0 (cutoffs: -1.751107 , 2.122505 )\nNumber of samples of group 3 classified as group 3 : 48 (cutoffs: 2.122505 , NA )\n\n```", "```\ndef webprint_confusion_matrix(confusion_matrix, classes_names):\n    display(pd.DataFrame(confusion_matrix, index=[\"Is group \"+i for i in classes_names], columns=[\"Allocated to group \"+i for i in classes_names]))\n\nwebprint_confusion_matrix(confusion_matrix, lda.classes_)\n\n```", "```\nimport sklearn.metrics as metrics\n\ndef lda_classify(v, levels, cutoffpoints):\n    for level, cutoff in zip(reversed(levels), reversed(cutoffpoints)):\n        if v > cutoff: return level\n    return levels[0]\n\ny_pred = lda_values[\"x\"].iloc[:, 0].apply(lda_classify, args=(lda.classes_, [-1.751107, 2.122505],)).values\ny_true = y\n\n```", "```\n# from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#example-model-selection-plot-confusion-matrix-py\ndef plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(target_names))\n    plt.xticks(tick_marks, target_names, rotation=45)\n    plt.yticks(tick_marks, target_names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nprint(metrics.classification_report(y_true, y_pred))\ncm = metrics.confusion_matrix(y_true, y_pred)\nwebprint_confusion_matrix(cm, lda.classes_)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplot_confusion_matrix(cm_normalized, lda.classes_, title='Normalized confusion matrix')\n\n```", "```\n             precision    recall  f1-score   support\n\n          1       0.92      0.95      0.93        59\n          2       0.96      0.92      0.94        71\n          3       0.98      1.00      0.99        48\n\navg / total       0.95      0.95      0.95       178\n\n```"]