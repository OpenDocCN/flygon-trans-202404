- en: Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is all about using your computer to "learn" how to deal with
  prefs: []
  type: TYPE_NORMAL
- en: problems without “programming". (It’s a branch of artificial intelligence)
  prefs: []
  type: TYPE_NORMAL
- en: We take some data, train a model on that data, and use the trained model to
  prefs: []
  type: TYPE_NORMAL
- en: make predictions on new data. Basically is a way to make the computer create
    a
  prefs: []
  type: TYPE_NORMAL
- en: program that gives some output with a known input and that latter give a intelligent
  prefs: []
  type: TYPE_NORMAL
- en: output to a different but similar input.
  prefs: []
  type: TYPE_NORMAL
- en: We need machine learning on cases that would be difficult to program by hand
    all
  prefs: []
  type: TYPE_NORMAL
- en: possible variants of a classification/prediction problem
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic Idea of Machine Learning is to make the computer learn something
    from the data. Machine learning comes in two flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised Learning: You give to the computer some pairs of inputs/outputs,
    so in the future new when new inputs are presented you have an intelligent output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised Learning: You let the computer learn from the data itself without
    showing what is the expected output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](supervised_unsupervised.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Classification: Your train with images/labels. Then on the future when
    you give a new image expecting that the computer will recognise the new object
    (Classification)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Market Prediction: You train the computer with historical market data, and
    ask the computer to predict the new price on the future (Regression)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](Classification_Regression.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of Unsupervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering: You ask the computer to separate similar data on clusters, this
    is essential in research and science.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'High Dimension Visualisation: Use the computer to help us visualise high dimension
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generative Models: After a model capture the probability distribution of your
    input data, it will be able to generate more data. This is very useful to make
    your classifier more robust.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](TSNE.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](GenerativeModel.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the following problem, you are working on a system that should classify
    if a tumour is benign or malign, at a first moment the only information that you
    have to decide is the tumour size. We can see the your training data distribution
    bellow. Observe that the characteristic (or feature) tumour size does not seems
    to be alone a good indicator to decide if the tumour is malignant or benign.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Features1.PNG)'
  prefs:
  - PREF_H2
  type: TYPE_IMG
- en: Now consider that we add one more feature to the problem (Age).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Features2.PNG)'
  prefs: []
  type: TYPE_IMG
- en: The intuition is that with more features that are relevant to the problem that
    you want to classify, you will make your system more robust. Complex systems like
    this one could have up to thousand of features. One question that you may ask
    is how can I determine the features that are relevant to my problem. Also which
    algorithm to use best to tackle infinite amount of features, for example Support
    Vector machines have mathematical tricks that allow a very large number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to give a set of inputs and it's expected outputs, so after training
    we will have a model (hypothesis) that will then map new data to one of the categories
    trained.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Workflow1.PNG)'
  prefs: []
  type: TYPE_IMG
- en: 'Ex: Imagine that you give a set of images and the following categories, duck
    or not duck, the idea is that after training you can get an image of a duck from
    internet and the model should say "duck"'
  prefs: []
  type: TYPE_NORMAL
- en: '![](DuckNoDuck.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Bag of tricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of different machine learning algorithms, on this book we will
    concentrate more on neural networks, but there is no one single best algorithm
    it all depends on the problem that you need to solve, the amount of data available.
  prefs: []
  type: TYPE_NORMAL
- en: '![](CompareAlgos.png)![](ListAlgos.png)'
  prefs: []
  type: TYPE_IMG
- en: The Basic Recipe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the super simple recipe (maybe cover 50%), we will explain the “how”
    later but this gives some hint on how to think when dealing with a machine learning
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: First check if your model works well on the training data, and if not make the
    model more complex (Deeper, or more neurons)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If yes then test on the “test” data, if not you overfit, and the most reliable
    way to cure overfit is to get more data (Putting the test data on the train data
    does not count)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the way the biggest public image dataset (imagenet) is not big enough to
    the 1000 classes imagenet competition
  prefs: []
  type: TYPE_NORMAL
- en: '![](BasicRecipe.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the next chapter we will learn the basics of Linear Algebra needed on artificial
    intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Algebra is a omit important topic to understand, a lot of deep learning
    algorithms use it, so this chapter will teach the topics needed to understand
    what will come next.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars, Vectors and Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scalars: A single number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vector: Array of numbers, where each element is identified by an single index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Matrix: Is a 2D array of numbers, bellow we have a (2-row)X(3-col) matrix.
    On matrix a single element is identified by two indexes instead of one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](scalar-vector-matrix.gif)'
  prefs: []
  type: TYPE_IMG
- en: '![](enter-the-matrix-10-638.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here we show how to create them on matlab and python(numpy)
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatlabSimpleMatrix.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](PythonSimpleMatrix.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will show the important operations.
  prefs: []
  type: TYPE_NORMAL
- en: Transpose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you have an image(2d matrix) and multiply with a rotation matrix, you will
    have a rotated image. Now if you multiply this rotated image with the transpose
    of the rotation matrix, the image will be "un-rotated"
  prefs: []
  type: TYPE_NORMAL
- en: Basically transpose a matrix is to swap it's rows and cols. Or rotate the matrix
    around it's main diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatrixTranspose.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](TransposeAnim.gif)![](Transpose2.png)'
  prefs: []
  type: TYPE_IMG
- en: Addition/Subtraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically we add 2 matrices by adding each element with the other. Both matrices
    need to have same dimension
  prefs: []
  type: TYPE_NORMAL
- en: '![](matrix-addition.gif)'
  prefs: []
  type: TYPE_IMG
- en: '![](matrix-subtraction.gif)'
  prefs: []
  type: TYPE_IMG
- en: Multiply by scalar
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Multiply all elements of the matrix by a scalar
  prefs: []
  type: TYPE_NORMAL
- en: '![](matrix-multiply-constant.gif)'
  prefs: []
  type: TYPE_IMG
- en: Matrix Multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The matrix product of an n×m matrix with an m×ℓ matrix is an n×ℓ matrix. The
    (i,j) entry of the matrix product AB is the dot product of the ith row of A with
    the jth column of B.
  prefs: []
  type: TYPE_NORMAL
- en: The number of columns on the first matrix must match the number of rows on the
    second matrix. The result will be another matrix or a scalar with dimensions defined
    by the rows of the first matrix and columns of the second matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatrixMultiplication_Check.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically the operation is to "dot product" each row of the first matrix(k)
    with each column of the second matrix(m).
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatrixMultplyMovement.png)'
  prefs: []
  type: TYPE_IMG
- en: Some examples
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatrixMultiplication_Example.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Matrix_Multiply_0.gif)'
  prefs: []
  type: TYPE_IMG
- en: '![](Matrix_Multiply_1.gif)'
  prefs: []
  type: TYPE_IMG
- en: '![](Matrix_Multiply_2.gif)'
  prefs: []
  type: TYPE_IMG
- en: Commutative property
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Matrix multiplication is not always commutative ![](bcbe2544.png), but the dot
    product between 2 vectors is commutative, ![](b7244efc.png).
  prefs: []
  type: TYPE_NORMAL
- en: Types of Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: There are some special matrices that are interesting to know.
  prefs: []
  type: TYPE_NORMAL
- en: 'Identity: If you multiply a matrix B by the identity matrix you will have the
    matrix B as result, the diagonal of the identity matrix is filled with ones, all
    the rest are zeros.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inverse: Used on matrix division and to solve linear systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](matrix-identity.gif)'
  prefs: []
  type: TYPE_IMG
- en: Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we need to organize information with more than 2 dimensions, we called
    tensor an n-dimension array.
  prefs: []
  type: TYPE_NORMAL
- en: For example an 1d tensor is a vector, a 2d tensor is a matrix, a 3d tensor is
    a 3d tensor is a cube, and a 4d tensor is an vector of cubes, a 5d tensor is a
    matrix of cubes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Tensor_2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Tensor_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Practical example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will show to use matrix multiplication to implement a linear classifier.
    Don't care now about what linear classifier does, just pay attention that we use
    linear algebra do solve it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Linear_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Merging the weights and bias (bias trick) to solve the linear classification
    as a single matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: '![](BiasTrick.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On Matlab
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatlabLinearClassifer.png)'
  prefs: []
  type: TYPE_IMG
- en: On Python
  prefs: []
  type: TYPE_NORMAL
- en: '![](PythonLinearClassifer.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next chapter we will learn about Linear Classification.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will learn about Supervised learning as well as talk a bit
    about Cost Functions and Gradient descent. Also we will learn about 2 simple algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression (for Regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression (for Classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thing to learn about supervised learning is that every sample data
    point x has an expected output or label y, in other words your training is composed
    of ![](1241931f.png) pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example consider the table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size in feet (x) | Price (y) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2104 | 460 |'
  prefs: []
  type: TYPE_TB
- en: '| 1416 | 232 |'
  prefs: []
  type: TYPE_TB
- en: '| 1534 | 315 |'
  prefs: []
  type: TYPE_TB
- en: '| 852 | 178 |'
  prefs: []
  type: TYPE_TB
- en: This table (or training set) shows the sizes of houses along with their price.
    So a house of size 2104 feet costs 460\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is that we could use data like this to create models that can predict
    some outcome (ie: Price) from different inputs (ie: Size in feet).'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Regression is about returning a continuous scalar number from some input. The
    model or hypothesis that we will learn will predict a value following a linear
    rule. However sometimes a linear model is not enough to capture the underlying
    nature of your data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](LinearRegExample.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hypothesis:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Basically linear regression trys to create a line ![](e8e26b41.png), that fits
    the data on training, e.g.
  prefs: []
  type: TYPE_NORMAL
- en: '![](af468600.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](83f7d33e.png)'
  prefs: []
  type: TYPE_IMG
- en: The whole idea of supervised learning is that we try to learn the best parameters
    (theta in this case) from our training set.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we talk about how to learn the parameters (also called weights) of our
    hypothesis we need to know how to evaluate if our current set of weights are already
    doing a good job. The function that does this job is called Loss or Cost function.
    Basically it will return a scalar value between 0(No error) and infinity (Really
    bad).
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of such a function is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](6e23d77c.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: 'm: Number of items in your dataset (i): i-th element of your dataset y: Label(expected
    value) in dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paticular cost function is called mean-squared error loss, and is actually
    very useful for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: During training we want to minimise our loss by continually changing the theta
    parameters. Another nice feature of this paticular function is that it is a convex
    function so it is guaranteed to have no more than one minimum, which will also
    be it's global minimum. This makes it easier for us to optimise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our task is therefor to find:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](738d7cd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is a simple algorithm that will try to find the local minimum
    of a function. We use gradient descent to minimise our loss function. One important
    feature to observe on gradient descent is that it will more often than not get
    stuck into the first local minimum that it encounters. However there is no guarantee
    that this local minimum it finds is the best (global) one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](GradDecAlgo.png)'
  prefs: []
  type: TYPE_IMG
- en: The gradient descent needs the first derivative of the function that you want
    to minimise. So if we want to minimise some function by changing parameters, you
    need to derive this function with respect to these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note is as this algorithm will execute on all samples in your training
    set it does not scale well for bigger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Simple example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bellow we have some simple implementation in matlab that uses gradient descent
    to minimise the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](cc7a762a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s derivative with respect to x is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](e1c43b85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code to find at what point our local minimum occurs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Gradient descent for linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to use gradient descent for linear regression you need to calculate
    the derivative of it's loss (means squared error) with respect to it's parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This derivative will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](5c0d652.png)'
  prefs: []
  type: TYPE_IMG
- en: Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The name may sound confusing but actually Logistic Regression is simply all
    about classification. For instance consider the example below where we want to
    classify 2 classes: x''s and o''s. Now our output y will have two possible values
    [0,1].'
  prefs: []
  type: TYPE_NORMAL
- en: Normally we do not use Logistic Regression if we have a large number of features
    (e.g. more than 100 features).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image%20%5b5%5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Hypothesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our hypothesis is almost the same compared to the linear regression however
    the difference is that now we use a function that will force our output to give
    ![](e640a3a9.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](7c76275d.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](f5fb1593.png) is the logistic or sigmoid function, therefore'
  prefs: []
  type: TYPE_NORMAL
- en: '![](9f0ba3d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here the sigmoid function will convert a scalar number to some probability between
    0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Cost function for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with classification problems, we should use a different cost/loss
    function. A good candidate for classification is the cross-entropy cost function.
    By the way this function can be found by using the Maximum Likelihood estimation
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-entropy cost function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](a5d0adaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Again our objective is to find the best parameter theta that minimize this function,
    so to use gradient descent we need to calculate the derivative of this function
    with respect to theta
  prefs: []
  type: TYPE_NORMAL
- en: '![](5c0d652.png)'
  prefs: []
  type: TYPE_IMG
- en: One cool thing to note is that the derivative is the same as the derivative
    from linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting (Variance) and Underfitting (Bias)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main idea of training in machine learning is to let the computer learn the
    underlying structure of the training set and not just the specific training set
    that it sees. If it does not learn the underlying structure and instead learns
    just the structure of the training samples then we say our model has overfit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can tell overfitting has occured when our model has a very good accuracy
    on the training set (e.g.: 99.99%) but does not perform that well on the test
    set (e.g.: 60%).'
  prefs: []
  type: TYPE_NORMAL
- en: This basically occurs when your model is too complex in relation to the available
    data, and/or you don't have enough data to capture the underlying data pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The opposite of this problem is called Underfitting, basically it happens when
    your model is too simple to capture the concept given in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some graphical examples of these problems are shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Left:** Underfit'
  prefs: []
  type: TYPE_NORMAL
- en: '**Middle:** Perfect'
  prefs: []
  type: TYPE_NORMAL
- en: '**Right:** Overfit'
  prefs: []
  type: TYPE_NORMAL
- en: '![](UnderOverFit.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Solving Overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Get more data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use regularisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check other model architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving Underfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Add more layers or more parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check other architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: It's a method that helps overfitting by forcing your hypothesis parameters to
    have nice small values and to force your model to use all the available parameters.
    To use regularization you just need to add an extra term to your cost/loss function
    during training. Below we have an example of the Mean squared error loss function
    with an added regularization term.
  prefs: []
  type: TYPE_NORMAL
- en: '![](80fcd243.png)'
  prefs: []
  type: TYPE_IMG
- en: Also the regularized version of the cross-entropy loss function
  prefs: []
  type: TYPE_NORMAL
- en: '![](185df188.png)'
  prefs: []
  type: TYPE_IMG
- en: This term will basically multiply by ![](41d912dc.png) all parameters ![](5a7fef68.png)
    from your hypothesis
  prefs: []
  type: TYPE_NORMAL
- en: Normally we don't need to regularise the bias term of our hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: During gradient descent you also need to calculate the derivative of those terms.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can imagine that besides forcing the weights to have lower values, the regularisation
    will also spread the "concept" across more weights. One way to think about what
    happens when we regularise is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: For our input x we can have 2 weight vectors w1 or w2\. When we multiply our
    input with our weight vector we will get the same output of 1 for each. However
    the weight vector w2 is a better choice of weight vector as this will look at
    more of our input x compared to w1 which only looks at one value of x.
  prefs: []
  type: TYPE_NORMAL
- en: '![](cadca0ba.png)'
  prefs: []
  type: TYPE_IMG
- en: In practice this might reduce performance on our training set but will improve
    generalisation and thus performance when testing.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Linear Hypothesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes our hypothesis needs to have non-linear terms to be able to predict
    more complex classification/regression problems. We can for instance use Logistic
    Regression with quadratic terms to capture this complexity. As mentioned earlier
    this does not scale well for large number of features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](NonLinearFeatures.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance if we would like to classify a 100x100 grayscale image using logistic
    regression we would need 50 million parameters to include the quadratic terms.
    Training with this number of features will need at least 10x more images (500
    millions) to avoid overfitting. Also the computational cost will be really high.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters we will learn other algorithms that scale better for bigger
    number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Local minimas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As we increase the number of parameters on our model our loss function will
    start to find multiple local minima. This can be a problem for training because
    the gradient descent can get stuck in one of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](sgd_stuck.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Actually some recent papers also try to prove that some of the methods used
    today (ie: Deep Neural networks) the local-minima is actually really close to
    the global minima, so you don''t need to care about using a Convex loss or about
    getting stuck in a local-minima.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeeperLocalMinimas.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Family of models that takes a very “loose” inspiration from the brain, used
    to approximate functions that depends on a large number of inputs. (Is a very
    good Pattern recognition model).
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are examples of Non-Linear hypothesis, where the model can learn
    to classify much more complex relations. Also it scale better than Logistic Regression
    for large number of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s formed by artificial neurons, where those neurons are organised in layers.
    We have 3 types of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We classify the neural networks from their number of hidden layers and how they
    connect, for instance the network above have 2 hidden layers. Also if the neural
    network has/or not loops we can classify them as Recurrent or Feed-forward neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks from more than 2 hidden layers can be considered a deep neural
    network. The advantage of using more deep neural networks is that more complex
    patterns can be recognised.
  prefs: []
  type: TYPE_NORMAL
- en: '![](neuralnetworks.png)'
  prefs:
  - PREF_H2
  type: TYPE_IMG
- en: Bellow we have an example of a 2 layer feed forward artificial neural network.
    Imagine that the connections between neurons are the parameters that will be learned
    during training. On this example Layer L1 will be the input layer, L2/L3 the hidden
    layer and L4 the output layer
  prefs: []
  type: TYPE_NORMAL
- en: '![](NeuralNetwork.png)'
  prefs: []
  type: TYPE_IMG
- en: Brain Differences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now wait before you start thinking that you can just create a huge neural network
    and call strong AI, there are some few points to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a list:'
  prefs: []
  type: TYPE_NORMAL
- en: The artificial neuron fires totally different than the brain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A human brain has 100 billion neurons and 100 trillion connections (synapses)
    and operates on 20 watts(enough to run a dim light bulb) - in comparison the biggest
    neural network have 10 million neurons and 1 billion connections on 16,000 CPUs
    (about 3 million watts)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The brain is limited to 5 types of input data from the 5 senses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Children do not learn what a cow is by reviewing 100,000 pictures labelled “cow”
    and “not cow”, but this is how machine learning works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probably we don't learn by calculating the partial derivative of each neuron
    related to our initial concept. (By the way we don't know how we learn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real Neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](neuron.png)'
  prefs: []
  type: TYPE_IMG
- en: Artificial Neuron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](neuron_model.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The single artificial neuron will do a dot product between w and x, then add
    a bias, the result is passed to an activation function that will add some non-linearity.
    The neural network will be formed by those artificial neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The non-linearity will allow different variations of an object of the same class
    to be learned separately. Which is a different behaviour compared to the linear
    classifier that tries to learn all different variations of the same class on a
    single set of weights. More neurons and more layers is always better but it will
    need more data to train.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer learn a concept, from it's previous layer. So it's better to have
    deeper neural networks than a wide one. (Took 20 years to discover this)
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: After the neuron do the dot product between it's inputs and weights, it also
    apply a non-linearity on this result. This non-linear function is called Activation
    Function.
  prefs: []
  type: TYPE_NORMAL
- en: On the past the popular choice for activation functions were the sigmoid and
    tanh. Recently it was observed the ReLU layers has better response for deep neural
    networks, due to a problem called vanishing gradient. So you can consider using
    only ReLU neurons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ffa45100.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](ActivationFunctions.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Example of simple network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the Neural network bellow with 1 hidden layer, 3 input neurons, 3 hidden
    neurons and one output neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleANN.png)'
  prefs: []
  type: TYPE_IMG
- en: We can define all the operation that this network will do as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](2c729b18.png)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](9d409d72.png) means the activation(output) of the first neuron of layer
    2 (hidden layer on this case). The first layer, (input layer) can be considered
    as ![](3d270619.png) and it's values are just the input vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the connections between each layer as a matrix of parameters. Consider
    those matrices as the connections between layers. On this case we have to matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](afc31031.png): Map the layer 1 to layer 2 (Input and Hidden layer)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](e286ad01.png): Map layer 2 to to layer 2 (Hidden and output layer)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Also you consider the dimensions of ![](afc31031.png) as [number of neurons
    on layer 2] x [Number of neurons layer 1 +1]. In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](2fa47a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](a72adc56.png): Number of neurons on next layer'
  prefs: []
  type: TYPE_NORMAL
- en: '![](e8716ce7.png): Number of neurons on the current layer + 1'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this is only true if we consider to add the bias as part of our
    weight matrices, this could depend from implementation to implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Why is better than Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Consider that the neural network as a cascaded chain of logistic regression,
    where the input of each layer is the output of the previous one. Another way to
    think on this is that each layer learn a concept with the output of the previous
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeepConcept.png)'
  prefs: []
  type: TYPE_IMG
- en: This is nice because the layer does not need to learn the whole concept at once,
    but actually build a chain of features that build that knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: You can calculate the output of the whole layer ![](af123120.png)as a matrix
    multiplication followed by a element-wise activation function. This has the advantage
    of performance, considering that you are using tools like Matlab, Numpy, or also
    if you are implementing on hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism of calculating the output of each layer with the output of the
    previous layer, from the beginning(input layer) to it's end(output layer) is called
    forward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ForwardPropagation.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To better understanding let''s break the activation of some layer as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](b1457857.png)'
  prefs: []
  type: TYPE_IMG
- en: So using this formulas you can calculate the activation of each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple class problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the multi-class classification problem you need to allocate one neuron for
    each class, than during training you provide a one-hot vector for each one of
    your desired class. This is somehow easier than the logistic regression one-vs-all
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](MultiClassProb.png)'
  prefs: []
  type: TYPE_IMG
- en: Cost function (Classification)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function of neural networks, it''s a little more complicated than
    the logistic regression. So for classification on Neural networks we should use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](6a7b9836.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'L: Number of layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'm: Dataset size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: Number of classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](bd75c77e.png): Number of neurons (not counting bias) from layer l'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During training we need to calculate the partial derivative of this cost function
    with respect to each parameter on your neural network. Actually what we need to
    compute is:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss itself (Forward-propagation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of the loss w.r.t each parameter (Back-propagation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation it's an efficient algorithm that helps you calculate the derivative
    of the cost function with respect to each parameter of the neural network. The
    name backpropagation comes from the fact that now we start calculating errors
    from all your neurons from the output layer to the input layer direction. After
    those errors are calculated we simply multiply them by the activation calculated
    during forward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](BackPropNeuralNet.png)'
  prefs: []
  type: TYPE_IMG
- en: Doing the backpropagation (Vectorized)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned the backpropagation will flow on the reverse order iterating from
    the last layer. So starting from the output layer we calculate the output layer
    error.
  prefs: []
  type: TYPE_NORMAL
- en: The "error values" for the last layer are simply the differences of our actual
    results in the last layer and the correct outputs in y.
  prefs: []
  type: TYPE_NORMAL
- en: '![](c4959933.png)'
  prefs: []
  type: TYPE_IMG
- en: Where
  prefs: []
  type: TYPE_NORMAL
- en: '![](72faac05.png): Expected output from training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](7ace1459.png): Network output/activation of the last L layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all other layers (layers before the last layer until the input)
  prefs: []
  type: TYPE_NORMAL
- en: '![](4c4e4cd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Where
  prefs: []
  type: TYPE_NORMAL
- en: '![](70fbe2a7.png): Error of layer l'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](7b18f6d2.png): Derivative of activation function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](7893cf17.png): Pre-activation of layer l'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '.*: Element wise multiplication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After all errors(delta) are calculated we need to actually calculate the derivative
    of the loss, which is the product of the error times the activation of each respective
    neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](9c286de2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we're ignoring the regularisation term.
  prefs: []
  type: TYPE_NORMAL
- en: Complete algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bellow we describe the whole procedure in pseudo-code
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlgoBackProp.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient Checking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to verify if your backpropagation code is right we can estimate the
    gradient, using other algorithm, unfortunately we cannot use this algorithm in
    practice because it will make the training slow, but we can use to compare it''s
    results with the backpropagation. Basically we will calculate numerically the
    derivative of the loss with respect to each parameter by calculating the loss
    and adding a small perturbation (ie: ![](2af86d7a.png)) to each parameter one
    at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](GradientCheck.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](1151abb.png)'
  prefs: []
  type: TYPE_IMG
- en: Actually you will compare this gradient with the output of the backpropagation
    ![](d94e9f68.png).
  prefs: []
  type: TYPE_NORMAL
- en: Again this will be really slow because we need to calculate the loss again with
    this small perturbation twice for each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: For example suppose that you have 3 parameters ![](75a959e9.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](c1fe604b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](765311e0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](62de42ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The way that you initialize your network parameters is also important, you cannot
    for instance initialize all your weights to zero, normally you want to initialize
    them with small random values on the range ![](3ab3a524.png) but somehow also
    take into account that you don't want to have some sort of symmetry of the random
    values between layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What we can see from the code above is that we create random numbers independently
    for each layer, and all of them in between the range ![](3ab3a524.png)
  prefs: []
  type: TYPE_NORMAL
- en: Training steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now just to list the steps required to train a neural network. Here we mention
    the term epoch which means a complete pass intro all elements of your training
    set. Actually you repeat your training set over and over because the weights don't
    completely learn a concept in a single epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize weights randomly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each epoch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the forward propagation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the backward propagation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update weights with Gradient descent (Optionally use gradient checking to verify
    backpropagation)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step 2 until you finish all epochs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training/Validation/Test data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Some good practices to create your dataset for training your hypothesis models
  prefs: []
  type: TYPE_NORMAL
- en: Collect as many data as possible
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge/Shuffle all this data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide this dataset into train(60%)/validation(20%)/test(20%) set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid having test from a different distribution of your train/validation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the validation set to tune your model (Number of layers/neurons)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check overall performance with the test set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we say to use the validation set it means that we're going to change parameters
    of our model and check which one get better results on this validation set, don't
    touch your training set. If you are having bad results on your test set consider
    getting more data, and verify if your train/test/val come from the same distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Effects of deep neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier having deeper and bigger neural networks is always better
    in terms of recognition performance, but some problems also arise with more complex
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper and more complex neural networks, need more data to train (10x number
    of parameters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over-fit can become a problem so do regularization (Dropout, L2 regularization)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prediction time will increase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks as computation graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In order to calculate the back-propagation, it's easier if you start representing
    your hypothesis as computation graphs. Also in next chapters we use different
    types of layers working together, so to simplify development consider the neural
    networks as computation graphs. The idea is that if you provide for each node
    of your graph the forward/backward implementation, the back propagation becomes
    much more easier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](NeuralNetworkGraph.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A linear classifier does classification decision based on the value of a linear
    combination of the characteristics. Imagine that the linear classifier will merge
    into it's weights all the characteristics that define a particular class. (Like
    merge all samples of the class cars together)
  prefs: []
  type: TYPE_NORMAL
- en: This type of classifier works better when the problem is linear separable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](linear_vs_nonlinear_problems.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](bffbeee2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](pixelspaceLinear.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The weight matrix will have one row for every class that needs to be classified,
    and one column for ever element(feature) of x.On the picture above each line will
    be represented by a row in our weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Weight and Bias Effect
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The effect of changing the weight will change the line angle, while changing
    the bias, will move the line left/right
  prefs: []
  type: TYPE_NORMAL
- en: Parametric Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](ParametricModel.jpg)'
  prefs:
  - PREF_H3
  type: TYPE_IMG
- en: 'The idea is that out hypothesis/model has parameters, that will aid the mapping
    between the input vector to a specific class score. The parametric model has two
    important components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Score Function: Is a function ![](647cc9bd.png) that will map our raw input
    vector to a score vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss Function: Quantifies how well our current set of weights maps some input
    x to a expected output y, the loss function is used during training time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On this approach, the training phase will find us a set of parameters that will
    change the hypothesis/model to map some input, to some of the output class.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, which consist as a optimisation problem, the weights
    (W) and bias (b) are the only thing that we can change.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Linear_Classification.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now some topics that are important on the diagram above:'
  prefs: []
  type: TYPE_NORMAL
- en: The input image x is stretched to a single dimension vector, this loose spatial
    information
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weight matrix will have one column for every element on the input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weight matrix will have one row for every element of the output (on this
    case 3 labels)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bias will have one row for every element of the output (on this case 3 labels)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss will receive the current scores and the expected output for it's current
    input X
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider each row of W a kind of pattern match for a specified class. The score
    for each class is calculated by doing a inner product between the input vector
    X and the specific row for that class. Ex:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](3bab8058.png)'
  prefs: []
  type: TYPE_IMG
- en: Example on Matlab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](calc_scores_matlab.PNG)'
  prefs: []
  type: TYPE_IMG
- en: The image bellow reshape back the weights to an image, we can see by this image
    that the training try to compress on each row of W all the variants of the same
    class. (Check the horse with 2 heads)
  prefs: []
  type: TYPE_NORMAL
- en: '![](LinearTemplate.png)'
  prefs: []
  type: TYPE_IMG
- en: Bias trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Some learning libraries implementations, does a trick to consider the bias as
    part of the weight matrix, the advantage of this approach is that we can solve
    the linear classification with a single matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: '![](8124c541.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Bias_Trick.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Basically you add an extra row at the end of the input vector, and concatenate
    a column on the W matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Input and Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The input vector sometimes called feature vector, is your input data that is
    sent to the classifier. As the linear classifier does not handle non-linear problems,
    it is the responsibility of the engineer, process this data and present it in
    a form that is separable to the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The best case scenario is that you have a large number of features, and each
    of them has a high correlation to the desired output and low correlation between
    thems
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As mention earlier the Loss/Cost functions are mathematical functions that will
    answer how well your classifier is doing it's job with the current set of parameters
    (Weights and Bias). One important step on supervised learning is the choice of
    the right loss function for the job/task.
  prefs: []
  type: TYPE_NORMAL
- en: Model Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models learn by updating it's parameters (weights and biases)
    towards the direction of the correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: Basic structure of a learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the picture bellow we will show the basic blocks of a machine learning model
  prefs: []
  type: TYPE_NORMAL
- en: '![](MachineLearningModel.png)'
  prefs: []
  type: TYPE_IMG
- en: On this picture we can detect the following components
  prefs: []
  type: TYPE_NORMAL
- en: 'Training dataset: Basically a high speed disk containing your training data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Batch of samples: A list of pairs (X,Y), consisting on inputs, expected outputs,
    for example X can be a image and Y the label "cat"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Parameters: Set of parameters used by your model layers, to map X to Y'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model: Set of computing layers that transform an input X and weights W, into
    a score (probable Y)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loss function: Responsible to say how far our score is from the ideal response
    Y, the output of the loss function is a scalar. Another way is also to consider
    that the loss function say how bad is your current set of parameters W.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What we will do?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically we need an algorithm that will change our weight and biases, in order
    to minimize our loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The Loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine the loss function here as a place with some mountains, and your
    objective is to find it's vale (lowest) place. Your only instrument is the gadget
    that returns your altitude (loss). You need to find out which direction to take.
  prefs: []
  type: TYPE_NORMAL
- en: '![](LossAlps.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we can also observe two things:'
  prefs: []
  type: TYPE_NORMAL
- en: You have more than one value (Local minima)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending where you landed you probably find one instead of the other (Importance
    of weight initialization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which direction to take (Gradient descent)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We can use calculus to discover which direction to take, for instance if we
    follow the derivative of the loss function, we can guarantee that we always go
    down. This is done just by subtracting the current set of weights by derivative
    of the loss function evaluated at that point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ddc9caec.png)'
  prefs: []
  type: TYPE_IMG
- en: On multiple dimensions we have a vector of partial derivatives, which we call
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that we multiply the gradient by a factor ![](c9b9fa39.png) (step-size,
    learning-rate), that is normally a small value ex: 0.001'
  prefs: []
  type: TYPE_NORMAL
- en: Simple Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To illustrate the gradient descent method let's follow a simple case in 1D.
    Consider the initial weight (-1.5)
  prefs: []
  type: TYPE_NORMAL
- en: '![](474e08d1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](gradDescentAnim.gif)'
  prefs: []
  type: TYPE_IMG
- en: On Matlab
  prefs: []
  type: TYPE_NORMAL
- en: '![](GradientDescentMatCode.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that after we use calculus do find the derivative of the loss
    function, we just needed to evaluate it with the current weight. After that we
    just take the evaluated value from the current weight.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate to big
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Using a big learning rate can accelerate convergence, but could also make the
    gradient descent oscillate and diverge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](BigGradientDescent.gif)'
  prefs: []
  type: TYPE_IMG
- en: Numerical Gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Slow method to evaluate the gradient, but we can use it to verify if our code
    is right
  prefs: []
  type: TYPE_NORMAL
- en: Mini Batch Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](loss.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Instead of running through all the training set to calculate the loss, then
    do gradient descent, we can do in a small (batch) portions. This leads to similar
    results and compute faster. Normally the min-batch size is dependent of the GPU
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: If you analyze your loss decay over time the full-batch version is less noisy
    but much more difficult to compute.
  prefs: []
  type: TYPE_NORMAL
- en: If your mini-batch size goes to 1, which means you compute the gradient descent
    for each sample. On this case we have the Stochastic Gradient Descent.
  prefs: []
  type: TYPE_NORMAL
- en: This is relatively less common to see because in practice due to vectorized
    code optimizations it can be computationally much more efficient to evaluate the
    gradient for 100 examples, than the gradient for one example 100 times.
  prefs: []
  type: TYPE_NORMAL
- en: Effects of learning rate on loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually what people do is to choose a high (but not so high) learning rate,
    then decay with time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](learningrates.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are 2 ways to decay the learning rate with time while training:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the learning rate (![](efff0667.png)) by 2 every x epochs
  prefs: []
  type: TYPE_NORMAL
- en: '![](c36c0e20.png), where t is time and k is the decay parameter'
  prefs: []
  type: TYPE_NORMAL
- en: '![](4480a80c.png), where t is the time and k the decay parameter'
  prefs: []
  type: TYPE_NORMAL
- en: Other algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The gradient descent is the simplest idea to do model optimization. There are
    a few other nice algorithms to try when thinking about model optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic Gradient descent with momentum (Very popular)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adagrad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam (Very good because you need to take less care about learning rate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rmsprop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](OtherOptimizers.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now what we need is a way to get the gradient vector of our model, next chapter
    we will talk about back-propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back-propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation is an algorithm that calculate the partial derivative of every
    node on your model (ex: Convnet, Neural network). Those partial derivatives are
    going to be used during the training phase of your model, where a loss function
    states how much far your are from the correct result. This error is propagated
    backward from the model output back to it''s first layers. The backpropagation
    is more easily implemented if you structure your model as a computational graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Circuit1.png)'
  prefs: []
  type: TYPE_IMG
- en: The most important thing to have in mind here is how to calculate the forward
    propagation of each block and it's gradient. Actually most of the deep learning
    libraries code is about implementing those gates forward/backward code.
  prefs: []
  type: TYPE_NORMAL
- en: Basic blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some examples of basic blocks are, add, multiply, exp, max. All we need to do
    is observe their forward and backward calculation
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](SumGate.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](MulGate.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](MaxGate.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](GradientBranches.png)'
  prefs: []
  type: TYPE_IMG
- en: Some other derivatives
  prefs: []
  type: TYPE_NORMAL
- en: '![](2c1d1490.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe that we output 2 gradients because we have 2 inputs... Also observe
    that we need to save (cache) on memory the previous inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Chain Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you have an output y, that is function of g, which is function
    of f, which is function of x. If you want to know how much g will change with
    a small change on dx (dg/dx), we use the chain rule. Chain rule is a formula for
    computing the derivative of the composition of two or more functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ChainRule1.png)'
  prefs: []
  type: TYPE_IMG
- en: The chain rule is the work horse of back-propagation, so it's important to understand
    it now. On the picture bellow we get a node f(x,y) that compute some function
    with two inputs x,y and output z. Now on the right side, we have this same node
    receiving from somewhere (loss function) a gradient dL/dz which means. "How much
    L will change with a small change on z". As the node has 2 inputs it will have
    2 gradients. One showing how L will a small change dx and the other showing how
    L will change with a small change (dz)
  prefs: []
  type: TYPE_NORMAL
- en: '![](chainrule_example.PNG) In order to calculate the gradients we need the
    input dL/dz (dout), and the derivative of the function f(x,y), at that particular
    input, then we just multiply them. Also we need the previous cached input, saved
    during forward propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: Gates Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Observe bellow the implementation of the multiply and add gate on python
  prefs: []
  type: TYPE_NORMAL
- en: '![](PythonCircuits.png)'
  prefs: []
  type: TYPE_IMG
- en: Step by step example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: With what we learn so far, let's calculate the partial derivatives of some graphs
  prefs: []
  type: TYPE_NORMAL
- en: Simple example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we have a graph for the function ![](9e90bd2.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleGraph.png)'
  prefs: []
  type: TYPE_IMG
- en: Start from output node f, and consider that the gradient of f related to some
    criteria is 1 (dout)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dq=(dout(1) * z), which is -4 (How the output will change with a change in q)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dz=(dout(1) * q), which is 3 (How the output will change with a change in z)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sum gate distribute it's input gradients, so dx=-4, dy=-4 (How the output
    will change with x,z)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perceptron with 2 inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This following graph represent the forward propagation of a simple 2 inputs,
    neural network with one output layer with sigmoid activation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](6fcb84f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](SimplePerceptron.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](StepByStepExample.png)'
  prefs: []
  type: TYPE_IMG
- en: Start from the output node, considering that or error(dout) is 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradient of the input of the 1/x will be -1/(1.37^2), -0.53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The increment node does not change the gradient on it's input, so it will be
    (-0.53 * 1), -0.53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The exp node input gradient will be (exp(-1(cached input)) * -0.53), -0.2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The negative gain node will be it's input gradient (-1 * -0.2), 0.2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sum node will distribute the gradients, so, dw2=0.2, and the sum node also
    0.2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sum node again distribute the gradients so again 0.2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dw0 will be (0.2 * -1), -0.2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dx0 will be (0.2 * 2). 0.4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next chapter we will learn about Feature Scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the life of gradient descent algorithms easier, there are some
    techniques that can be applied to your data on training/test phase. If the features
    ![](bddfd27d.png) on your input vector ![](761426bc.png), are out of scale your
    loss space ![](42a195a9.png) will be somehow stretched. This will make the gradient
    descent convergence harder, or at least slower.
  prefs: []
  type: TYPE_NORMAL
- en: On the example bellow your input X has 2 features (house size, and number of
    bedrooms). The problem is that house size feature range from 0...2000, while number
    of bedrooms range from 0...5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FeatureScaling.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Centralize data and normalize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](PreProcessing.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Bellow we will pre-process our input data to fix the following problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Data not centered around zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features out of scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider your input data ![](e84a970b.png), where N is the number of samples
    on your input data (batch size) and D the dimensions (On the previous example
    D is 2, size house, num bedrooms).
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to do is to subtract the mean value of the input data, this
    will centralize the data dispersion around zero
  prefs: []
  type: TYPE_NORMAL
- en: '![](b53e5544.png)'
  prefs: []
  type: TYPE_IMG
- en: On prediction phase is common to store this mean value to be subtracted from
    a test example. On the case of image classification, it's also common to store
    a mean image created from a batch of images on the training-set, or the mean value
    from every channel.
  prefs: []
  type: TYPE_NORMAL
- en: After your data is centralized around zero, you can make all features have the
    same range by dividing X by it's standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](8008dab6.png)'
  prefs: []
  type: TYPE_IMG
- en: Again this operation fix our first, problem, because all features will range
    similarly. But this should be used if somehow you know that your features have
    the same "weight". On the case of image classification for example all pixels
    have the same range (0..255) and a pixel alone has no bigger meaning (weight)
    than the other, so just mean subtraction should suffice for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common mistake:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: An important point to make about the preprocessing is that any preprocessing
    statistics (e.g. the data mean) must only be computed on the training data, and
    then applied to the validation / test data. Computing the mean and subtracting
    it from every image across the entire dataset and then splitting the data into
    train/val/test splits would be a mistake. Instead, the mean must be computed only
    over the training data and then subtracted equally from all splits (train/val/test).
  prefs: []
  type: TYPE_NORMAL
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next chapter we will learn about Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Model Initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: One important topic we should learn before we start training our models, is
    about the weight initialization. Bad weight initialization, can lead to a "never
    convergence training" or a slow training.
  prefs: []
  type: TYPE_NORMAL
- en: Weight matrix format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'As observed on previous chapters, the weight matrix has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](9e63ed7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the number of outputs ![](64649e57.png), as rows and the number of
    inputs ![](5f0028c.png) as columns. You could also consider another format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](109a7901.png)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](64649e57.png) as columns and ![](5f0028c.png) as rows.
  prefs: []
  type: TYPE_NORMAL
- en: The whole point is that our weights is going to be a 2d matrix function of ![](5f0028c.png)
    and ![](64649e57.png)
  prefs: []
  type: TYPE_NORMAL
- en: Initialize all to zero
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you initialize your weights to zero, your gradient descent will never converge
  prefs: []
  type: TYPE_NORMAL
- en: '![](all_zeros_initialization.png)'
  prefs: []
  type: TYPE_IMG
- en: Initialize with small values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'A better idea is to initialize your weights with values close to zero (but
    not zero), ie: 0.01'
  prefs: []
  type: TYPE_NORMAL
- en: '![](9979e35b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here randn gives random data with zero mean, unit standard deviation. ![](20895e70.png)
    are the number of input and outputs. The 0.01 term will keep the random weights
    small and close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](matlab_randn.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem with the previous way to do initialization is that the variance
    of the outputs will grow with the number of inputs. To solve this issue we can
    divide the random term by the square root of the number of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](fc90afec.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](xavier_inits.png)'
  prefs: []
  type: TYPE_IMG
- en: Now it seems that we don't have dead neurons, the only problem with this approach
    is to use it with Relu neurons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](xavier_inits_relu.png)'
  prefs: []
  type: TYPE_IMG
- en: To solve this just add a simple (divide by 2) term....
  prefs: []
  type: TYPE_NORMAL
- en: '![](20772a44.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](xavier_inits_relu_fix.png)'
  prefs: []
  type: TYPE_IMG
- en: So use this second form to initialize Relu layers.
  prefs: []
  type: TYPE_NORMAL
- en: Bath Norm layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On future chapters we're going to learn a technique that make your model more
    resilient to specific initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we start talk about convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On previous forward neural networks, our output was a function between the current
    input and a set of weights. On recurrent neural networks(RNN), the previous network
    state is also influence the output, so recurrent neural networks also have a "notion
    of time". This effect by a loop on the layer output to it's input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](recurrent.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the RNN will be a function with inputs ![](2683a688.png) (input
    vector) and previous state ![](93d89ee3.png). The new state will be ![](22b53607.png).
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent function, ![](d9cebfcd.png), will be fixed after training and
    used to every time step.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks are the best model for regression, because it take
    into account past values.
  prefs: []
  type: TYPE_NORMAL
- en: RNN are computation "Turing Machines" which means, with the correct set of weights
    it can compute anything, imagine this weights as a program.
  prefs: []
  type: TYPE_NORMAL
- en: Just to not let you too overconfident on RNN, there is no automatic back-propagation
    algorithms, that will find this "perfect set of weights".
  prefs: []
  type: TYPE_NORMAL
- en: 'Bellow we have a simple implementation of RNN recurrent function: (Vanilla
    version)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](13091812.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code that calculate up to the next state ![](22b53607.png) looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Observe that in our case of RNN we are now more interested on the next state,
    ![](22b53607.png) not exactly the output, ![](4d21efaf.png)
  prefs: []
  type: TYPE_NORMAL
- en: Before we start let's just make explicit how to backpropagate the tanh block.
  prefs: []
  type: TYPE_NORMAL
- en: '![](tanhBlock.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can do the backpropagation step (For one single time-step)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A point to be noted is that the same function ![](4c48cbdd.png) and the same
    set of parameters will be applied to every time-step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RecurrentNeuralNetwork.png)'
  prefs: []
  type: TYPE_IMG
- en: A good initialization for the RNN states ![](22b53607.png) is zero. Again this
    is just the initial RNN state not it's weights.
  prefs: []
  type: TYPE_NORMAL
- en: These looping feature on RNNs can be confusing first but actually you can think
    as a normal neural network repeated(unrolled) multiple times. The number of times
    that you unroll can be consider how far in the past the network will remember.
    In other words each time is a time-step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RNN_Unrolling.png)'
  prefs: []
  type: TYPE_IMG
- en: Forward and backward propagation on each time-step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the previous examples we presented code for forward and backpropagation
    for one time-step only. As presented before the RNN are unroled for each time-step
    (finite). Now we present how to do the forward propagation for each time-step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Bellow we show a diagram that present the multiple ways that you could use a
    recurrent neural network compared to the forward networks. Consider the inputs
    the red blocks, and the outputs the blue blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Recurrent_Forward.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'One to one: Normal Forward network, ie: Image on the input, label on the output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One to many(RNN): (Image captioning) Image in, words describing the scene out
    (CNN regions detected + RNN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many to one(RNN): (Sentiment Analysis) Words on a phrase on the input, sentiment
    on the output (Good/Bad) product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many to many(RNN): (Translation), Words on English phrase on input, Portuguese
    on output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many to many(RNN): (Video Classification) Video in, description of video on
    output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we describe how we add "depth" to RNN and also how to unroll RNNs to
    deal with time.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the output of the RNNs are feed to deeper layers, while the state
    is feed for dealing with past states.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RNN_Stacking.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple regression example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we present a simple case where we want the RNN to complete the word, we
    give to the network the characters h,e,l,l , our vocabulary here is [h,e,l,o].
    Observe that after we input the first 'h' the network want's to output the wrong
    answer (right is on green), but near the end, after the second 'l' it want's to
    output the right answer 'o'. Here the order that the characters come in does matter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RNNSampleWithDict.png)'
  prefs: []
  type: TYPE_IMG
- en: Describing images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you connect a convolution neural network, with pre-trained RNN. The RNN will
    be able to describe what it "see" on the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](CNN_RNN.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Basically we get a pre-trained CNN (ie: VGG) and connect the second-to-last
    FC layer and connect to a RNN. After this you train the whole thing end-to-end.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](CNN_RNN_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Long Short Term Memory networks(LSTM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM provides a different recurrent formula ![](d9cebfcd.png), it's more powefull
    thanvanilla RNN, due to it's complex ![](d9cebfcd.png) that add "residual information"
    to the next state instead of just transforming each state. Imagine LSTM are the
    "residual" version of RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In other words LSTM suffer much less from vanishing gradients than normal RNNs.
    Remember that the plus gates distribute the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: So by suffering less from vanishing gradients, the LSTMs can remember much more
    in the past. So from now just use LSTMs when you think about RNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_RNN.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe from the animation bellow how hast the gradients on the RNN disappear
    compared to LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](rnn_LSTM_gradient.gif)'
  prefs: []
  type: TYPE_IMG
- en: The vanishing problem can be solved with LSTM, but another problem that can
    happen with all recurrent neural network is the exploding gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: To fix the exploding gradient problem, people normally do a gradient clipping,
    that will allow only a maximum gradient value.
  prefs: []
  type: TYPE_NORMAL
- en: This highway for the gradients is called Cell-State, so one difference compared
    to the RNN that has only the state flowing, on LSTM we have states and the cell
    state.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Flow_LSTM_RNN.png)'
  prefs: []
  type: TYPE_IMG
- en: LSTM Gate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Doing a zoom on the LSTM gate. This also improves how to do the backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](LSTMBlockDiagram.png)'
  prefs: []
  type: TYPE_IMG
- en: Code for lstm forward propagation for one time-step
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now the backward propagation for one time-step
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](DLPic.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning is a branch of machine learning based on a set of algorithms that
    learn to represent the data. Bellow we list the most popular ones.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Belief Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Auto-Encoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (LSTM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the promises of deep learning is that they will substitute hand-crafted
    feature extraction. The idea is that they will "learn" the best features needed
    to represent the given data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeepLearning.png)'
  prefs: []
  type: TYPE_IMG
- en: Layers and layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models are formed by multiple layers. On the context of artificial
    neural networks the multi layer perceptron (MLP) with more than 2 hidden layers
    is already a Deep Model.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb deeper models will perform better than shallow models, the
    problem is that more deep you go more data, you will need to avoid over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeepLearningModel.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we list some of the most used layers
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pooling Layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout Layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch normalization layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fully Connected layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Relu, Tanh, sigmoid layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Softmax, Cross Entropy, SVM, Euclidean (Loss layers)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid over-fitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides getting more data, there are some techniques used to combat over-fitting,
    here is a list of the most common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's a technique that randomly turns off some neurons from the fully connected
    layer during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Dropout.png)'
  prefs: []
  type: TYPE_IMG
- en: The dropout forces the fully connected layers to learn the same concept in different
    ways
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dasdaasdasadasdad
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Synthetically create new training examples by applying some transformations
    on the input data. For examples fliping images. During Imagenet Competition, Alex
    Krizhevesky (Alexnet) used data augmentation of a factor of 2048, where each class
    on imagenet has 1000 elements.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Augmentation1.png)![](Augmentation2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Augmentation3.png)'
  prefs: []
  type: TYPE_IMG
- en: Automatic Hierarchical representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea is to let the learning algorithm to find the best representation that
    it can for every layer starting from the inputs to the more deepest ones.
  prefs: []
  type: TYPE_NORMAL
- en: The shallow layers learn to represent data on it's simpler form and deepest
    layers learn to represent the data with the concepts learned from the previous
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](HieararchicalRepresentation.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](DBN_Faces.png)'
  prefs: []
  type: TYPE_IMG
- en: Old vs New
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Actually the only new thing is the usage of something that will learn how to
    represent the data (feature selection) automatically and based on the dataset
    given. Is not about saying that SVM or decision trees are bad, actually some people
    use SVMs at the end of the deep neural network to do classificaion.
  prefs: []
  type: TYPE_NORMAL
- en: The only point is that the feature selection can be easily adapted to new data.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest advantage on this is that if your problem get more complex you just
    make your model "deeper" and get more data (a lot) to train to your new problem.
  prefs: []
  type: TYPE_NORMAL
- en: Some guys from Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](SomeGuysMachineLearning.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next chapter we will learn about Convolution Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution is a mathematical operation that does the integral of the product
    of 2 functions(signals), with one of the signals flipped. For example bellow we
    convolve 2 signals f(t) and g(t).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1.png)'
  prefs: []
  type: TYPE_IMG
- en: So the first thing to do is to flip (180 degrees) the signal g, then slide the
    flipped g over f, multiplying and accumulating all it's values.
  prefs: []
  type: TYPE_NORMAL
- en: The order that you convolve the signals does not matter for the end result,
    so conv(a,b)==conv(b,a)
  prefs: []
  type: TYPE_NORMAL
- en: On this case consider that the blue signal ![](6003a9f8.png) is our input signal
    and ![](e5429842.png) our kernel, the term kernel is used when you use convolutions
    to filter signals.
  prefs: []
  type: TYPE_NORMAL
- en: Output signal size 1D
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On the case of 1d convolution the output size is calculated like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](f8e75e8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Application of convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People use convolution on signal processing for the following use cases
  prefs: []
  type: TYPE_NORMAL
- en: Filter Signals (1d audio, 2d image processing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check how much a signal is correlated to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find patterns on signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple example in matlab and python(numpy)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bellow we convolve to signals x = (0,1,2,3,4) with w = (1,-1,2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleConv.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Convolve1d_Python.png)'
  prefs: []
  type: TYPE_IMG
- en: Doing by hand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand better then concept of convolution let's do the example above
    by hand. Basically we're going to convolve 2 signals (x,w). The first thing is
    to flip W horizontally (Or rotate to left 180 degrees)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1_ManualStart.png)'
  prefs: []
  type: TYPE_IMG
- en: After that we need to slide the flipped W over the input X
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Manual.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe that on steps 3,4,5 the flipped window is completely inside the input
    signal. Those results are called 'valid'. The cases where the flipped window is
    not fully inside the input window(X), we can consider to be zero, or calculate
    what is possible to be calculated, ex on step 1 we multiply 1 by zero, and the
    rest is simply ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Input padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to keep the convolution result size the same size as the input, and
    to avoid an effect called circular convolution, we pad the signal with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where you put the zeros depends on what you want to do, ie: on the 1d case
    you can concatenate them on the end, but on 2d is normally around the original
    signal'
  prefs: []
  type: TYPE_NORMAL
- en: '![](zeroPadding_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](zeroPadding.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On matlab you can use the command padarray to pad the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Transforming convolution to computation graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to calculate partial derivatives of every node inputs and parameters,
    it's easier to transform it to a computational graph. Here I'm going to transform
    the previous 1d convolution, but this can be extended to 2d convolution as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Manual_symbolic.png)'
  prefs: []
  type: TYPE_IMG
- en: Here our graph will be created on the valid cases where the flipped kernel(weights)
    will be fully inserted on our input window.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Simple_1d_Conv_graph.png)'
  prefs: []
  type: TYPE_IMG
- en: We're going to use this graph in the future to infer the gradients of the inputs
    (x) and weights (w) of the convolution layer
  prefs: []
  type: TYPE_NORMAL
- en: 2d Convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Extending to the second dimension, 2d convolutions are used on image filters,
    and when you would like to find a specific patch on image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv2dUsage1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](PatternMatch.png)'
  prefs: []
  type: TYPE_IMG
- en: Matlab and python examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](MatlabConv2_Example.png)'
  prefs: []
  type: TYPE_IMG
- en: Doing by hand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First we should flip the kernel, then slide the kernel on the input signal.
  prefs: []
  type: TYPE_NORMAL
- en: Before doing this operation by hand check out the animation showing how this
    sliding works
  prefs: []
  type: TYPE_NORMAL
- en: '![](Convolution_schematic.gif)'
  prefs: []
  type: TYPE_IMG
- en: '![](Conv2d_Manual.png)'
  prefs: []
  type: TYPE_IMG
- en: Stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default when we're doing convolution we move our window one pixel at a time
    (stride=1), but some times in convolutional neural networks we move more than
    one pixel. Strides of 2 are used on pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that bellow the red window is moving much more than one pixel time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Pooling_schematic.gif)'
  prefs: []
  type: TYPE_IMG
- en: Output size for 2d
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we consider the padding and stride, the output size of convolution is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](200d3e57.png)'
  prefs: []
  type: TYPE_IMG
- en: F is the size of the kernel, normally we use square kernels, so F is both the
    width and height of the kernel
  prefs: []
  type: TYPE_NORMAL
- en: Implementing convolution operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The example bellow will convolve a 5x5x3 (WxHx3) input, with a conv layer with
    the following parameters Stride=2, Pad=1, F=3(3x3 kernel), and K=2 (two filters).
    Our input has 3 channels, so we need a 3x3x3 kernel weight. We have 2 filters
    (K=2) so we have 2 output activation (3x3x2). Calculating the output size we have:
    (5 - 3 + 2)/2 + 1 = 3'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Example.png)'
  prefs: []
  type: TYPE_IMG
- en: So basically we need to calculate 2 convolutions, one for each 3x3x3 filter
    (w0,w1), and remembering to add the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Vanilla_1.png)'
  prefs: []
  type: TYPE_IMG
- en: The code bellow (vanilla version) cannot be used on real life, because it will
    be slow. Usually deep learning libraries do the convolution as a matrix multiplication,
    using im2col/col2im.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next chapter we will learn about Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](ConvnetDiagram.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Conv_Relu_pool_FC.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A CNN is composed of layers that filters(convolve) the inputs to get usefull
    information. These convolutional layers have parameters(kernel) that are learned
    so that these filters are adjusted automatically to extract the most useful information
    for the task at hand without feature selection. CNN are better to work with images.
    Normal Neural networks does not fit well on image classification problems
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of Normal Neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On normal neural networks, we need to convert the image to a single 1d vector
    ![](49e0430d.png),then send this data to a hidden layer which is fully connected.
    On this scenario each neuron will have ![](51d4cd80.png) parameters per neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '![](WeightSharing.png)'
  prefs: []
  type: TYPE_IMG
- en: Common architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Normally the pattern [CONV->ReLU->Pool->CONV->ReLU->Pool->FC->Softmax_loss(during
    train)] is quite commom.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Common_CNN_Arch.png)'
  prefs: []
  type: TYPE_IMG
- en: Main actor the convolution layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most important operation on the convolutional neural network are the convolution
    layers, imagine a 32x32x3 image if we convolve this image with a 5x5x3 (The filter
    depth must have the same depth as the input), the result will be an activation
    map 28x28x1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ezgif.com-optimize.gif)'
  prefs: []
  type: TYPE_IMG
- en: The filter will look for a particular thing on all the image, this means that
    it will look for a pattern in the whole image with just one filter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv11.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Now consider that we want our convolution layer to look for 6 different things.
    On this case our convolution layer will have 6 5x5x3 filters. Each one looking
    for a particular pattern on the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv2.PNG)'
  prefs: []
  type: TYPE_IMG
- en: By the way the convolution by itself is a linear operation, if we don't want
    to suffer from the same problem of the linear classifers we need to add at the
    end of the convolution layer a non-linear layer. (Normally a Relu)
  prefs: []
  type: TYPE_NORMAL
- en: Another important point of using convolution as pattern match is that the position
    where the thing that we want to search on the image is irrelevant. On the case
    of neural networks the model/hypothesis will learn an object on the exact location
    where the object is located during training.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layer Hyper parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Those are the parameters that are used to configure a convolution layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel size(K): Small is better (But if is on the first layer, takes a lot
    of memory)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stride(S): How many pixels the kernel window will slide (Normally 1, in conv
    layers, and 2 on pooling layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero Padding(pad): Put zeros on the image border to allow the conv output size
    be the same as the input size (F=1, PAD=0; F=3, PAD=1; F=5, PAD=2; F=7, PAD=3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of filters(F): Number of patterns that the conv layer will look for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default the convolution output will always have a result smaller than the
    input. To avoid this behaviour we need to use padding. To calculate the convolution
    output (activation map) size we need this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](36938e90.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](b60caaa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Vizualizing convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will see some examples of the convolution window sliding on the input
    image and change some of it's hyper parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution with no padding and stride of 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we have a input 4x4 convolved with a filter 3x3 (K=3) with stride (S=1)
    and padding (pad=0)
  prefs: []
  type: TYPE_NORMAL
- en: '![](no_padding_no_strides.gif)'
  prefs: []
  type: TYPE_IMG
- en: Convolution with padding and stride of 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we have an input 5x5 convolved with a filter 3x3 (k=3) with stride (S=1)
    and padding (pad=1). On some libraries there is a feature that always calculate
    the right amount of padding to keep the output spatial dimensions the "same" as
    the input dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](same_padding_no_strides.gif)'
  prefs: []
  type: TYPE_IMG
- en: Number of parameters(weights)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we show how to calculate the number of parameters used by one convolution
    layer. We will illustrate with a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: 32x32x3, 32x32 RGB image'
  prefs: []
  type: TYPE_NORMAL
- en: 'CONV: Kernel(F):5x5, Stride:1, Pad:2, numFilters:10'
  prefs: []
  type: TYPE_NORMAL
- en: '![](edc7a412.png)'
  prefs: []
  type: TYPE_IMG
- en: You can omit the "+1" parameter (Bias), to simplify calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Amount of memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we show how to calculate the amount of memory needed on the convolution
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: 32x32x3, 32x32 RGB image'
  prefs: []
  type: TYPE_NORMAL
- en: 'CONV: Kernel(F):5x5, Stride:1, Pad:2, numFilters:10, as we use padding our
    output volume will be 32x32x10, so the ammount of memory in bytes is: 10240 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: So the amount of memory is basically just the product of the dimensions of the
    output volume which is a 4d tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](41e9054c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](e768559f.png): Output batch size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](48a9667f.png): The outpt volume or on the case of convolution the number
    of filters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](fdb9624c.png): The height of the output activation map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](6b7ff93.png): The width of the output activation map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1x1 Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This type if convolution is normally used to adapt depths, by merging them,
    without changing the spatial information.
  prefs: []
  type: TYPE_NORMAL
- en: Substituting Big convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we explain what is the effect of cascading several small convolutions,
    on the diagram bellow we have 2 3x3 convolution layers. If you start from the
    second layer on the right, one neuron on the second layer, has a 3x3 receptive
    field, and each neuron on the first layer create a 5x5 receptive field on the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: So in simpler words cascading can be used to represent bigger ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](CascadingConvolutions.png)'
  prefs: []
  type: TYPE_IMG
- en: The new trend on new successful models is to use smaller convolutions, for example
    a 7x7 convolution can be substituted with 3 3x3 convolutions with the same depth.
    This substitution cannot be done on the first conv layer due to the depth mismatch
    between the first conv layer and the input file depth (Unless if your first layer
    has only 3 filters).
  prefs: []
  type: TYPE_NORMAL
- en: '![](ConvLayer.png)'
  prefs: []
  type: TYPE_IMG
- en: On the diagram above we substitute one 7x7 convolution by 3 3x3 convolutions,
    observe that between them we have relu layers, so we have more non-linearities.
    Also we have less weights and multiply-add operations so it will be faster to
    compute.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the substitution of a 7x7 convolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine a 7x7 convolution, with C filters, being used on a input volume WxHxC
    we can calculate the number of weights as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](d1009d9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now if we use 3 3x3 convolutions with C filters, we would have
  prefs: []
  type: TYPE_NORMAL
- en: '![](f12a1879.png)'
  prefs: []
  type: TYPE_IMG
- en: We still have less parameters, as we need to use Relu between the layers to
    break the linearity (otherwise the conv layers in cascade will appear as a single
    3x3 layer) we have more non-linearity, less parameters, and more performance.
  prefs: []
  type: TYPE_NORMAL
- en: Substitution on the first layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, we cannot substitute large convolutions on the first layer.
    Actually small convolutions on the first layer cause a memory consume explosion.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the problem let's compare the first layer of a convolution neural
    network as been 3x3 with 64 filters and stride of 1 and the same depth with 7x7
    and stride of 2, consider the image size to be 256x256x3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](6843150f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'TODO: How the stride and convolution size affect the memory consumption'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing on 3x3 substitution (Bottleneck)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: It's also possible to simplify the 3x3 convolution with a mechanism called bottleneck.
    This again will have the same representation of a normal 3x3 convolution but with
    less parameters, and more non-linearities.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the substitution is made on the 3x3 convolution that has the same
    depth as the previous layer (On this case 50x50x64)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Bottleneck.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we calculate how much parameters we use on the bottleneck, remember that
    on 3x3 is ![](3251489b.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Botleneck_Calc.PNG)'
  prefs: []
  type: TYPE_IMG
- en: So the bottleneck uses ![](b94f9821.png), which is less.
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck is also used on microsoft residual network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ResnetBottleneck.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another option to break 3x3xC convolutions is to use 1x3xC, then 3x1xC, this
    has been used on residual googlenet inception layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Subs_3x3.PNG)'
  prefs: []
  type: TYPE_IMG
- en: '![](NewInception.PNG)'
  prefs: []
  type: TYPE_IMG
- en: FC -> Conv Layer Conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: It's possible to convert Fully connected layers to convolution layers and vice-versa,
    but we are more interest on the FC->Conv conversion. This is done to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example imagine a FC layer with output K=4096 and input 7x7x512, the conversion
    would be:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CONV: Kernel:7x7, Pad:0, Stride:1, numFilters:4096.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the 2d convolution formula size: ![](26d0dd.png), which will be 1x1x4096.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In resume what you gain by converting the FC layers to convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: It''s faster to compute due to the weight sharing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use images larger than the ones that you trained, without changing nothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will be able to detect 2 objects on the same image (If you use a bigger
    image) your final output will be bigger then a single row vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](MultipleCharacters.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next chapter we will learn about Fully Connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fully Connected Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain how to implement in matlab and python the fully connected
    layer, including the forward and back-propagation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FullyConnectedLayer.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First consider the fully connected layer as a black box with the following
    properties: On the forward propagation'
  prefs: []
  type: TYPE_NORMAL
- en: Has 3 inputs (Input signal, Weights, Bias)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has 1 output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the back propagation
  prefs: []
  type: TYPE_NORMAL
- en: Has 1 input (dout) which has the same size as output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has 3 (dx,dw,db) outputs, that has the same size as the inputs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural network point of view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](FC_Layer_NN.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just by looking the diagram we can infer the outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](ce09aba1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now vectorizing (put on matrix form): (Observe 2 possible versions)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](9e63ed7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the format that you choose to represent W attention to this because
    it can be confusing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example if we choose X to be a column vector, our matrix multiplication
    must be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](109a7901.png)'
  prefs: []
  type: TYPE_IMG
- en: Computation graph point of view
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In order to discover how each input influence the output (backpropagation) is
    better to represent the algorithm as a computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Graph_Fully_Connected_Layer.png)'
  prefs: []
  type: TYPE_IMG
- en: Now for the backpropagation let's focus in one of the graphs, and apply what
    we learned so far on backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Graph_Fully_Connected_Layer_Backprop.png)'
  prefs: []
  type: TYPE_IMG
- en: Summarizing the calculation for the first output (y1), consider a global error
    L(loss) and ![](6b7d96f4.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](a0d260d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](3f51c64e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](34f8377a.png)'
  prefs: []
  type: TYPE_IMG
- en: Also extending to the second output (y2)
  prefs: []
  type: TYPE_NORMAL
- en: '![](c78583bb.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](468a2295.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](51bc5114.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Merging the results, for dx:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](d269dc97.png)'
  prefs: []
  type: TYPE_IMG
- en: On the matrix form
  prefs: []
  type: TYPE_NORMAL
- en: '![](7f5d0677.png), or ![](3fe949c9.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the format that you choose to represent X (as a row or column vector),
    attention to this because it can be confusing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for dW It''s important to not that every gradient has the same dimension
    as it''s original value, for instance dW has the same dimension as W, in other
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](db7dc2ed.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](fca2e763.png)'
  prefs: []
  type: TYPE_IMG
- en: And dB ![](c88de6e2.png)
  prefs: []
  type: TYPE_NORMAL
- en: Expanding for bigger batches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: All the examples so far, deal with single elements on the input, but normally
    we deal with much more than one example at a time. For instance on GPUs is common
    to have batches of 256 images at the same time. The trick is to represent the
    input signal as a 2d matrix [NxD] where N is the batch size and D the dimensions
    of the input signal. So if you consider the CIFAR dataset where each digit is
    a 28x28x1 (grayscale) image D will be 784, so if we have 10 digits on the same
    batch our input will be [10x784].
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of argument, let''s consider our previous samples where the vector
    X was represented like ![](8a28794f.png), if we want to have a batch of 4 elements
    we will have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](4ffc17e6.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case W must be represented in a way that support this matrix multiplication,
    so depending how it was created it may need to be transposed
  prefs: []
  type: TYPE_NORMAL
- en: '![](eaba624a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Continuing the forward propagation will be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](3c17fe53.png)'
  prefs: []
  type: TYPE_IMG
- en: One point to observe here is that the bias has repeated 4 times to accommodate
    the product X.W that in this case will generate a matrix [4x2]. On matlab the
    command "repmat" does the job. On python it does automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatlabRepmat.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Using Symbolic engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping to implementation is good to verify the operations on Matlab
    or Python (sympy) symbolic engine. This will help visualize and explore the results
    before acutally coding the functions.
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic forward propagation on Matlab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here after we defined the variables which will be symbolic, we create the matrix
    W,X,b then calculate ![](c5476bc3.png), compare the final result with what we
    calculated before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](SymbForwardMatlab.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Symbolic backward propagation on Matlab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we also confirm the backward propagation formulas. Observe the function
    "latex" that convert an expression to latex on matlab
  prefs: []
  type: TYPE_NORMAL
- en: '![](SymbBackwardMatlab.PNG) Here I''ve just copy and paste the latex result
    of dW or " ![](a06dcfa1.png) " from matlab'
  prefs: []
  type: TYPE_NORMAL
- en: '![](d485c17d.png)'
  prefs: []
  type: TYPE_IMG
- en: Input Tensor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our library will be handling images, and most of the time we will be handling
    matrix operations on hundreds of images at the same time. So we must find a way
    to represent them, here we will represent batch of images as a 4d tensor, or an
    array of 3d matrices. Bellow we have a batch of 4 rgb images (width:160, height:120).
    We're going to load them on matlab/python and organize them one a 4d matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](ImgsBatch.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe that in matlab the image becomes a matrix 120x160x3\. Our tensor will
    be 120x160x3x4 ![](MatlabLoadImages.png)
  prefs: []
  type: TYPE_NORMAL
- en: On Python before we store the image on the tensor we do a transpose to convert
    out image 120x160x3 to 3x120x160, then to store on a tensor 4x3x120x160
  prefs: []
  type: TYPE_NORMAL
- en: '![](PythonLoadImages.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_FC_Forward.png)'
  prefs: []
  type: TYPE_IMG
- en: Backward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_FC_Backward.png)'
  prefs: []
  type: TYPE_IMG
- en: Matlab Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: One special point to pay attention is the way that matlab represent high-dimension
    arrays in contrast with matlab. Also another point that may cause confusion is
    the fact that matlab represent data on col-major order and numpy on row-major
    order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Row_Col_Major.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multidimensional arrays in python and matlab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](MultiMatlabArray.png)'
  prefs: []
  type: TYPE_IMG
- en: One difference on how matlab and python represent multidimensional arrays must
    be noticed. We want to create a 4 channel matrix 2x3\. So in matlab you need to
    create a array (2,3,4) and on python it need to be (4,2,3)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_MultiDim.png)'
  prefs: []
  type: TYPE_IMG
- en: Matlab Reshape order
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before matlab will run the command reshape one column at a time,
    so if you want to change this behavior you need to transpose first the input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Reshape.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are dealing with more than 2 dimensions you need to use the "permute"
    command to transpose. Now on Python the default of the reshape command is one
    row at a time, or if you want you can also change the order (This options does
    not exist in matlab)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_Reshape.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bellow we have a reshape on the row-major order as a new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_reshape_row.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The other option would be to avoid this permutation reshape is to have the
    weight matrix on a different order and calculate the forward propagation like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](20ffcef4.png)'
  prefs: []
  type: TYPE_IMG
- en: With x as a column vector and the weights organized row-wise, on the example
    that is presented we keep using the same order as the python example.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Forward.png)'
  prefs: []
  type: TYPE_IMG
- en: Backward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Backward.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Relu layers
  prefs: []
  type: TYPE_NORMAL
- en: Relu Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rectified-Linear unit Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We will start this chapter explaining how to implement in Python/Matlab the
    ReLU layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ReluLayer.png)'
  prefs: []
  type: TYPE_IMG
- en: In simple words, the ReLU layer will apply the function ![](13304fde.png) in
    all elements on a input tensor, without changing it's spatial or depth information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ReluMatlabSimpleExample.png) ![](Relu.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the picture above, observe that all positive elements remain unchanged
    while the negatives become zero. Also the spatial information and depth are the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thinking about neural networks, it''s just a new type of Activation function,
    but with the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to compute (forward/backward propagation)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suffer much less from vanishing gradient on deep models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A bad point is that they can irreversibly die if you use a big learning rate
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Change all negative elements to zero while retaining the value of the positive
    elements. No spatial/depth information is changed.
  prefs: []
  type: TYPE_NORMAL
- en: Python forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_forward_relu.png)'
  prefs: []
  type: TYPE_IMG
- en: Matlab forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically we're just applying the max(0,x) function to every ![](bbe484e5.png)
    input element. From the back-propagation chapter we can notice that the gradient
    dx will be zero if the element ![](aa4b6abf.png)is negative or ![](8166a298.png)
    if the element is positive.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ReluGraph.png)'
  prefs: []
  type: TYPE_IMG
- en: Python backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_ReLU_Backward_Propagation.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Dropout layers
  prefs: []
  type: TYPE_NORMAL
- en: Dropout Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is a technique used to improve over-fit on neural networks, you should
    use Dropout along with other techniques like L2 Regularization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DropoutLayers.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellow we have a classification error (Not including loss), observe that the
    test/validation error is smaller using dropout
  prefs: []
  type: TYPE_NORMAL
- en: '![](With_Without_Dropout.png)'
  prefs: []
  type: TYPE_IMG
- en: As other regularization techniques the use of dropout also make the training
    loss error a little worse. But that's the idea, basically we want to trade training
    performance for more generalization. Remember that's more capacity you add on
    your model (More layers, or more neurons) more prone to over-fit it becomes.
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we have a plot showing both training, and validation loss with and without
    dropout
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout_loss_val_train.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically during training half of neurons on a particular layer will be deactivated.
    This improve generalization because force your layer to learn with different neurons
    the same "concept".
  prefs: []
  type: TYPE_NORMAL
- en: During the prediction phase the dropout is deactivated.
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where to use Dropout layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Normally some deep learning models use Dropout on the fully connected layers,
    but is also possible to use dropout after the max-pooling layers, creating some
    kind of image noise augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement this neuron deactivation, we create a mask(zeros and ones)
    during forward propagation. This mask is applied to the layer outputs during training
    and cached for future use on back-propagation. As explained before this dropout
    mask is used only during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the backward propagation we''re interested on the neurons that was activated
    (we need to save mask from forward propagation). Now with those neurons selected
    we just back-propagate dout. The dropout layer has no learnable parameters, just
    it''s input (X). During back-propagation we just return "dx". In other words:
    ![](a6e8a0fe.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Python Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout_forward_python_code.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout_backward_python_code.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Convolution layer
  prefs: []
  type: TYPE_NORMAL
- en: Convolution Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain how to implement the convolution layer on python and
    matlab.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ConvolutionLayer.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In simple terms the convolution layer, will apply the convolution operator
    on all images on the input tensor, and also transform the input depth to match
    the number of filters. Bellow we explain it''s parameters and signals:'
  prefs: []
  type: TYPE_NORMAL
- en: 'N: Batch size (Number of images on the 4d tensor)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'F: Number of filters on the convolution layer'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'kW/kH: Kernel Width/Height (Normally we use square images, so kW=kH)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H/W: Image height/width (Normally H=W)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H''/W'': Convolved image height/width (Remains the same as input if proper
    padding is used)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stride: Number of pixels that the convolution sliding window will travel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Padding: Zeros added to the border of the image to keep the input and output
    size the same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Depth: Volume input depth (ie if the input is a RGB image depth will be 3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output depth: Volume output depth (same as F)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the forward propagation, you must remember, that we're going to "convolve"
    each input depth with a different filter, and each filter will look for something
    different on the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](filters_per_layer.png)'
  prefs: []
  type: TYPE_IMG
- en: Here observe that all neurons(flash-lights) from layer 1 share the same set
    of weights, other filters will look for different patterns on the image.
  prefs: []
  type: TYPE_NORMAL
- en: Matlab Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically we can consider the previous "convn_vanilla" function on the [Convolution
    chapter](convolution_split_000.html) and apply for each depth on the input and
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Forward_CONV.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The only point to observe here is that due to the way the multidimensional arrays
    are represented in python our tensors will have different order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_Forward_CONV.png)'
  prefs: []
  type: TYPE_IMG
- en: Back-propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In order to derive the convolution layer back-propagation it's easier to think
    on the 1d convolution, the results will be the same for 2d.
  prefs: []
  type: TYPE_NORMAL
- en: So doing a 1d convolution, between a signal ![](666fe02f.png) and ![](678ee8ba.png),
    and without padding we will have ![](e271041e.png), where ![](a751091f.png). Here
    flip can be consider as a 180 degrees rotation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Manual_symbolic.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we convert all the "valid cases" to a computation graph, observe that for
    now we're adding the bias because it is used on the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the graphs are basically the same as the fully connected layer,
    the only difference is that we have shared weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Simple_1d_Conv_Bias.png)'
  prefs: []
  type: TYPE_IMG
- en: Now changing to the back-propagation
  prefs: []
  type: TYPE_NORMAL
- en: '![](Simple_1d_Conv_Back.png)'
  prefs: []
  type: TYPE_IMG
- en: If you follow the computation graphs backward, as was presented on the [Backpropagation
    chapter](backpropagation_split_000.html) we will have the following formulas for
    ![](a537b4ba.png), which means how the loss will change with the input X ![](5b05ec2e.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider some things:'
  prefs: []
  type: TYPE_NORMAL
- en: dX must have the same size of X, so we need padding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dout must have the same size of Y, which in this case is 3 (Gradient input)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To save programming effort we want to calculate the gradient as a convolution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On dX gradient all elements are been multiplied by W so we're probably convolving
    W and dout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following the output size rule for the 1d convolution: ![](34481da6.png) Our
    desired size is 3, our original input size is 3, and we''re going to convolve
    with the W matrix that also have 3 elements. So we need to pad our input with
    2 zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Backprop_dX.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The convolution above implement all calculations needed for ![](a537b4ba.png),
    so in terms of convolution: ![](1af9a957.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's continue for ![](7086761.png), considering that they must have the
    same size as W. ![](85992f4e.png)
  prefs: []
  type: TYPE_NORMAL
- en: Again by just looking to the expressions that we took from the graph we can
    see that is possible to represent them as a convolution between dout and X. Also
    as the output will be 3 elements, there is no need to do padding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Backprop_dW.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So in terms of convolution the calculations for ![](a06dcfa1.png) will be:
    ![](85ad1578.png) Just one point to remember, if you consider X to be the kernel,
    and dout the signal, X will be automatically flipped. ![](2bd929bc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now for the bias, the calculation will be similar to the Fully Connected layer.
    Basically we have one bias per filter (depth) ![](ce8280a3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before jumping to the code some points need to be reviewed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use some parameter (ie: Stride/Pad) during forward propagation you need
    to apply them on the backward propagation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On Python our multidimensional tensor will be "input=[N x Depth x H x W]" on
    matlab they will be "input=[H x W x Depth x N]"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As mentioned before the gradients of a input, has the same size as the input
    itself "size(x)==size(dx)"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matlab Backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Backward_CONV.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_Backward_CONV.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Pooling layer
  prefs: []
  type: TYPE_NORMAL
- en: Making faster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making faster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we show a way to convert your convolution operation into a matrix
    multiplication. This has the advantage to compute faster, at the expense of more
    memory usage. We employ the **im2col** operation that will transform the input
    image or batch into a matrix, then we multiply this matrix with a reshaped version
    of our kernel. Then at the end we reshape this multiplied matrix back to an image
    with the **col2im** operation.
  prefs: []
  type: TYPE_NORMAL
- en: Im2col
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As shown on previous source code, we use a lot for for-loops to implement the
    convolutions, while this is useful for learning purpose, it's not fast enough.
    On this section we will learn how to implement convolutions on a vectorized fashion.
  prefs: []
  type: TYPE_NORMAL
- en: First, if we inspect closer the code for convolution is basically a dot-product
    between the kernel filter and the local regions selected by the moving window,
    that sample a patch with the same size as our kernel.
  prefs: []
  type: TYPE_NORMAL
- en: What would happens if we expand all possible windows on memory and perform the
    dot product as a matrix multiplication. Answer 200x or more speedups, at the expense
    of more memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Convolution_With_Im2col.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if the input is [227x227x3] and it is to be convolved with 11x11x3
    filters at stride 4 and padding 0, then we would take [11x11x3] blocks of pixels
    in the input and stretch each block into a column vector of size ![](ce877a37.png)
    = 363.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating with input 227 with stride 4 and padding 0, gives ((227-11)/4)+1
    = 55 locations along both width and height, leading to an output matrix X_col
    of size [363 x 3025].
  prefs: []
  type: TYPE_NORMAL
- en: Here every column is a stretched out receptive field (patch with depth) and
    there are 55*55 = 3025 of them in total.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize how we calculate the im2col output sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The weights of the CONV layer are similarly stretched out into rows. For example,
    if there are 96 filters of size [11x11x3] this would give a matrix W_row of size
    [96 x 363], where 11x11x3=363
  prefs: []
  type: TYPE_NORMAL
- en: '![](im2col_operation.png)'
  prefs: []
  type: TYPE_IMG
- en: After the image and the kernel are converted, the convolution can be implemented
    as a simple matrix multiplication, in our case it will be W_col[96 x 363] multiplied
    by X_col[363 x 3025] resulting as a matrix [96 x 3025], that need to be reshaped
    back to [55x55x96].
  prefs: []
  type: TYPE_NORMAL
- en: This final reshape can also be implemented as a function called col2im.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that some implementations of im2col will have this result transposed,
    if this is the case then the order of the matrix multiplication must be changed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Im2Col_cs231n.png)'
  prefs: []
  type: TYPE_IMG
- en: Forward graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In order to help the usage of im2col with convolution and also to derive the
    back-propagation, let's show the convolution with im2col as a graph. Here the
    input tensor is single a 3 channel 4x4 image. That will pass to a convolution
    layer with S:1 P:0 K:2 and F:1 (Output volume).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Graph_Im2col.png)'
  prefs: []
  type: TYPE_IMG
- en: Backward graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Using the im2col technique the computation graph resembles the FC layer with
    the same format ![](479bdf40.png), the difference that now we have a bunch of
    reshapes, transposes and the im2col block.
  prefs: []
  type: TYPE_NORMAL
- en: About the reshapes and transposes during back propagation you just need to invert
    their operations using again another reshape or transpose, the only important
    thing to remember is that if you use a reshape row major during forward propagation
    you need to use a reshape row major on the backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: The only point to pay attention is the im2col backpropagation operation. The
    issue is that it cannot be implemented as a simple reshape. This is because the
    patches could actually overlap (depending on the stride), so you need to sum the
    gradients where the patches intersect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Graph_Im2col_Backward.png)'
  prefs: []
  type: TYPE_IMG
- en: Matlab forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Matlab backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Matlab im2col
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Matlab im2col backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Python example for forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Python example for backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Im2col and Col2im sources in python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This implementation will receive a image on the format of a 3 dimension tensor
    [channels, rows, cols] and will create a 2d matrix on the format [rows=(new_h*new_w),
    cols=(kw*kw*C)] notice that this algorithm will output the transposed version
    of the diagram above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Smaller example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: To make things simpler on our heads, follow the simple example of convolving
    X[3x3] with W[2x2]
  prefs: []
  type: TYPE_NORMAL
- en: '![](simple_im2col.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](im2col_matlab_example.png)'
  prefs: []
  type: TYPE_IMG
- en: Pooling Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](MaxPoolingLayer.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The pooling layer, is used to reduce the spatial dimensions, but not depth,
    on a convolution neural network, model, basically this is what you gain:'
  prefs: []
  type: TYPE_NORMAL
- en: By having less spatial information you gain computation performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Less spatial information also means less parameters, so less chance to over-fit
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You get some translation invariance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some projects don't use pooling, specially when they want to "learn" some object
    specific position. Learn how to play atari games.
  prefs: []
  type: TYPE_NORMAL
- en: On the diagram bellow we show the most common type of pooling the max-pooling
    layer, which slides a window, like a normal convolution, and get the biggest value
    on the window as the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Pooling_Simple_max.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The most important parameters to play:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: H1 x W1 x Depth_In x N'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stride: Scalar that control the amount of pixels that the window slide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: Kernel size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regarding it''s Output H2 x W2 x Depth_Out x N:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](d8085eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: It's also valid to point out that there is no learnable parameters on the pooling
    layer. So it's backpropagation is simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The window movement mechanism on pooling layers is the same as convolution layer,
    the only change is that we will select the biggest value on the window.
  prefs: []
  type: TYPE_NORMAL
- en: Python Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_maxpool_forward.png)'
  prefs: []
  type: TYPE_IMG
- en: Matlab Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Backward Propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: From the [backpropagation chapter](backpropagation_split_000.html) we learn
    that the max node simply act as a router, giving the input gradient "dout" to
    the input that has value bigger than zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](MaxGate.png) You can consider that the max pooling use a series of max
    nodes, on it''s computation graph. So consider the backward propagation of the
    max pooling layer as a product between a mask containing all elements that were
    selected during the forward propagation and dout.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](BackPropagation_MaxPool.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words the gradient with respect to the input of the max pooling layer
    will be a tensor make of zeros except on the places that was selected during the
    forward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Python Backward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_maxpool_backward.png)'
  prefs: []
  type: TYPE_IMG
- en: Improving performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On future chapter we will learn a technique that improves the convolution performance,
    until them we will stick with the naive implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Batch Norm layer
  prefs: []
  type: TYPE_NORMAL
- en: Batch Norm layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch Norm layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we will learn about the batch norm layer. Previously we said
    that [feature scaling](https://www.gitbook.com/book/leonardoaraujosantos/artificial-inteligence/edit#/edit/master/feature_scaling.md)
    make the job of the gradient descent easier. Now we will extend this idea and
    normalize the activation of every Fully Connected layer or Convolution layer during
    training. This also means that while we're training we will select an batch calculate
    it's mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: You can think that the batch-norm will be some kind of adaptive (or learnable)
    pre-processing block with trainable parameters. Which also means that we need
    to back-propagate them.
  prefs: []
  type: TYPE_NORMAL
- en: The original batch-norm paper can be found [here](http://arxiv.org/pdf/1502.03167v3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the list of advantages of using Batch-Norm:'
  prefs: []
  type: TYPE_NORMAL
- en: Improves gradient flow, used on very deep models (Resnet need this)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Allow higher learning rates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce dependency on initialization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gives some kind of regularization (Even make Dropout less important but keep
    using it)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a rule of thumb if you use Dropout+BatchNorm you don't need L2 regularization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It basically force your activations (Conv,FC ouputs) to be unit standard deviation
    and zero mean.
  prefs: []
  type: TYPE_NORMAL
- en: To each learning batch of data we apply the following normalization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](f4932fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Compute_BatchNorm.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the batch norm layer, has the ![](87a87a8c.png) are parameters.
    Those parameters will be learned to best represent your activations. Those parameters
    allows a learnable (scale and shift) factor ![](dd16f1ee.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now summarizing the operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_fp.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](9fe1930b.png) is a small number, 1e-5.
  prefs: []
  type: TYPE_NORMAL
- en: Where to use the Batch-Norm layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The batch norm layer is used after linear layers (ie: FC, conv), and before
    the non-linear layers (relu). There is actually 2 batch norm implementations one
    for FC layer and the other for conv layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](BatchNorm_Placement.png)'
  prefs: []
  type: TYPE_IMG
- en: Test time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: At prediction time that batch norm works differently. The mean/std are not computed
    based on the batch. Instead, we need to build a estimate during training of the
    mean/std of the whole dataset(population) for each batch norm layer on your model.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to estimating the population mean and variance during training
    is to use an [exponential moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average).
  prefs: []
  type: TYPE_NORMAL
- en: '![](749e2654.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where: ![](c5ae95b1.png): Current and previous estimation ![](de87fcce.png):
    Represents the degree of weighting decrease, a constant smoothing factor between
    0 and 1 ![](c03f5ce1.png): Current value (could be mean or std) that we''re trying
    to estimate'
  prefs: []
  type: TYPE_NORMAL
- en: Normally when we implement this layer we have some kind of flag that detects
    if we're on training or testing.
  prefs: []
  type: TYPE_NORMAL
- en: As reference we can find some tutorials with [Tensorflow](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html)
    or [manually on python](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html).
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier we need to know how to backpropagate on the batch-norm
    layer, first as we did with other layers we need to create the computation graph.
    After this step we need to calculate the derivative of each node with respect
    to it's inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to find the partial derivatives on back-propagation is better to visualize
    the algorithm as a computation graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_computation_graph.png)'
  prefs: []
  type: TYPE_IMG
- en: New nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By inspecting this graph we have some new nodes (![](48a4007c.png), ![](cf0feaac.png),
    ![](f52524b5.png), ![](59e67941.png)). To simplify things you can use [Wolfram
    alpha](https://www.wolframalpha.com/) to find the derivatives. For backpropagate
    other nodes refer to the [Back-propagation chapter](https://www.gitbook.com/book/leonardoaraujosantos/artificial-inteligence/edit#/edit/master/backpropagation.md)
  prefs: []
  type: TYPE_NORMAL
- en: '[Block 1/x](https://www.wolframalpha.com/input/?i=derivative+1%2Fx)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_1_over_x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](7b972ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where: ![](b8367cc3.png) means the cached (or saved) input from the forward
    propagation. ![](9a616dbf.png) means the previous block gradient'
  prefs: []
  type: TYPE_NORMAL
- en: '[Block sqrt(x-epsilon)](https://www.wolframalpha.com/input/?i=derivative+of+sqrt(x-epsilon)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_sqrt_x.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](496a1ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where: ![](b8367cc3.png): the cached (or saved) input from the forward propagation.
    ![](9a616dbf.png): the previous block gradient ![](9fe1930b.png): Some small number
    0.00005'
  prefs: []
  type: TYPE_NORMAL
- en: '[Block x^2](https://www.wolframalpha.com/input/?i=derivative+of+x%5E2)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_x2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](5d7daa76.png)'
  prefs: []
  type: TYPE_IMG
- en: Block Summation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_SUM.png)'
  prefs: []
  type: TYPE_IMG
- en: Like the SUM block this block will copy the input gradient dout equally to all
    it's inputs. So for all elements in X we will divide by N and multiply by dout.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python Forward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_fp_python.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Backward Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_bp_python.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about how to optimize our model weights.
  prefs: []
  type: TYPE_NORMAL
- en: Model Solver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Solver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The mission of the model solver is to find the best set of parameters, that
    minimize the train/accuracy errors. On this chapter we will give a UML description
    with some piece of python/matlab code that allows you implement it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ClassDiagram.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the UML description we can infer some information about the Solver class:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses the training set, and has a reference to your model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Uses different type of optimizers(ex: SGD, ADAM, SGD with momentum)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep tracks of all the loss, accuracy during the training phase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the set of parameters, that achieved best validation performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Solver_Usage.png)'
  prefs: []
  type: TYPE_IMG
- en: Train operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the method called when you actually want to start a model training,
    the methods Step, Check_Accuracy are called inside the Train method:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate number of iterations per epoch, based on number of epochs, train size,
    and batch size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call step, for each iteration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decay the learning rate
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the validation accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cache the best parameters based on validation accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically during the step operation the following operations are done:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract a batch from the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the model loss and gradients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a parameter update with one of the optimizers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This method basically is called at the end of each epoch. Basically it uses
    the current set of parameters, and predict the whole validation set. The objective
    is at the end get the accuracy. ![](7f8bb141.png)
  prefs: []
  type: TYPE_NORMAL
- en: Model loss operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned during the "Step" operation that we get the model loss and gradients.
    This operation is implemented by the "getLoss" method. Consider the following
    basic model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleModelLoss.png) Bellow we have the "getLoss" function for the previous
    simple model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](getLoss.png)'
  prefs: []
  type: TYPE_IMG
- en: Also bellow we have the "softmax_loss" function including "dout", ![](50803af0.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Softmax_Loss_Layer.png)'
  prefs: []
  type: TYPE_IMG
- en: Object Localization and Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object Localization and Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On this chapter we're going to learn about using convolution neural networks
    to localize and detect objects on images
  prefs: []
  type: TYPE_NORMAL
- en: '![](LocalizationDetection.png)'
  prefs: []
  type: TYPE_IMG
- en: RCNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast RCNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster RCNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yolo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Localize objects with regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression is about returning a number instead of a class, in our case we're
    going to return 4 numbers (x0,y0,width,height) that are related to a bounding
    box. You train this system with an image an a ground truth bounding box, and use
    L2 distance to calculate the loss between the predicted bounding box and the ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](LocalizationRegression1.png)'
  prefs: []
  type: TYPE_IMG
- en: Normally what you do is attach another fully connected layer on the last convolution
    layer
  prefs: []
  type: TYPE_NORMAL
- en: '![](LocalizationRegression2.png)'
  prefs: []
  type: TYPE_IMG
- en: This will work only for one object at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Some people attach the regression part after the last convolution (Overfeat)
    layer, while others attach after the fully connected layer (RCNN). Both works.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing bounding box prediction accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically we need to compare if the Intersect Over Union (ioU) between the prediction
    and the ground truth is bigger than some threshold (ex > 0.5)
  prefs: []
  type: TYPE_NORMAL
- en: '![](Intersection-over-Union-IoU-of.png)'
  prefs: []
  type: TYPE_IMG
- en: RCNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RCNN (Regions + CNN) is a method that relies on a external region proposal system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RCNNSimple.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The problem of RCNN is that it''s never made to be fast, for instance the steps
    to train the network are these:'
  prefs: []
  type: TYPE_NORMAL
- en: Take a pre-trained imagenet cnn (ex Alexnet)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-train the last fully connected layer with the objects that need to be detected
    + "no-object" class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get all proposals(=~2000 p/image), resize them to match the cnn input, then
    save to disk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train SVM to classify between object and background (One binary SVM for each
    class)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'BB Regression: Train a linear regression classifier that will output some correction
    factor'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 3 Save and pre-process proposals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](Step3RCNN.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 5 (Adjust bounding box)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](Step5RCNN.png)'
  prefs: []
  type: TYPE_IMG
- en: Fast RCNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fast RCNN method receive region proposals from some external system (Selective
    search). This proposals will sent to a layer (Roi Pooling) that will resize all
    regions with their data to a fixed size. This step is needed because the fully
    connected layer expect that all the vectors will have same size
  prefs: []
  type: TYPE_NORMAL
- en: '![](Fast_RCNN.png)'
  prefs: []
  type: TYPE_IMG
- en: Proposals example, boxes=[r, x1, y1, x2, y2]
  prefs: []
  type: TYPE_NORMAL
- en: '![](Proposals.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Fast_RCnn_Caffe_LastPart.png)'
  prefs: []
  type: TYPE_IMG
- en: Still depends on some external system to give the region proposals (Selective
    search)
  prefs: []
  type: TYPE_NORMAL
- en: Roi Pooling layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](RoiPoolingLayer.png)'
  prefs: []
  type: TYPE_IMG
- en: It's a type of max-pooling with a pool size dependent on the input, so that
    the output always has the same size. This is done because fully connected layer
    always expected the same input size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RoiPoolingLayerCaffe.png)'
  prefs: []
  type: TYPE_IMG
- en: The inputs of the Roi layer will be the proposals and the last convolution layer
    activations. For example consider the following input image, and it's proposals.
  prefs: []
  type: TYPE_NORMAL
- en: Input image
  prefs: []
  type: TYPE_NORMAL
- en: '![](InImage.png)'
  prefs: []
  type: TYPE_IMG
- en: Two proposed regions
  prefs: []
  type: TYPE_NORMAL
- en: '![](InImageRegions.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the activations on the last convolution layer (ex: conv5)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv5_Activations.png)'
  prefs: []
  type: TYPE_IMG
- en: For each convolution activation (each cell from the image above) the Roi Pooling
    layer will resize, the region proposals (in red) to the same resolution expected
    on the fully connected layer. For example consider the selected cell in green.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv5_Activation_and_Proposals.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here the output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Roi_PoolingLayer1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Roi_PoolingLayer2.png)'
  prefs: []
  type: TYPE_IMG
- en: Faster RCNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Faster_Rcnn.png)'
  prefs: []
  type: TYPE_IMG
- en: The main idea is use the last (or deep) conv layers to infer region proposals.
  prefs: []
  type: TYPE_NORMAL
- en: Faster-RCNN consists of two modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'RPN (Region proposals): Gives a set of rectangles based on deep convolution
    layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fast-RCNN Roi Pooling layer: Classify each proposal, and refining proposal
    location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Region proposal Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we break on a block diagram how Faster RCNN works.
  prefs: []
  type: TYPE_NORMAL
- en: Get a trained (ie imagenet) convolution neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get feature maps from the last (or deep) convolution layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a region proposal network that will decide if there is an object or not
    on the image, and also propose a box location
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give results to a custom (python) layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give proposals to a ROI pooling layer (like Fast RCNN)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After all proposals get reshaped to a fix size, send to a fully connected layer
    to continue the classification
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](RegionProposalNetwork.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](RPN_Network.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Basically the RPN slides a small window (3x3) on the feature map, that classify
    what is under the window as object or not object, and also gives some bounding
    box location.
  prefs: []
  type: TYPE_NORMAL
- en: For every slidding window center it creates fixed k anchor boxes, and classify
    those boxes as been object or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](RPN_Sliding.png)'
  prefs: []
  type: TYPE_IMG
- en: Faster RCNN training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the paper, each network was trained separately, but we also can train it
    jointly. Just consider the model having 4 losses.
  prefs: []
  type: TYPE_NORMAL
- en: RPN Classification (Object or not object)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RPN Bounding box proposal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast RCNN Classification (Normal object classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast RCNN Bounding-box regression (Improve previous BB proposal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](FasterRCNNTrain.png)'
  prefs: []
  type: TYPE_IMG
- en: Faster RCNN results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best result now is Faster RCNN with a resnet 101 layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FasterRCNNSpeedComparison.png)'
  prefs: []
  type: TYPE_IMG
- en: Complete Faster RCNN diagram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This diagram represents the complete structure of the Faster RCNN using VGG16,
    I've found on a github project [here](https://github.com/mitmul/chainer-faster-rcnn).
    It uses a framework called [Chainer](https://github.com/pfnet/chainer) which is
    a complete framework using only python (Sometimes cython).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Faster%20R-CNN.png)'
  prefs:
  - PREF_H2
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will discuss a different type of object detector called
    single shot detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Single Shot Detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Single Shot detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous methods of object detection all share one thing in common: they
    have one part of their network dedicated to providing region proposals followed
    by a high quality classifier to classify these proposals. These methods are very
    accurate but come at a big computational cost (low frame-rate), in other words
    they are not fit to be used on embedded devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Another way of doing object detection is by combining these two tasks into one
    network. We can do this by instead of having a network produce proposals we instead
    have a set of pre defined boxes in which to look for objects.
  prefs: []
  type: TYPE_NORMAL
- en: Using convolutional features maps from later layers of a network we run small
    conv filters over these features maps to predict class scores and bounding box
    offsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the family of object detectors that follow this strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD: Uses different activation maps (multiple-scales) for prediction of classes
    and bounding boxes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YOLO: Uses a single activation map for prediction of classes and bounding boxes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R-FCN(Region based Fully-Convolution Neural Networks): Like Faster Rcnn, but
    faster due to less computation ber box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multibox: asdasdas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these multiple scales helps to achieve a higher mAP(mean average precision)
    by being able to detect objects with different sizes on the image.
  prefs: []
  type: TYPE_NORMAL
- en: Summarising the strategy of these methods
  prefs: []
  type: TYPE_NORMAL
- en: Train a CNN with regression(box) and classification objective (loss function).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use sliding window (conv) and non-maxima suppression during prediction on the
    conv feature maps (output of conv-relu)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On this kind of detectors it is typical to have a collection of boxes overlaid
    on the image at different spatial locations, scales and aspect ratios that act
    as “anchors” (sometimes called “priors” or “default boxes”).
  prefs: []
  type: TYPE_NORMAL
- en: 'A model is then trained to make two predictions for each anchor:'
  prefs: []
  type: TYPE_NORMAL
- en: A discrete class prediction for each anchor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A continuous prediction of an offset by which the anchor needs to be shifted
    to fit the ground-truth bounding box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also the loss used on this methods are a combination of the 2 objectives, localization(regression)
    and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to learn how to classify each pixel on the image, the idea is
    to create a map of all detected object areas on the image. Basically what we want
    is the image bellow where every pixel has a label.
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we're going to learn how convolutional neural networks (CNN)
    can do the job.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ImageSegmentation.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Fully Convolutional network for segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A Fully Convolutional neural network (FCN) is a normal CNN, where the last fully
    connected layer is substituted by another convolution layer with a large "receptive
    field". The idea is to capture the global context of the scene (Tell what we have
    on the image and also give some rude location where it is).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Fully_Convolutional_Network_Semantic.PNG)'
  prefs: []
  type: TYPE_IMG
- en: Just remember that when we convert our last fully connected (FC) layer to a
    convolutional layer we gain some form of localization if we look where we have
    more activations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The idea is that if we choose our new last conv layer to be big enough we will
    have this localization effect scaled up to our input image size.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from normal CNN to FCN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we convert a normal CNN used for classification, ie: Alexnet to
    a FCN used for segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to remember this is how Alexnet looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexNet_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellow is also show the parameters for each layer
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexNet_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On Alexnet the inputs are fixed to be 227x227, so all the pooling effects will
    scale down the image from 227x227 to 55x55, 27x27, 13x13, then finally a single
    row vector on the FC layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexNet_0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now let's look on the steps needed to do the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 1) We start with a normal CNN for classification with
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 2) The second step is to convert all the FC layers to convolution layers 1x1
    we don't even need to change the weights at this point. (This is already a fully
    convolutional neural network). The nice property of FCN networks is that we can
    now use any image size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Observe here that with a FCN we can use a different size H x N.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_3.png)'
  prefs: []
  type: TYPE_IMG
- en: 3) The last step is to use a "deconv or transposed convolution" layer to recover
    the activation positions to something meaningful related to the image size. Imagine
    that we're just scaling up the activation size to the same image size.
  prefs: []
  type: TYPE_NORMAL
- en: This last "upsampling" layer also have lernable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now with this structure we just need to find some "ground truth" and to end
    to end learning, starting from e pre-trainned network ie: Imagenet.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this approach is that we loose some resolution by just doing
    this because the activations were downscaled on a lot of steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](FirstResultFCN_No_Skips.png)'
  prefs: []
  type: TYPE_IMG
- en: To solve this problem we also get some activation from previous layers and sum
    them together. This process is called "skip" from the creators of this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Even today (2016) the winners on Imagenet on the Segmentation category, used
    an ensemble of FCN to win the competition.
  prefs: []
  type: TYPE_NORMAL
- en: Those up-sampling operations used on skip are also learn-able.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Skip_Layers_FCN.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellow we show the effects of this "skip" process notice how the resolution
    of the segmentation improves after some "skips"
  prefs: []
  type: TYPE_NORMAL
- en: '![](AllSkips_FCN.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](SkipConnections.png)'
  prefs: []
  type: TYPE_IMG
- en: Transposed convolution layer (deconvolution "bad name")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Basically the idea is to scale up, the scale down effect made on all previous
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Deconv.PNG)'
  prefs: []
  type: TYPE_IMG
- en: '![](Deconv_exp.PNG)'
  prefs: []
  type: TYPE_IMG
- en: '![](animUpsampling.gif)'
  prefs: []
  type: TYPE_IMG
- en: It has this bad name because the upsamping forward propagation is the convolution
    backpropagation and the upsampling backpropagation is the convolution forward
    propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Also in caffe source code it is wrong called "deconvolution"
  prefs: []
  type: TYPE_NORMAL
- en: Extreme segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: There is another thing that we can do to avoid those "skiping" steps and also
    give better segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Deconvnet.png)'
  prefs: []
  type: TYPE_IMG
- en: This architechture is called "Deconvnet" which is basically another network
    but now with all convolution and pooling layers reversed. As you may suspect this
    is heavy, it takes 6 days to train on a TitanX. But the results are really good.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is that the trainning is made in 2 stages.
  prefs: []
  type: TYPE_NORMAL
- en: Also Deconvnets suffer less than FCN when there are small objects on the scene.
  prefs: []
  type: TYPE_NORMAL
- en: The deconvolution network output a probability map with the same size as the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeconvnetResults.png)'
  prefs: []
  type: TYPE_IMG
- en: Unpooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the deconvolution layer we also need now the unpooling layer. The max-pooling
    operation is non-invertible, but we can approximate, by recording the positions
    (Max Location switches) where we located the biggest values (during normal max-pool),
    then use this positions to reconstruct the data from the layer above (on this
    case a deconvolution)
  prefs: []
  type: TYPE_NORMAL
- en: '![](UnPoolinDiagram.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](Unpooling_1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](UnpoolResults.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will discuss some libraries that support deep learning
  prefs: []
  type: TYPE_NORMAL
- en: GoogleNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GoogleNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](GoogleNet.png)'
  prefs: []
  type: TYPE_IMG
- en: On this chapter you will learn about the googleNet (Winning architecture on
    ImageNet 2014) and it's inception layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](ImagenetTable.png)'
  prefs: []
  type: TYPE_IMG
- en: googleNet has 22 layer, and almost 12x less parameters (So faster and less then
    Alexnet and much more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Their idea was to make a model that also could be used on a smart-phone (Keep
    calculation budget around 1.5 billion multiply-adds on prediction).
  prefs: []
  type: TYPE_NORMAL
- en: Inception Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the inception layer is to cover a bigger area, but also keep a fine
    resolution for small information on the images. So the idea is to convolve in
    parallel different sizes from the most accurate detailing (1x1) to a bigger one
    (5x5).
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that a series of gabor filters with different sizes, will handle
    better multiple objects scales. With the advantage that all filters on the inception
    layer are learnable.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward way to improve performance on deep learning is to use
    more layers and more data, googleNet use 9 inception modules. The problem is that
    more parameters also means that your model is more prone to overfit. So to avoid
    a parameter explosion on the inception layers, all bottleneck techniques are exploited.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Naive_Version.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](InceptionModules.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](inception_1x1.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the bottleneck approaches we can rebuild the inception module with more
    non-linearities and less parameters. Also a max pooling layer is added to summarize
    the content of the previous layer. All the results are concatenated one after
    the other, and given to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Caffe Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we present 2 inception layers on cascade from the original googleNet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](caffe_inception.png)'
  prefs: []
  type: TYPE_IMG
- en: Residual Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Residual Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will present the 2016 state of the art on object classification.
    The ResidualNet it's basically a 150 deep convolution neural network made by equal
    "residual" blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is for real deep networks (more than 30 layers), all the known techniques
    (Relu, dropout, batch-norm, etc...) are not enough to do a good end-to-end training.
    This contrast with the common "empirical proven knowledge" that deeper is better.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the residual network is use blocks that re-route the input, and
    add to the concept learned from the previous layer. The idea is that during learning
    the next layer will learn the concepts of the previous layer plus the input of
    that previous layer. This would work better than just learn a concept without
    a reference that was used to learn that concept.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to visualize their solution is remember that the back-propagation
    of a sum node will replicate the input gradient with no degradation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_building_block.png)'
  prefs: []
  type: TYPE_IMG
- en: Bellow we show an example of a 34-deep residual net.
  prefs: []
  type: TYPE_NORMAL
- en: '![](residualnet_34.png)'
  prefs: []
  type: TYPE_IMG
- en: The ResidualNet creators proved empiricaly that it's easier to train a 34-layer
    residual compared to a 34-layer cascaded (Like VGG).
  prefs: []
  type: TYPE_NORMAL
- en: Observe that on the end of the residual net there is only one fully connected
    layer followed by a previous average pool.
  prefs: []
  type: TYPE_NORMAL
- en: Residual Block
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: At it's core the residual net is formed by the following structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_block.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically this jump and adder creates a path for back-propagation, allowing
    even really deep models to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: As mention before the Batch-Norm block alleviate the network initialization,
    but it can be omitted for not so deep models (less than 50 layers).
  prefs: []
  type: TYPE_NORMAL
- en: Again like googlenet we must use bottlenecks to avoid a parameter explosion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_bottleneck.png)'
  prefs: []
  type: TYPE_IMG
- en: Just to remember for the bottleneck to work the previous layer must have same
    depth.
  prefs: []
  type: TYPE_NORMAL
- en: Caffe Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we show 2 cascaded residual blocks form residual net, due to difficulties
    with batch-norm layers, they were omitted but still residual net gives good results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](caffe_residual.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep Learning Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Discussion, and some examples on the most common deep learning libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most basic characteristic of caffe is that is easy to train simple
    non recurrent models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most cool features:'
  prefs: []
  type: TYPE_NORMAL
- en: Good Performance, allows training with multiple GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation for CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code is easy to read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow layer definition in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has bidings for Python and Matlab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows network definition with text language (No need to write code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast dataset access through LMDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows network vizualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has web interface (Digits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caffe Main classes:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](CaffeOverview.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Blob: Used to store data and diffs(Derivatives)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Layer: Some operation that transform a bottom blob(input) to top blobs(outputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Net: Set of connected layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solver: Call Net forward and backward propagation, update weights using gradient
    methods (Gradient descent, SGD, adagrad, etc...)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe training/validation files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: path/to/image/1.jpg [label]
  prefs: []
  type: TYPE_NORMAL
- en: Simple example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here a logistic regression classifier. Imagine as a neural network with one
    layer and a sigmoid (cross-entropy softmax) non-linearity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Caffe_Logistic.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](Caffe_Proto_Logistic.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Caffe Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Need to write C++ / Cuda code for new layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad to write protofiles for big networks (Resnet, googlenet)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad to experience new architectures (Mainstream version does not support Fast
    RCNN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Really good for research, the problem is that use a new language called Lua.
  prefs: []
  type: TYPE_NORMAL
- en: Torch Pros
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flexible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very easy source code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy biding with C/C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web interface (Digits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: New language Lua
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to load data from directories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Matlab bidings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less Plug and play than caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not easy for RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Theano Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More manual
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matlab biding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slower than other frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No much pre-trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow Pros
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flexible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good for RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow distributed training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorboard for signal visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Numpy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorflow Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not much pre-trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Support for new object detection features (Ex Roi pooling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No support for datasets like Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slower than Caffe for single GPU training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNTK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: CNTK Pros
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flexible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good for RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows distributed training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNTK Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: No visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any error CNTK crash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No simple source code to read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New language (ndl) to describe networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No current matlab or python bindings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For research use Torch or Tensorflow (Last option Theano)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training convnets or use pre-trained models use Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CS231n Deep learning course summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](DeepLibrariesOverview.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Get features from known model (Alexnet, Googlenet, Vgg): Use caffe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine tune known models (Alexnet, Googlenet, Vgg): Use Caffe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Captioning: Torch or Tensorflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Segmentation: Caffe, Torch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object Detection: Caffe with python layers, Torch (More work)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language Modelling: Torch, Theano'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implement Bath Norm: Torch, Theano or Tensorflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally Tensorflow can be used in all cased that torch can, but if you need
    to understand what a specific layer does, or if you need to create a new layer,
    use torch instead of tensorflow. Torch is preferable on those cases, because the
    layer source code is more easy to read in torch.
  prefs: []
  type: TYPE_NORMAL
- en: Next Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will discuss Distributed Learning
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned on previous chapters, unsupervised learning is about learning information
    without the label information.
  prefs: []
  type: TYPE_NORMAL
- en: Here the term information means, "structure" for instance you would like to
    know how many groups exist in your dataset, even if you don't know what those
    groups mean.
  prefs: []
  type: TYPE_NORMAL
- en: Also we use unsupervised learning to visualize your dataset, in order to try
    to learn some insight from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Unlabeled data example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following dataset ![](750c80ea.png) (X has 2 features)
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleDataUnlabeled.png)'
  prefs: []
  type: TYPE_IMG
- en: One type of unsupervised learning algorithm called "clustering" is used to infer
    how many distinct groups exist on your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleDataClustered.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we still don't know what those groups means, but we know that there are
    4 groups that seems very distinct. On this case we choose a low dimensional dataset
    ![](6a67a3f7.png) but on real life it could be thousands of dimensions, ie ![](fe46b6da.png)
    for a grayscale 28x28 image.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to improve classification response time (not prediction performance)
    and sometimes for visualizing your high dimension dataset (2D, 3D), we use dimesionality
    reduction techniques (ie: PCA, T-Sne).'
  prefs: []
  type: TYPE_NORMAL
- en: For example the [MNIST datset](http://yann.lecun.com/exdb/mnist/) is composed
    with 60,000 training examples of (0..9) digits, each one with 784 dimensions.
    This high dimensionality is due to the fact that each digit is a 28x28 grayscale
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](MNIST_samples.png)'
  prefs: []
  type: TYPE_IMG
- en: It would be difficult to vizualize this dataset, so one option is to reduce
    it's dimensions to something visible on the monitor (2D,3D).
  prefs: []
  type: TYPE_NORMAL
- en: '![](mnist_tSNE.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here is easy to observe that a classifier could have problems to differentiate
    the digit 1 and 7.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is that this gives us some hint on how good is our current
    set of features.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We can also use neural networks to do dimensionality reduction the idea is that
    we have a neural network topology that approximate the input on the output layer.
    On the middle the autoencoder has smaller layer. After training the middle layer
    has a compressed version (lossy) of the input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](AutoEncoder.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution Neural network pre-train
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we don't need the label information to train autoencoders, we can use them
    as a pre-trainer to our convolution neural network. So in the future we can start
    your training with the weights initialized from unsupervised training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](conv_autoencoder.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some examples of this technique can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[With Python](https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[With Torch](https://siavashk.github.io/2016/02/22/autoencoder-imagenet/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Manifold
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Manifold Learning pursuits the goal to embed data that originally lies in a
    high dimensional space in a lower dimensional space, while preserving characteristic
    properties. This is possible because for any high dimensional data to be interesting,
    it must be intrinsically low dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: For example, images of faces might be represented as points in a high dimensional
    space (let’s say your camera has 5MP -- so your images, considering each pixel
    consists of three values [r,g,b], lie in a 15M dimensional space), but not every
    5MP image is a face. Faces lie on a sub-manifold in this high dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: A sub-manifold is locally Euclidean, i.e. if you take two very similar points,
    for example two images of identical twins they will be close on the euclidian
    space
  prefs: []
  type: TYPE_NORMAL
- en: '![](ManifoldSubspace.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example on the dataset above we have a high dimension manifold, but the
    faces sit's on a much lower dimension space (almost euclidian). So on this subspace
    things like distance has a meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the increase of more features, the data distribution will not be linear,
    so simpler linear techniques (ex: PCA) will not be useful for dimensionality reduction.
    On those cases we need other stuff like T-Sne, Autoencoders, etc..'
  prefs: []
  type: TYPE_NORMAL
- en: By the way dimensionality reduction on non-linear manifolds is sometimes called
    manifold learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](LinearNonLinear.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](ScatterPlotsMatlab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bellow we have a diagram that guide you depending on the type of problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](DimensionalityReduction.png)'
  prefs: []
  type: TYPE_IMG
- en: Here is a comparison between the T-SNE method against PCA on MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_TSNE.png)'
  prefs: []
  type: TYPE_IMG
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we're going to learn about Principal Component Analysis (PCA)
    which is a tool used to make dimensionality reduction. This is usefull because
    it make the job of classifiers easier in terms of speed, or to aid data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: So what are principal components then? They're the underlying structure in the
    data. They are the directions where there is the most variance on your data, the
    directions where the data is most spread out.
  prefs: []
  type: TYPE_NORMAL
- en: The only limitation if this algorithm is that it works better only when we have
    a linear manifold.
  prefs: []
  type: TYPE_NORMAL
- en: The PCA algorithm will try to fit a plane that minimize a projection error (sum
    of all red-line sizes)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the PCA will try to rotate your data looking for a angle where
    it see more variances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCAAnimation.gif)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned before you can use PCA when your data has a linear data manifold.
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_3d.png)'
  prefs: []
  type: TYPE_IMG
- en: But for non linear manifolds we're going to have a lot of projection errors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](SwissRoll_DataManifold.png) ![](PCA_SwissRoll.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocess the data: ![](106312a2.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the covariance matrix: ![](655dd0e4.png), ![](dfaeb377.png) is the
    number of elements, X is a matrix ![](ca7f258c.png) where n is experiment number
    and p the features'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the eigenvectors of the covariance matrix ![](27af2486.png), here the U
    matrix will be a nxn matrix where every column of U will be the principal components,
    if we want to reduce our data from n dimensions to k, we choose k columns from
    U.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preprocessing part sometimes includes a division by the standard deviation
    of each collumn, but there are cases that this is not needed. (The mean subtraction
    is more important)
  prefs: []
  type: TYPE_NORMAL
- en: Reducing input data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we calculate our principal components, which are stored on the matrix
    U, we will reduce our input data ![](aecd381d.png) from n dimensions to k dimensions
    ![](a86cb084.png). Here k is the number of columns of U. Depending on how you
    organized the data we can have 2 different formats for Z
  prefs: []
  type: TYPE_NORMAL
- en: '![](49ca84d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Get the data back
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reverse the transformation we do the following: ![](a800fa4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Example in Matlab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the whole process we're going to calculate the PCA from an image,
    and then restore it with less dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Get some data example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here our data is a matrix with 15 samples of 3 measurements [15x3]
  prefs: []
  type: TYPE_NORMAL
- en: '![](inputData_PCA.png)'
  prefs: []
  type: TYPE_IMG
- en: Data pre-processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to subtract the mean of each experiment from every column, then
    divide also each element by the standard deviation of each column.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Prep_InputPCA.png)'
  prefs: []
  type: TYPE_IMG
- en: mean and std will work on all columns of X
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the covariance matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](cov_matrix_pca.png)'
  prefs: []
  type: TYPE_IMG
- en: Get the principal components
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we use "svd" to get the principal components, which are the eigen-vectors
    and eigen-values of the covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](Pca_svd.png)'
  prefs: []
  type: TYPE_IMG
- en: There are different ways to calculate the PCA, for instance matlab gives already
    a function pca or princomp, which could give different signs on the eigenvectors
    (U) but they all represent the same components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](other_pca.png)'
  prefs: []
  type: TYPE_IMG
- en: The one thing that you should pay attention is the order of the input matrix,
    because some methods to find the PCA, expect that your samples and measurements,
    are in some pre-defined order.
  prefs: []
  type: TYPE_NORMAL
- en: Recover original data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now to recover the original data we use all the components, and also reverse
    the preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](X_Recover_PCA.png)'
  prefs: []
  type: TYPE_IMG
- en: Reducing our data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Actually normally we do something before we Now that we have our principal components
    let's apply for instance k=2
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_Reduce.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use the principal components Z to recreate the data X, but with some
    loss. The idea is that the data in Z is smaller than X, but with similar variance.
    On this case we have ![](21509b2a.png) awe could reproduce the data X_loss with
    ![](d2d11d37.png), so one dimension less.
  prefs: []
  type: TYPE_NORMAL
- en: '![](X_loss_PCA.png)'
  prefs: []
  type: TYPE_IMG
- en: Using PCA on images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Before finish the chapter we're going to use PCA on images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_Image_Compression_Code.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](PCA_image_compression.png)'
  prefs: []
  type: TYPE_IMG
- en: Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of generative models, is to be able to learn the probability distribution
    of the training set. By doing this the generative model can create more data from
    the original data. Imagine as been the perfect dataset augmentation system. So
    basically it can be used as a unsupervised way to generate samples to train other
    networks better.
  prefs: []
  type: TYPE_NORMAL
- en: Basically this done by having 2 neural networks playing against each other.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how the training of deep models can be distributed across multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: Map Reduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Map Reduce can be described on the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split your training set, in batches (ex: divide by the number of workers on
    your farm: 4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give each machine of your farm 1/4th of the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform Forward/Backward propagation, on each computer node (All nodes share
    the same model)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine results of each machine and perform gradient descent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update model version on all nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](MapReduceSimple.png)'
  prefs: []
  type: TYPE_IMG
- en: Example Linear Regression model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the batch gradient descent formula, which is the gradient descent
    applied on all training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](bd56965d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each machine will deal with 100 elements (After splitting the dataset), calculating
    ![](6a936391.png), then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](3bc3ebcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Each machine is calculating the back-propagation and error for it's own split
    of data. Remember that all machines have the same copy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: After each machine calculated their respective ![](706202b.png). Another machine
    will combine those gradients, calculate the new weights and update the model in
    all machines.
  prefs: []
  type: TYPE_NORMAL
- en: '![](215027ae.png)'
  prefs: []
  type: TYPE_IMG
- en: The whole point of this procedure is to check if we can combine the calculations
    of all nodes and still make sense, in terms of the final calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Who use this approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch (Parallel layer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This approach has some problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The complete model must fit on every machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is to big it will take time to update all machines with the same
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach whas used on google DistBelief project where they use a normal
    neural network model with weights separated between multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: '![](DistNeuralNetwork.png)'
  prefs: []
  type: TYPE_IMG
- en: On this approach only the weights (thick edges) that cross machines need to
    be synchronized between the workers. This technique could only be used on fully
    connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: If you mix both techniques (reference on Alexnet) paper, you do this share fully
    connected processing (Just a matrix multiplication), then when you need to the
    the convolution part, each convolution layer get one part of the batch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexnetDistribution.png)'
  prefs: []
  type: TYPE_IMG
- en: Google approach (old)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](GoogleDistOldApproach.png)'
  prefs: []
  type: TYPE_IMG
- en: Here each model replica is trained independently with pieces of data and a parameter
    server that synchronize the parameters between the workers.
  prefs: []
  type: TYPE_NORMAL
- en: Google new approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now google offer on Tensorflow some automation on choosing which strategy to
    follow depending on your work.
  prefs: []
  type: TYPE_NORMAL
- en: '![](TensorflowDistributes.png)'
  prefs: []
  type: TYPE_IMG
- en: Asynchronous Stochastic Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Methodology for usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will give a [recipe](https://www.youtube.com/watch?v=NKiwFF_zBu4&t=997s)
    on how to tackle Machine learning problems
  prefs: []
  type: TYPE_NORMAL
- en: 3 Step process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define what you need to do and how to measure(metrics) how well/bad you are
    going.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with a very simple model (Few layers)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add more complexity when needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define goals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normally by being slightly better than human performance(in terms of accuracy
    or prediction time), is already enough for a good product.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accuracy: % of correct examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coverage: Number of processed examples per unit of time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: Number of detections that are correct'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error: Amount of error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with simplest model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Look if available for the state-of-the-art method for solving that problem.
    If not available use the following recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lots of noise and now much structure(ex: House price from features like number
    of rooms,kitchen size, etc...): Don''t use deep learning'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Few noise but lot''s of structure (ex: Images, video, text): Use deep learning'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Examples for Non-deep methods:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosted decision trees (Previous favorite "default" algorithm), used a lot in
    robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of deep
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Few structure: Use only Fully-Connected layers 2-3 layers (Relu, Dropout, SGD+Momentum).
    Need at least few thousand examples per class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spatial structure: Use CONV layers (Inception/Residual, Relu, Droput, Batch-Norm,
    SGD+Momentum).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequencial structure (text, market movement): Use Recurrent networks (LSTM,
    SGD, Gradient clipping).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Residual/Inception networks start with the shallowest example possible.
  prefs: []
  type: TYPE_NORMAL
- en: Solving High train errors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Do the following actions on this order:'
  prefs: []
  type: TYPE_NORMAL
- en: Inspect for defects on the data. (Need human intervention)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check for software bugs on your library. (Use gradient check, it's probably
    a backprop error)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune learning rate (Make it smaller)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make network deeper. (You should start with a shallow network on the beginning)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solving High test errors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Do the following actions on this order:'
  prefs: []
  type: TYPE_NORMAL
- en: Do more data augmentation (Also try generative models to create more data)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add dropout and batch-norm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get more data (More data has more influence on Accuracy then anything else)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the late 2016 Andrew Ng [lecture](https://www.youtube.com/watch?v=F1ka6a13S9I&t=564s)
    these are the topics that we need to pay attention.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Scalability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a computing system that scale well for more data and more model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Team
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have on your team divided with with AI people and HPC (Cuda, OpenCl, etc...).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Data first
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data is more important than your model, always try to get more quality data
    before trying to change your model.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use normal data augmentation techniques plus Generative models(Unsupervised).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Make sure that Validation Set and Test set come from same distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This will avoid having a test or validation set that does not tell the reality.
    Also helps to check if your training is valid.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Have Human level performance metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a team of experts to compare with your current system performance. Also
    it drives decisions between getting more data, or making model more complex.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Data server
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a unified data-warehouse. All team must have access to data, with SSD quality
    access speed.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Using Games
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using games are cool to help augment datasets, but attention because games does
    not have the same variants of the same class as real life. For example GTA does
    not have enough car models compared to real life.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Ensembles always help
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training separately different networks and averaging their end results always
    gives some extra 2% accuracy. (Imagenet 2016 best results were simple ensambles)
  prefs: []
  type: TYPE_NORMAL
- en: 10\. What to do if you have more than 1000 classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use hierarchical Softmax to increase performance.
  prefs: []
  type: TYPE_NORMAL
- en: 11\. How many samples per class do we need to have good result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If training from scratch, use the same number of parameters. For example Model
    has 1000 parameters, so use 1000 samples per class.
  prefs: []
  type: TYPE_NORMAL
- en: If doing transfer learning is much less (Much is not defined yet, more is better).
  prefs: []
  type: TYPE_NORMAL
