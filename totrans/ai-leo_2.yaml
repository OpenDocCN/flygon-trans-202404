- en: Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Machine learning is all about using your computer to "learn" how to deal with
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习就是利用你的计算机来“学习”如何处理
- en: problems without “programming". (It’s a branch of artificial intelligence)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无需“编程”就能解决问题。（这是人工智能的一个分支）
- en: We take some data, train a model on that data, and use the trained model to
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取一些数据，对该数据进行模型训练，并使用训练好的模型来
- en: make predictions on new data. Basically is a way to make the computer create
    a
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在新数据上做出预测。基本上是一种让计算机创建一个的方法
- en: program that gives some output with a known input and that latter give a intelligent
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 给出已知输入并后来给出智能输出的程序
- en: output to a different but similar input.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 输出到一个不同但相似的输入。
- en: We need machine learning on cases that would be difficult to program by hand
    all
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要机器学习来解决一些难以手动编程的情况
- en: possible variants of a classification/prediction problem
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类/预测问题的可能变体
- en: 'The basic Idea of Machine Learning is to make the computer learn something
    from the data. Machine learning comes in two flavors:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的基本思想是让计算机从数据中学到东西。机器学习有两种味道：
- en: 'Supervised Learning: You give to the computer some pairs of inputs/outputs,
    so in the future new when new inputs are presented you have an intelligent output.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习：你给计算机一些输入/输出对，这样在未来当新输入被呈现时，你就有了一个智能的输出。
- en: 'Unsupervised Learning: You let the computer learn from the data itself without
    showing what is the expected output.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习：你让计算机从数据本身学习，而不告诉它期望的输出是什么。
- en: '![](supervised_unsupervised.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](supervised_unsupervised.png)'
- en: Examples of Supervised Learning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习示例
- en: '* * *'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Image Classification: Your train with images/labels. Then on the future when
    you give a new image expecting that the computer will recognise the new object
    (Classification)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像分类：你用图像/标签进行训练。然后在将来，当你提供一个新的图像时，期望计算机能够识别新对象（分类）。
- en: 'Market Prediction: You train the computer with historical market data, and
    ask the computer to predict the new price on the future (Regression)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 市场预测：你用历史市场数据训练计算机，并要求计算机预测未来的价格（回归）
- en: '![](Classification_Regression.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](Classification_Regression.png)'
- en: Examples of Unsupervised Learning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习示例
- en: '* * *'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Clustering: You ask the computer to separate similar data on clusters, this
    is essential in research and science.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类：你让计算机将相似的数据分开成簇，这在研究和科学中至关重要。
- en: 'High Dimension Visualisation: Use the computer to help us visualise high dimension
    data.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高维可视化：利用计算机帮助我们可视化高维数据。
- en: 'Generative Models: After a model capture the probability distribution of your
    input data, it will be able to generate more data. This is very useful to make
    your classifier more robust.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成模型：当一个模型捕获了输入数据的概率分布后，它就能够生成更多的数据。这对于使你的分类器更加稳健非常有用。
- en: '![](TSNE.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](TSNE.png)'
- en: '![](GenerativeModel.PNG)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](GenerativeModel.PNG)'
- en: Features
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征
- en: '* * *'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Imagine the following problem, you are working on a system that should classify
    if a tumour is benign or malign, at a first moment the only information that you
    have to decide is the tumour size. We can see the your training data distribution
    bellow. Observe that the characteristic (or feature) tumour size does not seems
    to be alone a good indicator to decide if the tumour is malignant or benign.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下以下问题，你正在处理一个系统，应该对肿瘤是良性还是恶性进行分类，在最初的时候，你决定的唯一信息是肿瘤的大小。我们可以看到下面的训练数据分布。注意，特征（或特征）肿瘤大小似乎不是一个单独的良性或恶性肿瘤的良好指标。
- en: '![](Features1.PNG)'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_IMG
  zh: '![](Features1.PNG)'
- en: Now consider that we add one more feature to the problem (Age).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑我们向问题中添加一个特征（年龄）。
- en: '![](Features2.PNG)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Features2.PNG)'
- en: The intuition is that with more features that are relevant to the problem that
    you want to classify, you will make your system more robust. Complex systems like
    this one could have up to thousand of features. One question that you may ask
    is how can I determine the features that are relevant to my problem. Also which
    algorithm to use best to tackle infinite amount of features, for example Support
    Vector machines have mathematical tricks that allow a very large number of features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉是，通过增加与你想要分类的问题相关的更多特征，你将使你的系统更加稳健。像这样的复杂系统可能有上千个特征。你可能会问的一个问题是，我如何确定与我的问题相关的特征。还有哪种算法最适合处理无限数量的特征，例如支持向量机有数学技巧允许处理非常大量的特征。
- en: Training
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '* * *'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The idea is to give a set of inputs and it's expected outputs, so after training
    we will have a model (hypothesis) that will then map new data to one of the categories
    trained.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是提供一组输入及其预期输出，因此训练后我们将得到一个模型（假设），然后将新数据映射到训练的类别之一。
- en: '![](Workflow1.PNG)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Workflow1.PNG)'
- en: 'Ex: Imagine that you give a set of images and the following categories, duck
    or not duck, the idea is that after training you can get an image of a duck from
    internet and the model should say "duck"'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：想象一下，你提供一组图像和以下类别，鸭子或非鸭子，想法是在训练后，你可以从互联网上获取一幅鸭子的图像，模型应该说“鸭子”。
- en: '![](DuckNoDuck.PNG)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](DuckNoDuck.PNG)'
- en: Bag of tricks
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧包
- en: There are a lot of different machine learning algorithms, on this book we will
    concentrate more on neural networks, but there is no one single best algorithm
    it all depends on the problem that you need to solve, the amount of data available.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多不同的机器学习算法，在本书中我们将更专注于神经网络，但没有一个单一最佳算法，一切取决于你需要解决的问题，可用的数据量。
- en: '![](CompareAlgos.png)![](ListAlgos.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](CompareAlgos.png)![](ListAlgos.png)'
- en: The Basic Recipe
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本配方
- en: This is the super simple recipe (maybe cover 50%), we will explain the “how”
    later but this gives some hint on how to think when dealing with a machine learning
    problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是超级简单的配方（可能覆盖50%），我们稍后会解释“如何”，但这给出了在处理机器学习问题时如何思考的一些提示。
- en: First check if your model works well on the training data, and if not make the
    model more complex (Deeper, or more neurons)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先检查你的模型在训练数据上表现如何，如果不好，让模型更复杂（更深或更多神经元）。
- en: If yes then test on the “test” data, if not you overfit, and the most reliable
    way to cure overfit is to get more data (Putting the test data on the train data
    does not count)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是，则在“测试”数据上进行测试，如果不是，你过拟合了，治愈过拟合的最可靠方法是获得更多数据（将测试数据放入训练数据中不算）
- en: By the way the biggest public image dataset (imagenet) is not big enough to
    the 1000 classes imagenet competition
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，最大的公共图像数据集（imagenet）对于1000类imagenet竞赛来说还不够大
- en: '![](BasicRecipe.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](BasicRecipe.png)'
- en: Next Chapter
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: On the next chapter we will learn the basics of Linear Algebra needed on artificial
    intelligence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习人工智能所需的线性代数基础知识。
- en: Linear Algebra
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性代数
- en: Introduction
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Linear Algebra is a omit important topic to understand, a lot of deep learning
    algorithms use it, so this chapter will teach the topics needed to understand
    what will come next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是一个非常重要的主题，很多深度学习算法都在使用它，因此本章将教授理解接下来内容所需的主题。
- en: Scalars, Vectors and Matrices
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量、向量和矩阵
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Scalars: A single number'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量：一个单一的数字
- en: 'Vector: Array of numbers, where each element is identified by an single index'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量：一组数字的数组，其中每个元素由单个索引标识
- en: 'Matrix: Is a 2D array of numbers, bellow we have a (2-row)X(3-col) matrix.
    On matrix a single element is identified by two indexes instead of one'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵：是一个数字的2D数组，下面是一个（2行）X（3列）矩阵。在矩阵中，一个单独的元素由两个索引而不是一个索引标识。
- en: '![](scalar-vector-matrix.gif)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](scalar-vector-matrix.gif)'
- en: '![](enter-the-matrix-10-638.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](enter-the-matrix-10-638.jpg)'
- en: Here we show how to create them on matlab and python(numpy)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示如何在matlab和python（numpy）中创建它们
- en: '![](MatlabSimpleMatrix.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](MatlabSimpleMatrix.png)'
- en: '![](PythonSimpleMatrix.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](PythonSimpleMatrix.png)'
- en: Matrix Operations
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵运算
- en: Here we will show the important operations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将展示重要的操作。
- en: Transpose
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转置
- en: '* * *'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you have an image(2d matrix) and multiply with a rotation matrix, you will
    have a rotated image. Now if you multiply this rotated image with the transpose
    of the rotation matrix, the image will be "un-rotated"
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一幅图像（2D矩阵）并与一个旋转矩阵相乘，你将得到一个旋转后的图像。现在如果你将这个旋转后的图像与旋转矩阵的转置相乘，图像将被“取消旋转”。
- en: Basically transpose a matrix is to swap it's rows and cols. Or rotate the matrix
    around it's main diagonal.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，转置一个矩阵是交换它的行和列。或者围绕其主对角线旋转矩阵。
- en: '![](MatrixTranspose.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](MatrixTranspose.jpg)'
- en: '![](TransposeAnim.gif)![](Transpose2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](TransposeAnim.gif)![](Transpose2.png)'
- en: Addition/Subtraction
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加法/减法
- en: '* * *'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Basically we add 2 matrices by adding each element with the other. Both matrices
    need to have same dimension
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们通过将每个元素与另一个元素相加来添加2个矩阵。两个矩阵需要具有相同的维度。
- en: '![](matrix-addition.gif)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](matrix-addition.gif)'
- en: '![](matrix-subtraction.gif)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](matrix-subtraction.gif)'
- en: Multiply by scalar
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乘以标量
- en: '* * *'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Multiply all elements of the matrix by a scalar
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵的所有元素乘以一个标量
- en: '![](matrix-multiply-constant.gif)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](matrix-multiply-constant.gif)'
- en: Matrix Multiplication
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The matrix product of an n×m matrix with an m×ℓ matrix is an n×ℓ matrix. The
    (i,j) entry of the matrix product AB is the dot product of the ith row of A with
    the jth column of B.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: n×m 矩阵与 m×ℓ 矩阵的矩阵积是一个 n×ℓ 矩阵。矩阵积 AB 的（i，j）条目是矩阵 A 的第 i 行与矩阵 B 的第 j 列的点积。
- en: The number of columns on the first matrix must match the number of rows on the
    second matrix. The result will be another matrix or a scalar with dimensions defined
    by the rows of the first matrix and columns of the second matrix.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个矩阵的列数必须与第二个矩阵的行数匹配。结果将是另一个矩阵或由第一个矩阵的行和第二个矩阵的列定义的标量。
- en: '![](MatrixMultiplication_Check.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](MatrixMultiplication_Check.png)'
- en: Basically the operation is to "dot product" each row of the first matrix(k)
    with each column of the second matrix(m).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，操作是将第一个矩阵（k）的每一行与第二个矩阵（m）的每一列进行“点乘”。
- en: '![](MatrixMultplyMovement.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](MatrixMultplyMovement.png)'
- en: Some examples
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子
- en: '![](MatrixMultiplication_Example.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](MatrixMultiplication_Example.png)'
- en: '![](Matrix_Multiply_0.gif)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](Matrix_Multiply_0.gif)'
- en: '![](Matrix_Multiply_1.gif)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](Matrix_Multiply_1.gif)'
- en: '![](Matrix_Multiply_2.gif)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Matrix_Multiply_2.gif)'
- en: Commutative property
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交换律
- en: Matrix multiplication is not always commutative ![](bcbe2544.png), but the dot
    product between 2 vectors is commutative, ![](b7244efc.png).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法不总是交换的 ![](bcbe2544.png)，但两个向量的点积是交换的，![](b7244efc.png)。
- en: Types of Matrix
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵的类型
- en: '* * *'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: There are some special matrices that are interesting to know.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些特殊的矩阵是有趣的。
- en: 'Identity: If you multiply a matrix B by the identity matrix you will have the
    matrix B as result, the diagonal of the identity matrix is filled with ones, all
    the rest are zeros.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单位矩阵：如果你用单位矩阵乘以一个矩阵 B，你将得到矩阵 B 作为结果，单位矩阵的对角线填满了 1，其余全部填满了 0。
- en: 'Inverse: Used on matrix division and to solve linear systems.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆：用于矩阵除法和解线性系统。
- en: '![](matrix-identity.gif)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](matrix-identity.gif)'
- en: Tensors
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量
- en: '* * *'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sometimes we need to organize information with more than 2 dimensions, we called
    tensor an n-dimension array.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要用多于 2 个维度来组织信息，我们将 n 维数组称为张量。
- en: For example an 1d tensor is a vector, a 2d tensor is a matrix, a 3d tensor is
    a 3d tensor is a cube, and a 4d tensor is an vector of cubes, a 5d tensor is a
    matrix of cubes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个 1 维张量是一个向量，一个 2 维张量是一个矩阵，一个 3 维张量是一个立方体，一个 4 维张量是一个立方体的向量，一个 5 维张量是一个立方体的矩阵。
- en: '![](Tensor_2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](Tensor_2.png)'
- en: '![](Tensor_1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](Tensor_1.png)'
- en: Practical example
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际例子
- en: Here we will show to use matrix multiplication to implement a linear classifier.
    Don't care now about what linear classifier does, just pay attention that we use
    linear algebra do solve it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们将展示如何使用矩阵乘法来实现线性分类器。现在不用担心线性分类器的作用是什么，只要注意我们使用线性代数来解决它即可。
- en: '![](Linear_1.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](Linear_1.jpg)'
- en: Merging the weights and bias (bias trick) to solve the linear classification
    as a single matrix multiplication
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将权重和偏置（偏置技巧）合并以解决线性分类问题作为单个矩阵乘法
- en: '![](BiasTrick.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](BiasTrick.jpeg)'
- en: On Matlab
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Matlab
- en: '![](MatlabLinearClassifer.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](MatlabLinearClassifer.png)'
- en: On Python
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python
- en: '![](PythonLinearClassifer.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](PythonLinearClassifer.png)'
- en: Next Chapter
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: The next chapter we will learn about Linear Classification.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习线性分类。
- en: Supervised Learning
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Introduction
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In this chapter we will learn about Supervised learning as well as talk a bit
    about Cost Functions and Gradient descent. Also we will learn about 2 simple algorithms:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习有监督学习，同时稍微谈一下成本函数和梯度下降。我们还将学习两个简单的算法：
- en: Linear Regression (for Regression)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归（用于回归）
- en: Logistic Regression (for Classification)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归（用于分类）
- en: The first thing to learn about supervised learning is that every sample data
    point x has an expected output or label y, in other words your training is composed
    of ![](1241931f.png) pairs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 关于有监督学习的第一件事是，每个样本数据点 x 都有一个预期输出或标签 y，换句话说，你的训练由 ![](1241931f.png) 对组成。
- en: 'For example consider the table below:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如考虑下表：
- en: '| Size in feet (x) | Price (y) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸（x） | 价格（y） |'
- en: '| --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 2104 | 460 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2104 | 460 |'
- en: '| 1416 | 232 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1416 | 232 |'
- en: '| 1534 | 315 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 1534 | 315 |'
- en: '| 852 | 178 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 852 | 178 |'
- en: This table (or training set) shows the sizes of houses along with their price.
    So a house of size 2104 feet costs 460\.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表（或训练集）显示了房屋尺寸以及它们的价格。所以一个尺寸为 2104 平方英尺的房子价格是 460 美元。
- en: 'The idea is that we could use data like this to create models that can predict
    some outcome (ie: Price) from different inputs (ie: Size in feet).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是我们可以使用这样的数据来创建能够从不同输入（即：尺寸）预测某种结果（即：价格）的模型。
- en: Linear Regression
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: '* * *'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Regression is about returning a continuous scalar number from some input. The
    model or hypothesis that we will learn will predict a value following a linear
    rule. However sometimes a linear model is not enough to capture the underlying
    nature of your data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是关于从某些输入返回连续的标量数的。我们将学习的模型或假设将按线性规则预测一个值。然而，有时一个线性模型不足以捕捉数据的潜在特性。
- en: '![](LinearRegExample.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](LinearRegExample.png)'
- en: 'Hypothesis:'
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假设：
- en: Basically linear regression trys to create a line ![](e8e26b41.png), that fits
    the data on training, e.g.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，线性回归试图创建一条线，以适应训练数据，例如：
- en: '![](af468600.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](af468600.png)'
- en: '![](83f7d33e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](83f7d33e.png)'
- en: The whole idea of supervised learning is that we try to learn the best parameters
    (theta in this case) from our training set.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的整个思想是，我们尝试从我们的训练集中学习最佳参数（在这种情况下是 theta）。
- en: Cost Function
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本函数
- en: Before we talk about how to learn the parameters (also called weights) of our
    hypothesis we need to know how to evaluate if our current set of weights are already
    doing a good job. The function that does this job is called Loss or Cost function.
    Basically it will return a scalar value between 0(No error) and infinity (Really
    bad).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何学习假设的参数（也称为权重）之前，我们需要知道如何评估我们当前的权重是否已经做得很好。执行这项工作的函数称为损失函数或成本函数。基本上它将返回一个介于
    0（无误差）和无穷大（非常糟糕）之间的标量值。
- en: 'An example of such a function is given below:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个函数的示例如下：
- en: '![](6e23d77c.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](6e23d77c.png)'
- en: where
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: 'm: Number of items in your dataset (i): i-th element of your dataset y: Label(expected
    value) in dataset'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'm: 数据集中的项数（i）：数据集中的第 i 个元素 y：数据集中的标签（期望值）'
- en: This paticular cost function is called mean-squared error loss, and is actually
    very useful for regression problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的成本函数称为均方误差损失，实际上对于回归问题非常有用。
- en: During training we want to minimise our loss by continually changing the theta
    parameters. Another nice feature of this paticular function is that it is a convex
    function so it is guaranteed to have no more than one minimum, which will also
    be it's global minimum. This makes it easier for us to optimise.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们希望通过不断改变 theta 参数来最小化我们的损失。这个特定函数的另一个好处是它是一个凸函数，因此保证不会有多于一个最小值，这也将是它的全局最小值。这使得我们更容易优化。
- en: 'Our task is therefor to find:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是找到：
- en: '![](738d7cd7.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](738d7cd7.png)'
- en: Gradient descent
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: '* * *'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Gradient descent is a simple algorithm that will try to find the local minimum
    of a function. We use gradient descent to minimise our loss function. One important
    feature to observe on gradient descent is that it will more often than not get
    stuck into the first local minimum that it encounters. However there is no guarantee
    that this local minimum it finds is the best (global) one.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种简单的算法，它会试图找到函数的局部最小值。我们使用梯度下降来最小化我们的损失函数。梯度下降的一个重要特征是，它往往会陷入它遇到的第一个局部最小值中。然而，并不能保证它找到的这个局部最小值是最好（全局）的。
- en: '![](GradDecAlgo.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](GradDecAlgo.png)'
- en: The gradient descent needs the first derivative of the function that you want
    to minimise. So if we want to minimise some function by changing parameters, you
    need to derive this function with respect to these parameters.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降需要你想要最小化的函数的一阶导数。所以，如果我们想通过改变参数来最小化某个函数，你需要针对这些参数对这个函数进行求导。
- en: One thing to note is as this algorithm will execute on all samples in your training
    set it does not scale well for bigger datasets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，由于此算法将在训练集中的所有样本上执行，因此对于更大的数据集来说，它不会很好地扩展。
- en: Simple example
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单的例子
- en: 'Bellow we have some simple implementation in matlab that uses gradient descent
    to minimise the following function:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的在 Matlab 中使用梯度下降来最小化以下函数的实现：
- en: '![](cc7a762a.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](cc7a762a.png)'
- en: 'It''s derivative with respect to x is:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 它相对于 x 的导数是：
- en: '![](e1c43b85.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](e1c43b85.png)'
- en: 'The code to find at what point our local minimum occurs is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 找到我们的局部最小值出现的点的代码如下所示：
- en: '[PRE0]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Gradient descent for linear regression
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归的梯度下降
- en: In order to use gradient descent for linear regression you need to calculate
    the derivative of it's loss (means squared error) with respect to it's parameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用梯度下降进行线性回归，你需要计算它的损失（均方误差）相对于它的参数的导数。
- en: 'This derivative will be:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数将会是：
- en: '![](5c0d652.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](5c0d652.png)'
- en: Logistic Regression
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '* * *'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The name may sound confusing but actually Logistic Regression is simply all
    about classification. For instance consider the example below where we want to
    classify 2 classes: x''s and o''s. Now our output y will have two possible values
    [0,1].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 名字可能听起来让人困惑，但实际上 Logistic 回归只是关于分类的一切。例如，考虑下面的例子，我们想要对 2 类进行分类：x 和 o。现在我们的输出
    y 将有两个可能的值 [0,1]。
- en: Normally we do not use Logistic Regression if we have a large number of features
    (e.g. more than 100 features).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果我们有大量特征（例如超过 100 个特征），我们就不使用 Logistic 回归。
- en: '![](Image%20%5b5%5d.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](Image%20%5b5%5d.png)'
- en: Hypothesis
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假设
- en: Our hypothesis is almost the same compared to the linear regression however
    the difference is that now we use a function that will force our output to give
    ![](e640a3a9.png)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设与线性回归几乎相同，然而区别在于现在我们使用一个函数来强制我们的输出给出 ![](e640a3a9.png)
- en: '![](7c76275d.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](7c76275d.png)'
- en: where
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](f5fb1593.png) is the logistic or sigmoid function, therefore'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![](f5fb1593.png) 是逻辑或 Sigmoid 函数，因此'
- en: '![](9f0ba3d5.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](9f0ba3d5.png)'
- en: Here the sigmoid function will convert a scalar number to some probability between
    0 and 1.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 Sigmoid 函数将一个标量数转换为介于 0 和 1 之间的概率。
- en: Cost function for classification
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类的成本函数
- en: When dealing with classification problems, we should use a different cost/loss
    function. A good candidate for classification is the cross-entropy cost function.
    By the way this function can be found by using the Maximum Likelihood estimation
    method.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 处理分类问题时，我们应该使用不同的成本/损失函数。用于分类的一个好选择是交叉熵成本函数。顺便说一句，通过使用最大似然估计方法可以找到这个函数。
- en: 'The cross-entropy cost function is:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵成本函数是：
- en: '![](a5d0adaa.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](a5d0adaa.png)'
- en: Again our objective is to find the best parameter theta that minimize this function,
    so to use gradient descent we need to calculate the derivative of this function
    with respect to theta
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们的目标是找到最小化这个函数的最佳参数 theta，因此为了使用梯度下降，我们需要计算这个函数相对于 theta 的导数
- en: '![](5c0d652.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](5c0d652.png)'
- en: One cool thing to note is that the derivative is the same as the derivative
    from linear regression.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的一个很酷的事情是，导数与线性回归的导数相同。
- en: Overfitting (Variance) and Underfitting (Bias)
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合（方差）和欠拟合（偏差）
- en: The main idea of training in machine learning is to let the computer learn the
    underlying structure of the training set and not just the specific training set
    that it sees. If it does not learn the underlying structure and instead learns
    just the structure of the training samples then we say our model has overfit.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中训练的主要思想是让计算机学习训练集的潜在结构，而不仅仅是它所见到的具体训练集。如果它没有学习到潜在结构，而是只学习到了训练样本的结构，那么我们就说我们的模型过拟合了。
- en: 'We can tell overfitting has occured when our model has a very good accuracy
    on the training set (e.g.: 99.99%) but does not perform that well on the test
    set (e.g.: 60%).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的模型在训练集上有非常高的准确率（例如：99.99%），但在测试集上表现不佳（例如：60%）时，我们可以判断出现了过拟合。
- en: This basically occurs when your model is too complex in relation to the available
    data, and/or you don't have enough data to capture the underlying data pattern.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型过于复杂相对于可用数据时，或者你没有足够的数据来捕捉潜在的数据模式时，基本上就会发生这种情况。
- en: The opposite of this problem is called Underfitting, basically it happens when
    your model is too simple to capture the concept given in the training set.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的相反称为欠拟合，基本上是指当你的模型过于简单而无法捕捉训练集中给定的概念时发生的。
- en: 'Some graphical examples of these problems are shown below:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的一些图形示例如下：
- en: '**Left:** Underfit'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**左侧：** 欠拟合'
- en: '**Middle:** Perfect'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**中间：** 完美'
- en: '**Right:** Overfit'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**右侧：** 过拟合'
- en: '![](UnderOverFit.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](UnderOverFit.jpg)'
- en: Solving Overfitting
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决过拟合
- en: Get more data
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更多数据
- en: Use regularisation
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化
- en: Check other model architectures
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查其他模型架构
- en: Solving Underfitting
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决欠拟合
- en: Add more layers or more parameters
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多层或更多参数
- en: Check other architectures
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查其他架构
- en: Regularization
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: '* * *'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: It's a method that helps overfitting by forcing your hypothesis parameters to
    have nice small values and to force your model to use all the available parameters.
    To use regularization you just need to add an extra term to your cost/loss function
    during training. Below we have an example of the Mean squared error loss function
    with an added regularization term.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种通过强制假设参数具有良好小值并迫使模型使用所有可用参数来帮助过拟合的方法。要使用正则化，你只需在训练过程中向你的成本/损失函数中添加一个额外的项。下面我们有一个带有额外正则化项的均方误差损失函数的示例。
- en: '![](80fcd243.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](80fcd243.png)'
- en: Also the regularized version of the cross-entropy loss function
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 还有交叉熵损失函数的正则化版本
- en: '![](185df188.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](185df188.png)'
- en: This term will basically multiply by ![](41d912dc.png) all parameters ![](5a7fef68.png)
    from your hypothesis
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项基本上将通过![](41d912dc.png)将你的假设的所有参数![](5a7fef68.png)相乘。
- en: Normally we don't need to regularise the bias term of our hypothesis.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不需要对我们假设的偏差项进行正则化。
- en: During gradient descent you also need to calculate the derivative of those terms.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降期间，您还需要计算这些项的导数。
- en: Intuition
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直觉
- en: You can imagine that besides forcing the weights to have lower values, the regularisation
    will also spread the "concept" across more weights. One way to think about what
    happens when we regularise is shown below.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象，除了强制权重具有较低的值之外，正则化还将把“概念”分散到更多的权重上。当我们进行正则化时，可以通过以下方式来思考会发生什么。
- en: For our input x we can have 2 weight vectors w1 or w2\. When we multiply our
    input with our weight vector we will get the same output of 1 for each. However
    the weight vector w2 is a better choice of weight vector as this will look at
    more of our input x compared to w1 which only looks at one value of x.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的输入x，我们可以有2个权重向量w1或w2。当我们用我们的权重向量乘以我们的输入时，我们将得到每个输出的相同输出1。然而，权重向量w2是更好的权重向量选择，因为它将查看我们的输入x的更多内容，而w1只查看x的一个值。
- en: '![](cadca0ba.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](cadca0ba.png)'
- en: In practice this might reduce performance on our training set but will improve
    generalisation and thus performance when testing.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这可能会降低我们训练集的性能，但会提高泛化性，从而在测试时提高性能。
- en: Non-Linear Hypothesis
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性假设
- en: '* * *'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sometimes our hypothesis needs to have non-linear terms to be able to predict
    more complex classification/regression problems. We can for instance use Logistic
    Regression with quadratic terms to capture this complexity. As mentioned earlier
    this does not scale well for large number of features.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们的假设需要具有非线性项，以便能够预测更复杂的分类/回归问题。例如，我们可以使用具有二次项的逻辑回归来捕捉这种复杂性。正如前面提到的，这对于特征数量较多的情况不具备良好的可扩展性。
- en: '![](NonLinearFeatures.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](NonLinearFeatures.png)'
- en: For instance if we would like to classify a 100x100 grayscale image using logistic
    regression we would need 50 million parameters to include the quadratic terms.
    Training with this number of features will need at least 10x more images (500
    millions) to avoid overfitting. Also the computational cost will be really high.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想要使用逻辑回归对一个100x100的灰度图像进行分类，我们将需要5000万个参数来包括二次项。使用这么多特征进行训练将至少需要10倍的图像（5亿）。此外，计算成本将非常高。
- en: In the next chapters we will learn other algorithms that scale better for bigger
    number of features.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习其他更适合具有更多特征的算法。
- en: Local minimas
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部最小值
- en: '* * *'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As we increase the number of parameters on our model our loss function will
    start to find multiple local minima. This can be a problem for training because
    the gradient descent can get stuck in one of them.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们增加模型的参数数量时，我们的损失函数将开始找到多个局部最小值。这对于训练来说可能是一个问题，因为梯度下降可能会陷入其中一个最小值中。
- en: '![](sgd_stuck.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](sgd_stuck.png)'
- en: 'Actually some recent papers also try to prove that some of the methods used
    today (ie: Deep Neural networks) the local-minima is actually really close to
    the global minima, so you don''t need to care about using a Convex loss or about
    getting stuck in a local-minima.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，一些最近的论文还试图证明今天使用的某些方法（例如：深度神经网络）的局部最小值实际上非常接近全局最小值，因此您不需要关心使用凸损失或者陷入局部最小值的问题。
- en: '![](DeeperLocalMinimas.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](DeeperLocalMinimas.jpg)'
- en: Neural Networks
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: Introduction
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Family of models that takes a very “loose” inspiration from the brain, used
    to approximate functions that depends on a large number of inputs. (Is a very
    good Pattern recognition model).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一族模型，其灵感来源于大脑，用于近似取决于大量输入的函数。（是一个非常好的模式识别模型）。
- en: Neural networks are examples of Non-Linear hypothesis, where the model can learn
    to classify much more complex relations. Also it scale better than Logistic Regression
    for large number of features.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是非线性假设的示例，模型可以学习分类更复杂的关系。而且它对于具有大量特征的情况比逻辑回归更具可扩展性。
- en: 'It''s formed by artificial neurons, where those neurons are organised in layers.
    We have 3 types of layers:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 它由人工神经元形成，其中这些神经元被组织成层。我们有3种类型的层：
- en: Input layer
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layers
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Output layer
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: We classify the neural networks from their number of hidden layers and how they
    connect, for instance the network above have 2 hidden layers. Also if the neural
    network has/or not loops we can classify them as Recurrent or Feed-forward neural
    networks.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从隐藏层数量以及它们的连接方式来对神经网络进行分类，例如上面的网络有2个隐藏层。此外，如果神经网络有/没有循环，我们可以将它们分类为循环或前馈神经网络。
- en: Neural networks from more than 2 hidden layers can be considered a deep neural
    network. The advantage of using more deep neural networks is that more complex
    patterns can be recognised.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 具有超过2个隐藏层的神经网络可以被认为是深度神经网络。使用更深层的神经网络的优势在于可以识别更复杂的模式。
- en: '![](neuralnetworks.png)'
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_IMG
  zh: '![](neuralnetworks.png)'
- en: Bellow we have an example of a 2 layer feed forward artificial neural network.
    Imagine that the connections between neurons are the parameters that will be learned
    during training. On this example Layer L1 will be the input layer, L2/L3 the hidden
    layer and L4 the output layer
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们有一个两层前馈人工神经网络的示例。想象一下神经元之间的连接是训练期间将要学习的参数。在这个例子中，层L1将是输入层，L2/L3是隐藏层，L4是输出层。
- en: '![](NeuralNetwork.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](NeuralNetwork.png)'
- en: Brain Differences
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大脑差异
- en: '* * *'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Now wait before you start thinking that you can just create a huge neural network
    and call strong AI, there are some few points to remember:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在你开始认为你可以创建一个巨大的神经网络并称之为强人工智能之前，请等一等，有一些要记住的要点：
- en: 'Just a list:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列个清单：
- en: The artificial neuron fires totally different than the brain
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经元的激活方式与大脑完全不同
- en: A human brain has 100 billion neurons and 100 trillion connections (synapses)
    and operates on 20 watts(enough to run a dim light bulb) - in comparison the biggest
    neural network have 10 million neurons and 1 billion connections on 16,000 CPUs
    (about 3 million watts)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类大脑有1000亿个神经元和100万亿个连接（突触），并且以20瓦的功率运行（足以点亮一个暗淡的灯泡）- 相比之下，最大的神经网络有1000万个神经元和10亿个连接，需要16000个CPU（约300万瓦）
- en: The brain is limited to 5 types of input data from the 5 senses.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大脑仅限于来自5种感官的5种类型的输入数据。
- en: Children do not learn what a cow is by reviewing 100,000 pictures labelled “cow”
    and “not cow”, but this is how machine learning works.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孩子们并不是通过审阅10万张标有“牛”的图片和“非牛”的图片来学习什么是牛的，但这就是机器学习的工作原理。
- en: Probably we don't learn by calculating the partial derivative of each neuron
    related to our initial concept. (By the way we don't know how we learn)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或许我们并不是通过计算与我们最初概念相关的每个神经元的偏导数来学习。（顺便说一句，我们不知道我们是如何学习的）
- en: Real Neuron
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真实神经元
- en: '![](neuron.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](neuron.png)'
- en: Artificial Neuron
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经元
- en: '* * *'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](neuron_model.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](neuron_model.jpeg)'
- en: The single artificial neuron will do a dot product between w and x, then add
    a bias, the result is passed to an activation function that will add some non-linearity.
    The neural network will be formed by those artificial neurons.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 单个人工神经元将对w和x进行点积，然后添加一个偏置，结果传递给一个激活函数，该函数将添加一些非线性。神经网络将由这些人工神经元组成。
- en: The non-linearity will allow different variations of an object of the same class
    to be learned separately. Which is a different behaviour compared to the linear
    classifier that tries to learn all different variations of the same class on a
    single set of weights. More neurons and more layers is always better but it will
    need more data to train.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性将允许学习同一类别的不同变化的不同版本。这与试图在单一权重集上学习同一类别的所有不同变化的线性分类器的行为不同。更多的神经元和更多的层总是更好的，但需要更多的数据来训练。
- en: Each layer learn a concept, from it's previous layer. So it's better to have
    deeper neural networks than a wide one. (Took 20 years to discover this)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层从它的前一层学习一个概念。因此，拥有更深层的神经网络比拥有更宽的神经网络更好。（花了20年的时间才发现这一点）
- en: Activation Functions
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: '* * *'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: After the neuron do the dot product between it's inputs and weights, it also
    apply a non-linearity on this result. This non-linear function is called Activation
    Function.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经元对其输入和权重执行点积后，它还会在这个结果上应用一个非线性。这个非线性函数称为激活函数。
- en: On the past the popular choice for activation functions were the sigmoid and
    tanh. Recently it was observed the ReLU layers has better response for deep neural
    networks, due to a problem called vanishing gradient. So you can consider using
    only ReLU neurons.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，激活函数的流行选择是Sigmoid和tanh。最近观察到ReLU层对深度神经网络有更好的响应，这是由于一个称为梯度消失的问题。因此，您可以考虑只使用ReLU神经元。
- en: '![](ffa45100.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](ffa45100.png)'
- en: '![](ActivationFunctions.PNG)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](ActivationFunctions.PNG)'
- en: Example of simple network
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单网络的示例
- en: Consider the Neural network bellow with 1 hidden layer, 3 input neurons, 3 hidden
    neurons and one output neuron.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑下面具有 1 个隐藏层、3 个输入神经元、3 个隐藏神经元和一个输出神经元的神经网络。
- en: '![](SimpleANN.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](SimpleANN.png)'
- en: We can define all the operation that this network will do as follows.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义该网络将执行的所有操作如下。
- en: '![](2c729b18.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](2c729b18.png)'
- en: Here ![](9d409d72.png) means the activation(output) of the first neuron of layer
    2 (hidden layer on this case). The first layer, (input layer) can be considered
    as ![](3d270619.png) and it's values are just the input vector.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![](9d409d72.png) 表示第 2 层（在本例中为隐藏层）第一个神经元的激活（输出）。第一层（输入层）可以被视为 ![](3d270619.png)，它的值只是输入向量。
- en: 'Consider the connections between each layer as a matrix of parameters. Consider
    those matrices as the connections between layers. On this case we have to matrices:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 将每一层之间的连接视为参数矩阵。将这些矩阵视为层之间的连接。在这种情况下，我们有两个矩阵：
- en: '![](afc31031.png): Map the layer 1 to layer 2 (Input and Hidden layer)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](afc31031.png)：将第 1 层映射到第 2 层（输入层和隐藏层）'
- en: '![](e286ad01.png): Map layer 2 to to layer 2 (Hidden and output layer)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](e286ad01.png)：将第 2 层映射到第 2 层（隐藏层和输出层）'
- en: 'Also you consider the dimensions of ![](afc31031.png) as [number of neurons
    on layer 2] x [Number of neurons layer 1 +1]. In other words:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑 ![](afc31031.png) 的维度，作为 [第 2 层的神经元数量] x [第 1 层的神经元数量 + 1]。换句话说：
- en: '![](2fa47a1a.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](2fa47a1a.png)'
- en: 'Where:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](a72adc56.png): Number of neurons on next layer'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![](a72adc56.png)：下一层的神经元数量'
- en: '![](e8716ce7.png): Number of neurons on the current layer + 1'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![](e8716ce7.png)：当前层的神经元数量 + 1'
- en: Notice that this is only true if we consider to add the bias as part of our
    weight matrices, this could depend from implementation to implementation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有在我们考虑将偏置添加为权重矩阵的一部分时，这才成立，这可能因实现而异。
- en: Why is better than Logistic Regression
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么比 logistic 回归更好
- en: '* * *'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Consider that the neural network as a cascaded chain of logistic regression,
    where the input of each layer is the output of the previous one. Another way to
    think on this is that each layer learn a concept with the output of the previous
    layer.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络视为一系列级联的 logistic 回归，其中每一层的输入是前一层的输出。另一种思考方式是，每一层学习了前一层输出的一个概念。
- en: '![](DeepConcept.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](DeepConcept.png)'
- en: This is nice because the layer does not need to learn the whole concept at once,
    but actually build a chain of features that build that knowledge.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，因为层不需要一次性学习整个概念，而实际上是构建了一个构建该知识的特征链。
- en: Vectorized Implementation
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化实现
- en: '* * *'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: You can calculate the output of the whole layer ![](af123120.png)as a matrix
    multiplication followed by a element-wise activation function. This has the advantage
    of performance, considering that you are using tools like Matlab, Numpy, or also
    if you are implementing on hardware.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以计算整个层的输出 ![](af123120.png) 作为矩阵乘法，然后是逐元素激活函数。这具有性能上的优势，考虑到你在使用像 Matlab、Numpy
    这样的工具，或者如果你正在硬件上实现。
- en: The mechanism of calculating the output of each layer with the output of the
    previous layer, from the beginning(input layer) to it's end(output layer) is called
    forward propagation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 计算从开始（输入层）到结束（输出层）的每一层的输出与前一层的输出的机制被称为前向传播。
- en: '![](ForwardPropagation.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](ForwardPropagation.png)'
- en: 'To better understanding let''s break the activation of some layer as following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们将某些层的激活拆分如下：
- en: '![](b1457857.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](b1457857.png)'
- en: So using this formulas you can calculate the activation of each layer.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用这些公式，你可以计算每一层的激活。
- en: Multiple class problems
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类问题
- en: '* * *'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On the multi-class classification problem you need to allocate one neuron for
    each class, than during training you provide a one-hot vector for each one of
    your desired class. This is somehow easier than the logistic regression one-vs-all
    training.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类别分类问题中，你需要为每个类别分配一个神经元，然后在训练过程中，为每个期望的类别提供一个独热向量。这在某种程度上比 logistic 回归的一对一训练更容易。
- en: '![](MultiClassProb.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](MultiClassProb.png)'
- en: Cost function (Classification)
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本函数（分类）
- en: '* * *'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The cost function of neural networks, it''s a little more complicated than
    the logistic regression. So for classification on Neural networks we should use:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的成本函数，比 logistic 回归要复杂一些。因此，对于神经网络的分类，我们应该使用：
- en: '![](6a7b9836.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](6a7b9836.png)'
- en: 'Where:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: 'L: Number of layers'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L：层数
- en: 'm: Dataset size'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: m：数据集大小
- en: 'K: Number of classes'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K：类别数
- en: '![](bd75c77e.png): Number of neurons (not counting bias) from layer l'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](bd75c77e.png)：第 l 层的神经元数量（不包括偏置）'
- en: 'During training we need to calculate the partial derivative of this cost function
    with respect to each parameter on your neural network. Actually what we need to
    compute is:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们需要计算神经网络上每个参数的此成本函数的偏导数。实际上，我们需要计算的是：
- en: The loss itself (Forward-propagation)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失本身（前向传播）
- en: The derivative of the loss w.r.t each parameter (Back-propagation)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失相对于每个参数的导数（反向传播）
- en: Backpropagation
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: '* * *'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Backpropagation it's an efficient algorithm that helps you calculate the derivative
    of the cost function with respect to each parameter of the neural network. The
    name backpropagation comes from the fact that now we start calculating errors
    from all your neurons from the output layer to the input layer direction. After
    those errors are calculated we simply multiply them by the activation calculated
    during forward propagation.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种有效的算法，可以帮助您计算神经网络每个参数的成本函数的导数。反向传播这个名字来自于现在我们从输出层向输入层方向开始计算所有神经元的错误。这些错误计算完成后，我们只需将它们与前向传播期间计算的激活相乘即可。
- en: '![](BackPropNeuralNet.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](BackPropNeuralNet.png)'
- en: Doing the backpropagation (Vectorized)
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行反向传播（矢量化）
- en: As mentioned the backpropagation will flow on the reverse order iterating from
    the last layer. So starting from the output layer we calculate the output layer
    error.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，反向传播将按照相反的顺序流动，从最后一层开始迭代。所以从输出层开始，我们计算输出层的错误。
- en: The "error values" for the last layer are simply the differences of our actual
    results in the last layer and the correct outputs in y.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层的“误差值”简单地是最后一层中我们的实际结果与 y 中的正确输出的差异。
- en: '![](c4959933.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](c4959933.png)'
- en: Where
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](72faac05.png): Expected output from training'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](72faac05.png)：训练的期望输出'
- en: '![](7ace1459.png): Network output/activation of the last L layer'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](7ace1459.png)：网络输出/最后 L 层的激活'
- en: For all other layers (layers before the last layer until the input)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有其他层（从最后一层之前的层到输入层）
- en: '![](4c4e4cd5.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](4c4e4cd5.png)'
- en: Where
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](70fbe2a7.png): Error of layer l'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](70fbe2a7.png)：第 l 层的错误'
- en: '![](7b18f6d2.png): Derivative of activation function'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](7b18f6d2.png)：激活函数的导数'
- en: '![](7893cf17.png): Pre-activation of layer l'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](7893cf17.png)：第 l 层的预激活'
- en: '.*: Element wise multiplication'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: .*：逐元素相乘
- en: 'After all errors(delta) are calculated we need to actually calculate the derivative
    of the loss, which is the product of the error times the activation of each respective
    neuron:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完所有误差（delta）之后，我们需要实际计算损失的导数，即每个相应神经元的误差与激活的乘积：
- en: '![](9c286de2.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](9c286de2.png)'
- en: Now we're ignoring the regularisation term.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们忽略正则化项。
- en: Complete algorithm
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整算法
- en: Bellow we describe the whole procedure in pseudo-code
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们用伪代码描述整个过程
- en: '![](AlgoBackProp.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](AlgoBackProp.png)'
- en: Gradient Checking
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度检查
- en: '* * *'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In order to verify if your backpropagation code is right we can estimate the
    gradient, using other algorithm, unfortunately we cannot use this algorithm in
    practice because it will make the training slow, but we can use to compare it''s
    results with the backpropagation. Basically we will calculate numerically the
    derivative of the loss with respect to each parameter by calculating the loss
    and adding a small perturbation (ie: ![](2af86d7a.png)) to each parameter one
    at a time.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证您的反向传播代码是否正确，我们可以使用其他算法来估计梯度，不幸的是，我们不能在实践中使用此算法，因为它会使训练变慢，但我们可以用它来比较其结果与反向传播。基本上，我们将通过分别对每个参数添加一个小扰动（即：![](2af86d7a.png)）来数值计算损失相对于每个参数的导数。
- en: '![](GradientCheck.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](GradientCheck.jpg)'
- en: '![](1151abb.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](1151abb.png)'
- en: Actually you will compare this gradient with the output of the backpropagation
    ![](d94e9f68.png).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您将比较此梯度与反向传播的输出！[](d94e9f68.png)。
- en: Again this will be really slow because we need to calculate the loss again with
    this small perturbation twice for each parameter.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，这将非常缓慢，因为我们需要为每个参数计算两次带有此小扰动的损失。
- en: For example suppose that you have 3 parameters ![](75a959e9.png)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您有 3 个参数！[](75a959e9.png)
- en: '![](c1fe604b.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](c1fe604b.png)'
- en: '![](765311e0.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](765311e0.png)'
- en: '![](62de42ed.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](62de42ed.png)'
- en: Weights initialization
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重初始化
- en: '* * *'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The way that you initialize your network parameters is also important, you cannot
    for instance initialize all your weights to zero, normally you want to initialize
    them with small random values on the range ![](3ab3a524.png) but somehow also
    take into account that you don't want to have some sort of symmetry of the random
    values between layers.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化网络参数的方式也很重要，例如你不能将所有权重初始化为零，通常你希望将它们初始化为在范围 ![](3ab3a524.png) 内的小随机值，但同时也要考虑到你不希望在层之间具有某种随机值的对称性。
- en: '[PRE1]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What we can see from the code above is that we create random numbers independently
    for each layer, and all of them in between the range ![](3ab3a524.png)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码中可以看出，我们独立为每一层创建随机数，而且它们都在范围内 ![](3ab3a524.png)
- en: Training steps
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练步骤
- en: '* * *'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now just to list the steps required to train a neural network. Here we mention
    the term epoch which means a complete pass intro all elements of your training
    set. Actually you repeat your training set over and over because the weights don't
    completely learn a concept in a single epoch.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只需列出训练神经网络所需的步骤。在这里我们提到时期这个术语，它意味着对你的训练集中的所有元素进行完整的遍历。实际上，你一遍又一遍地重复你的训练集，因为权重不会在一个时期内完全学会一个概念。
- en: Initialize weights randomly
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重
- en: For each epoch
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个时期
- en: Do the forward propagation
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行正向传播
- en: Calculate loss
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失
- en: Do the backward propagation
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行反向传播
- en: Update weights with Gradient descent (Optionally use gradient checking to verify
    backpropagation)
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新权重（可选使用梯度检查来验证反向传播）
- en: Go to step 2 until you finish all epochs
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到步骤2，直到完成所有时期
- en: Training/Validation/Test data
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练/验证/测试数据
- en: '* * *'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Some good practices to create your dataset for training your hypothesis models
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一些创建用于训练假设模型的数据集的良好实践
- en: Collect as many data as possible
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集尽可能多的数据
- en: Merge/Shuffle all this data
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并/洗牌所有这些数据
- en: Divide this dataset into train(60%)/validation(20%)/test(20%) set
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个数据集分成训练（60%）/验证（20%）/测试（20%）集
- en: Avoid having test from a different distribution of your train/validation
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免测试来自不同分布的训练/验证
- en: Use the validation set to tune your model (Number of layers/neurons)
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用验证集来调整你的模型（层数/神经元数量）
- en: Check overall performance with the test set
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试集检查整体性能
- en: When we say to use the validation set it means that we're going to change parameters
    of our model and check which one get better results on this validation set, don't
    touch your training set. If you are having bad results on your test set consider
    getting more data, and verify if your train/test/val come from the same distribution.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说要使用验证集时，意味着我们将更改模型的参数，并检查哪一个在这个验证集上获得更好的结果，不要触摸你的训练集。如果你在测试集上的结果不好，请考虑获取更多数据，并验证你的训练/测试/验证是否来自相同的分布。
- en: Effects of deep neural networks
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络的影响
- en: '* * *'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As mentioned earlier having deeper and bigger neural networks is always better
    in terms of recognition performance, but some problems also arise with more complex
    models.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，拥有更深层次和更大规模的神经网络在识别性能方面总是更好的，但是更复杂的模型也会带来一些问题。
- en: Deeper and more complex neural networks, need more data to train (10x number
    of parameters)
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更深层次和更复杂的神经网络，需要更多数据来训练（参数数量的10倍）
- en: Over-fit can become a problem so do regularization (Dropout, L2 regularization)
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过拟合可能会成为问题，因此进行正则化（Dropout、L2正则化）
- en: Prediction time will increase.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测时间将会增加。
- en: Neural networks as computation graphs
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络作为计算图
- en: '* * *'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In order to calculate the back-propagation, it's easier if you start representing
    your hypothesis as computation graphs. Also in next chapters we use different
    types of layers working together, so to simplify development consider the neural
    networks as computation graphs. The idea is that if you provide for each node
    of your graph the forward/backward implementation, the back propagation becomes
    much more easier.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算反向传播，如果你开始将你的假设表示为计算图，那会更容易。另外，在接下来的章节中，我们将使用不同类型的层一起工作，所以为了简化开发，请将神经网络视为计算图。这个想法是，如果你为你的图的每个节点提供正向/反向实现，那么反向传播就会变得更容易。
- en: '![](NeuralNetworkGraph.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](NeuralNetworkGraph.png)'
- en: Linear Classification
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性分类
- en: Introduction
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A linear classifier does classification decision based on the value of a linear
    combination of the characteristics. Imagine that the linear classifier will merge
    into it's weights all the characteristics that define a particular class. (Like
    merge all samples of the class cars together)
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器根据特征的线性组合的值做出分类决策。想象一下，线性分类器将在其权重中合并定义特定类的所有特征。（就像将类别为汽车的所有样本合并在一起）
- en: This type of classifier works better when the problem is linear separable.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题是线性可分时，这种类型的分类器效果更好。
- en: '![](linear_vs_nonlinear_problems.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](linear_vs_nonlinear_problems.png)'
- en: '![](bffbeee2.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](bffbeee2.png)'
- en: '![](pixelspaceLinear.jpeg)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](pixelspaceLinear.jpeg)'
- en: The weight matrix will have one row for every class that needs to be classified,
    and one column for ever element(feature) of x.On the picture above each line will
    be represented by a row in our weight matrix.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵将为需要分类的每个类别具有一行，并且对于 x 的每个元素（特征）都具有一列。在上面的图片中，每一行都将由我们的权重矩阵中的一行表示。
- en: Weight and Bias Effect
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重和偏置的影响
- en: '* * *'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The effect of changing the weight will change the line angle, while changing
    the bias, will move the line left/right
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 改变权重的效果将改变线的角度，而改变偏置将使线左右移动
- en: Parametric Approach
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数化方法
- en: '* * *'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](ParametricModel.jpg)'
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_IMG
  zh: '![](ParametricModel.jpg)'
- en: 'The idea is that out hypothesis/model has parameters, that will aid the mapping
    between the input vector to a specific class score. The parametric model has two
    important components:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 思想是我们的假设/模型具有参数，这些参数将有助于将输入向量映射到特定类得分。参数模型有两个重要组成部分：
- en: 'Score Function: Is a function ![](647cc9bd.png) that will map our raw input
    vector to a score vector'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分函数：是一个函数 ![](647cc9bd.png)，将我们的原始输入向量映射到一个评分向量
- en: 'Loss Function: Quantifies how well our current set of weights maps some input
    x to a expected output y, the loss function is used during training time.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数：衡量我们当前权重集合将一些输入 x 映射到期望输出 y 的程度，损失函数在训练时使用。
- en: On this approach, the training phase will find us a set of parameters that will
    change the hypothesis/model to map some input, to some of the output class.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，训练阶段将为我们找到一组参数，这些参数将改变假设/模型以将某些输入映射到某些输出类。
- en: During the training phase, which consist as a optimisation problem, the weights
    (W) and bias (b) are the only thing that we can change.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，即优化问题，权重 (W) 和偏置 (b) 是我们唯一可以改变的东西。
- en: '![](Linear_Classification.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](Linear_Classification.png)'
- en: 'Now some topics that are important on the diagram above:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在上面图表中一些重要的主题：
- en: The input image x is stretched to a single dimension vector, this loose spatial
    information
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入图像 x 被拉伸为单维向量，这会丢失空间信息
- en: The weight matrix will have one column for every element on the input
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重矩阵将为每个输入元素具有一列
- en: The weight matrix will have one row for every element of the output (on this
    case 3 labels)
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重矩阵将为每个输出元素（在本例中为 3 个标签）的每个元素都有一行。
- en: The bias will have one row for every element of the output (on this case 3 labels)
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏置将为每个输出元素（在本例中为 3 个标签）的每个元素都有一行。
- en: The loss will receive the current scores and the expected output for it's current
    input X
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数将接收当前分数和其当前输入 X 的期望输出
- en: 'Consider each row of W a kind of pattern match for a specified class. The score
    for each class is calculated by doing a inner product between the input vector
    X and the specific row for that class. Ex:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 将 W 的每一行视为针对指定类的一种模式匹配。每个类别的分数通过对输入向量 X 和该类别的特定行进行内积计算得出。例如：
- en: '![](3bab8058.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](3bab8058.png)'
- en: Example on Matlab
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Matlab 上的示例
- en: '* * *'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](calc_scores_matlab.PNG)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](calc_scores_matlab.PNG)'
- en: The image bellow reshape back the weights to an image, we can see by this image
    that the training try to compress on each row of W all the variants of the same
    class. (Check the horse with 2 heads)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像将权重重塑回图像，我们可以通过这个图像看到训练尝试将同一类的所有变体压缩在 W 的每一行上。（查看有 2 个头的马）
- en: '![](LinearTemplate.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](LinearTemplate.png)'
- en: Bias trick
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏置技巧
- en: '* * *'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Some learning libraries implementations, does a trick to consider the bias as
    part of the weight matrix, the advantage of this approach is that we can solve
    the linear classification with a single matrix multiplication.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 一些学习库的实现会做一个技巧来将偏置视为权重矩阵的一部分，这种方法的优势在于我们可以通过单次矩阵乘法解决线性分类问题。
- en: '![](8124c541.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![](8124c541.png)'
- en: '![](Bias_Trick.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](Bias_Trick.jpg)'
- en: Basically you add an extra row at the end of the input vector, and concatenate
    a column on the W matrix.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，你需要在输入向量的末尾添加一行，并在 W 矩阵上连接一列。
- en: Input and Features
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入和特征
- en: '* * *'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The input vector sometimes called feature vector, is your input data that is
    sent to the classifier. As the linear classifier does not handle non-linear problems,
    it is the responsibility of the engineer, process this data and present it in
    a form that is separable to the classifier.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量有时被称为特征向量，是发送到分类器的输入数据。由于线性分类器不能处理非线性问题，因此工程师的责任是处理这些数据并以分类器可分离的形式呈现它。
- en: The best case scenario is that you have a large number of features, and each
    of them has a high correlation to the desired output and low correlation between
    thems
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的情况是你有很多特征，每个特征与所需输出具有高相关性，而它们之间的相关性很低
- en: Loss Function
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: Introduction
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As mention earlier the Loss/Cost functions are mathematical functions that will
    answer how well your classifier is doing it's job with the current set of parameters
    (Weights and Bias). One important step on supervised learning is the choice of
    the right loss function for the job/task.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，损失/成本函数是数学函数，它将回答你的分类器在当前参数（权重和偏差）下工作得有多好。在监督学习中，选择正确的损失函数是至关重要的一步。
- en: Model Optimization
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化
- en: Model Optimization
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化
- en: Introduction
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Machine learning models learn by updating it's parameters (weights and biases)
    towards the direction of the correct classification.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通过更新其参数（权重和偏差）来学习，朝着正确分类的方向。
- en: Basic structure of a learning model
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习模型的基本结构
- en: '* * *'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On the picture bellow we will show the basic blocks of a machine learning model
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图片中，我们将展示机器学习模型的基本组成部分
- en: '![](MachineLearningModel.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](MachineLearningModel.png)'
- en: On this picture we can detect the following components
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图片上，我们可以发现以下组件
- en: 'Training dataset: Basically a high speed disk containing your training data'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据集：基本上是一个包含您的训练数据的高速磁盘
- en: 'Batch of samples: A list of pairs (X,Y), consisting on inputs, expected outputs,
    for example X can be a image and Y the label "cat"'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本批次：一个由（X，Y）对组成的列表，包括输入和期望输出，例如X可以是一张图片，Y是标签“猫”
- en: 'Parameters: Set of parameters used by your model layers, to map X to Y'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数：由您的模型层使用的一组参数，将X映射到Y
- en: 'Model: Set of computing layers that transform an input X and weights W, into
    a score (probable Y)'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型：一组计算层，将输入X和权重W转换为一个得分（可能的Y）
- en: 'Loss function: Responsible to say how far our score is from the ideal response
    Y, the output of the loss function is a scalar. Another way is also to consider
    that the loss function say how bad is your current set of parameters W.'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数：负责表明我们的分数距离理想响应Y有多远，损失函数的输出是一个标量。另一种方式也是考虑损失函数表明您当前的参数W有多糟糕。
- en: What we will do?
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们将做什么？
- en: '* * *'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Basically we need an algorithm that will change our weight and biases, in order
    to minimize our loss function.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们需要一种算法来改变我们的权重和偏差，以便最小化我们的损失函数。
- en: The Loss function
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: '* * *'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: You can imagine the loss function here as a place with some mountains, and your
    objective is to find it's vale (lowest) place. Your only instrument is the gadget
    that returns your altitude (loss). You need to find out which direction to take.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象这里的损失函数就像一个有些山的地方，你的目标是找到它的低谷（最低）处。你唯一的工具是返回你的海拔高度（损失）的小玩意儿。你需要找出应该走哪个方向。
- en: '![](LossAlps.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![](LossAlps.png)'
- en: 'Here we can also observe two things:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们还可以观察到两件事：
- en: You have more than one value (Local minima)
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有超过一个值（局部最小值）
- en: Depending where you landed you probably find one instead of the other (Importance
    of weight initialization)
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取决于你降落在哪里，你可能会找到一个而不是另一个（权重初始化的重要性）
- en: Which direction to take (Gradient descent)
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 要采取哪个方向（梯度下降）
- en: '* * *'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We can use calculus to discover which direction to take, for instance if we
    follow the derivative of the loss function, we can guarantee that we always go
    down. This is done just by subtracting the current set of weights by derivative
    of the loss function evaluated at that point.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用微积分来发现应该采取的方向，例如，如果我们遵循损失函数的导数，我们可以保证我们总是朝着下降的方向前进。这只需将当前的权重集减去该点处的损失函数的导数即可完成。
- en: '![](ddc9caec.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](ddc9caec.png)'
- en: On multiple dimensions we have a vector of partial derivatives, which we call
    gradient.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个维度上，我们有一个偏导数的向量，我们称之为梯度。
- en: 'Observe that we multiply the gradient by a factor ![](c9b9fa39.png) (step-size,
    learning-rate), that is normally a small value ex: 0.001'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将梯度乘以一个因子![](c9b9fa39.png)（步长，学习速率），通常是一个小值，例如：0.001
- en: Simple Example
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单示例
- en: To illustrate the gradient descent method let's follow a simple case in 1D.
    Consider the initial weight (-1.5)
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明梯度下降方法，让我们按照一维中的一个简单情况来进行。考虑初始权重（-1.5）
- en: '![](474e08d1.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](474e08d1.png)'
- en: '![](gradDescentAnim.gif)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](gradDescentAnim.gif)'
- en: On Matlab
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Matlab 上
- en: '![](GradientDescentMatCode.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](GradientDescentMatCode.png)'
- en: We can observe that after we use calculus do find the derivative of the loss
    function, we just needed to evaluate it with the current weight. After that we
    just take the evaluated value from the current weight.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，在使用微积分找到损失函数的导数之后，我们只需要用当前权重评估它。之后，我们只需从当前权重中取得评估值。
- en: Learning rate to big
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率过大
- en: '* * *'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Using a big learning rate can accelerate convergence, but could also make the
    gradient descent oscillate and diverge.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大的学习率可以加速收敛，但也可能使梯度下降振荡并发散。
- en: '![](BigGradientDescent.gif)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](BigGradientDescent.gif)'
- en: Numerical Gradient
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值梯度
- en: '* * *'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Slow method to evaluate the gradient, but we can use it to verify if our code
    is right
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 评估梯度的慢方法，但我们可以用它来验证我们的代码是否正确
- en: Mini Batch Gradient Descent
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: '* * *'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](loss.jpeg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](loss.jpeg)'
- en: Instead of running through all the training set to calculate the loss, then
    do gradient descent, we can do in a small (batch) portions. This leads to similar
    results and compute faster. Normally the min-batch size is dependent of the GPU
    memory.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 不是运行整个训练集来计算损失，然后进行梯度下降，我们可以分成小的（批量）部分来做。这导致类似的结果并且计算更快。通常，小批量大小取决于 GPU 内存。
- en: If you analyze your loss decay over time the full-batch version is less noisy
    but much more difficult to compute.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分析随时间衰减的损失，完全批量版本更少噪音但更难计算。
- en: If your mini-batch size goes to 1, which means you compute the gradient descent
    for each sample. On this case we have the Stochastic Gradient Descent.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的小批量大小变为 1，这意味着你为每个样本计算梯度下降。在这种情况下，我们有随机梯度下降。
- en: This is relatively less common to see because in practice due to vectorized
    code optimizations it can be computationally much more efficient to evaluate the
    gradient for 100 examples, than the gradient for one example 100 times.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 这相对较少见，因为在实践中，由于矢量化代码优化，评估 100 个示例的梯度通常比 100 次评估一个示例的梯度更加高效。
- en: Effects of learning rate on loss
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习速率对损失的影响
- en: Actually what people do is to choose a high (but not so high) learning rate,
    then decay with time.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上人们做的是选择一个较高（但不是太高）的学习速率，然后随时间衰减。
- en: '![](learningrates.jpeg)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](learningrates.jpeg)'
- en: 'Here are 2 ways to decay the learning rate with time while training:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两种训练过程中随时间衰减学习率的方法：
- en: Divide the learning rate (![](efff0667.png)) by 2 every x epochs
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 每 x 个周期将学习速率（![](efff0667.png)）减半
- en: '![](c36c0e20.png), where t is time and k is the decay parameter'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '![](c36c0e20.png)，其中 t 是时间，k 是衰减参数'
- en: '![](4480a80c.png), where t is the time and k the decay parameter'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '![](4480a80c.png)，其中 t 是时间，k 是衰减参数'
- en: Other algorithms
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他算法
- en: 'The gradient descent is the simplest idea to do model optimization. There are
    a few other nice algorithms to try when thinking about model optimization:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是进行模型优化的最简单的方法。在考虑模型优化时，还有一些不错的算法可以尝试：
- en: Stochastic Gradient descent
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Stochastic Gradient descent with momentum (Very popular)
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有动量的随机梯度下降（非常流行）
- en: Nag
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nag
- en: Adagrad
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: Adam (Very good because you need to take less care about learning rate)
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam（非常好，因为你不需要太在意学习速率）
- en: rmsprop
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rmsprop
- en: '![](OtherOptimizers.gif)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![](OtherOptimizers.gif)'
- en: Next Chapter
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: Now what we need is a way to get the gradient vector of our model, next chapter
    we will talk about back-propagation.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一种方法来获取我们模型的梯度向量，下一章我们将讨论反向传播。
- en: Backpropagation
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Back-propagation
  id: totrans-503
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Introduction
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Backpropagation is an algorithm that calculate the partial derivative of every
    node on your model (ex: Convnet, Neural network). Those partial derivatives are
    going to be used during the training phase of your model, where a loss function
    states how much far your are from the correct result. This error is propagated
    backward from the model output back to it''s first layers. The backpropagation
    is more easily implemented if you structure your model as a computational graph.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种计算模型中每个节点（例如：Convnet，神经网络）的偏导数的算法。这些偏导数将在模型的训练阶段中使用，其中损失函数说明了您与正确结果相差多少。这个错误是从模型输出向后传播到它的第一层。如果您将模型结构化为计算图，则反向传播更容易实现。
- en: '![](Circuit1.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![](Circuit1.png)'
- en: The most important thing to have in mind here is how to calculate the forward
    propagation of each block and it's gradient. Actually most of the deep learning
    libraries code is about implementing those gates forward/backward code.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最重要的事情是如何计算每个块的前向传播及其梯度。实际上，大多数深度学习库的代码都是关于实现这些门的前向/后向代码。
- en: Basic blocks
  id: totrans-509
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本块
- en: Some examples of basic blocks are, add, multiply, exp, max. All we need to do
    is observe their forward and backward calculation
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基本块的示例包括加法、乘法、指数、最大值。我们所需要做的就是观察它们的前向和后向计算。
- en: '* * *'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](SumGate.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![](SumGate.png)'
- en: '![](MulGate.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![](MulGate.png)'
- en: '![](MaxGate.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![](MaxGate.png)'
- en: '![](GradientBranches.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![](GradientBranches.png)'
- en: Some other derivatives
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些导数
- en: '![](2c1d1490.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![](2c1d1490.png)'
- en: Observe that we output 2 gradients because we have 2 inputs... Also observe
    that we need to save (cache) on memory the previous inputs.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们输出了 2 个梯度，因为我们有 2 个输入... 还要注意我们需要将先前的输入保存在内存中。
- en: Chain Rule
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链式法则
- en: '* * *'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Imagine that you have an output y, that is function of g, which is function
    of f, which is function of x. If you want to know how much g will change with
    a small change on dx (dg/dx), we use the chain rule. Chain rule is a formula for
    computing the derivative of the composition of two or more functions.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一个输出 y，它是 g 的函数，而 g 是 f 的函数，而 f 是 x 的函数。如果你想知道 g 在 dx 上的微小变化（dg/dx），我们使用链式法则。链式法则是计算两个或更多函数组合的导数的公式。
- en: '![](ChainRule1.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![](ChainRule1.png)'
- en: The chain rule is the work horse of back-propagation, so it's important to understand
    it now. On the picture bellow we get a node f(x,y) that compute some function
    with two inputs x,y and output z. Now on the right side, we have this same node
    receiving from somewhere (loss function) a gradient dL/dz which means. "How much
    L will change with a small change on z". As the node has 2 inputs it will have
    2 gradients. One showing how L will a small change dx and the other showing how
    L will change with a small change (dz)
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则是反向传播的主要工作，因此现在理解它很重要。在下面的图片中，我们有一个节点 f(x,y)，它计算具有两个输入 x、y 和输出 z 的某个函数。现在在右边，我们有这个相同的节点从某处（损失函数）接收一个梯度
    dL/dz，这意味着。“L 会随着 z 的微小变化而改变多少”。由于节点有 2 个输入，它将有 2 个梯度。一个显示 L 如何随着 dx 的微小变化而变化，另一个显示
    L 如何随着 dz 的微小变化而变化。
- en: '![](chainrule_example.PNG) In order to calculate the gradients we need the
    input dL/dz (dout), and the derivative of the function f(x,y), at that particular
    input, then we just multiply them. Also we need the previous cached input, saved
    during forward propagation.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '![](chainrule_example.PNG) 为了计算梯度，我们需要输入的 dL/dz（dout），以及在该特定输入处的函数 f(x,y) 的导数，然后我们只需将它们相乘。同时，我们需要在前向传播期间保存的先前缓存的输入。'
- en: Gates Implementation
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门的实现
- en: '* * *'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Observe bellow the implementation of the multiply and add gate on python
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 下面观察 python 上乘法和加法门的实现
- en: '![](PythonCircuits.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![](PythonCircuits.png)'
- en: Step by step example
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐步示例
- en: '* * *'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: With what we learn so far, let's calculate the partial derivatives of some graphs
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们迄今所学，让我们计算一些图的偏导数
- en: Simple example
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单示例
- en: Here we have a graph for the function ![](9e90bd2.png)
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个函数的图表 ![](9e90bd2.png)
- en: '![](SimpleGraph.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![](SimpleGraph.png)'
- en: Start from output node f, and consider that the gradient of f related to some
    criteria is 1 (dout)
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输出节点 f 开始，并考虑 f 相对于某些标准的梯度为 1（dout）
- en: dq=(dout(1) * z), which is -4 (How the output will change with a change in q)
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dq=(dout(1) * z)，这是 -4（输出将如何随着 q 的变化而变化）
- en: dz=(dout(1) * q), which is 3 (How the output will change with a change in z)
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dz=(dout(1) * q)，这是 3（输出将如何随着 z 的变化而变化）
- en: The sum gate distribute it's input gradients, so dx=-4, dy=-4 (How the output
    will change with x,z)
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总和门分配了它的输入梯度，所以 dx=-4，dy=-4（输出将如何随着 x，z 的变化而变化）
- en: Perceptron with 2 inputs
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有 2 个输入的感知机
- en: This following graph represent the forward propagation of a simple 2 inputs,
    neural network with one output layer with sigmoid activation.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表表示一个简单的 2 输入、具有 sigmoid 激活的输出层的神经网络的前向传播。
- en: '![](6fcb84f4.png)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![](6fcb84f4.png)'
- en: '![](SimplePerceptron.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![](SimplePerceptron.png)'
- en: '![](StepByStepExample.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](StepByStepExample.png)'
- en: Start from the output node, considering that or error(dout) is 1
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输出节点开始，考虑到我们的误差(dout)是 1
- en: The gradient of the input of the 1/x will be -1/(1.37^2), -0.53
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1/x 输入的梯度将是 -1/(1.37^2)，-0.53
- en: The increment node does not change the gradient on it's input, so it will be
    (-0.53 * 1), -0.53
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增量节点不会改变其输入的梯度，因此它将是 (-0.53 * 1)，-0.53
- en: The exp node input gradient will be (exp(-1(cached input)) * -0.53), -0.2
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: exp 节点的输入梯度将是 (exp(-1(缓存输入)) * -0.53)，-0.2
- en: The negative gain node will be it's input gradient (-1 * -0.2), 0.2
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负增益节点将是其输入梯度（-1 * -0.2），0.2
- en: The sum node will distribute the gradients, so, dw2=0.2, and the sum node also
    0.2
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总和节点将分配梯度，因此 dw2=0.2，并且总和节点也为 0.2
- en: The sum node again distribute the gradients so again 0.2
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总和节点再次分配梯度，因此再次为 0.2
- en: dw0 will be (0.2 * -1), -0.2
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dw0 将是 (0.2 * -1)，-0.2
- en: dx0 will be (0.2 * 2). 0.4
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dx0 将是 (0.2 * 2)。0.4
- en: Next Chapter
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: Next chapter we will learn about Feature Scaling.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习关于特征缩放的内容。
- en: Feature Scaling
  id: totrans-555
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Feature Scaling
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Introduction
  id: totrans-557
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In order to make the life of gradient descent algorithms easier, there are some
    techniques that can be applied to your data on training/test phase. If the features
    ![](bddfd27d.png) on your input vector ![](761426bc.png), are out of scale your
    loss space ![](42a195a9.png) will be somehow stretched. This will make the gradient
    descent convergence harder, or at least slower.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使梯度下降算法的生活更加轻松，可以在训练/测试阶段对数据应用一些技术。如果输入向量 ![](bddfd27d.png) 中的特征不成比例，那么你的损失空间
    ![](42a195a9.png) 将会在某种程度上被拉伸。这将使梯度下降的收敛更加困难，或者至少更慢。
- en: On the example bellow your input X has 2 features (house size, and number of
    bedrooms). The problem is that house size feature range from 0...2000, while number
    of bedrooms range from 0...5.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，你的输入 X 有 2 个特征（房屋大小和卧室数量）。问题是房屋大小特征范围为 0...2000，而卧室数量范围为 0...5。
- en: '![](FeatureScaling.jpg)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![](FeatureScaling.jpg)'
- en: Centralize data and normalize
  id: totrans-562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中心化和归一化
- en: '* * *'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](PreProcessing.jpeg)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![](PreProcessing.jpeg)'
- en: 'Bellow we will pre-process our input data to fix the following problems:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将对我们的输入数据进行预处理，以解决以下问题：
- en: Data not centered around zero
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据未围绕零中心化
- en: Features out of scale
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征不成比例
- en: Consider your input data ![](e84a970b.png), where N is the number of samples
    on your input data (batch size) and D the dimensions (On the previous example
    D is 2, size house, num bedrooms).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑你的输入数据 ![](e84a970b.png)，其中 N 是你的输入数据样本数（批量大小），D 是维度（在前面的示例中，D 是 2，房屋大小，卧室数量）。
- en: The first thing to do is to subtract the mean value of the input data, this
    will centralize the data dispersion around zero
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是减去输入数据的均值，这将使数据的分散围绕零中心化
- en: '![](b53e5544.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![](b53e5544.png)'
- en: On prediction phase is common to store this mean value to be subtracted from
    a test example. On the case of image classification, it's also common to store
    a mean image created from a batch of images on the training-set, or the mean value
    from every channel.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测阶段，通常会存储这个均值以便从测试样本中减去。在图像分类的情况下，通常会存储从训练集的一批图像创建的均值图像，或者每个通道的均值。
- en: After your data is centralized around zero, you can make all features have the
    same range by dividing X by it's standard deviation.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据围绕零中心化后，你可以通过将 X 除以它的标准差来使所有特征具有相同的范围。
- en: '![](8008dab6.png)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![](8008dab6.png)'
- en: Again this operation fix our first, problem, because all features will range
    similarly. But this should be used if somehow you know that your features have
    the same "weight". On the case of image classification for example all pixels
    have the same range (0..255) and a pixel alone has no bigger meaning (weight)
    than the other, so just mean subtraction should suffice for image classification.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这个操作解决了我们的第一个问题，因为所有的特征将会有相似的范围。但是如果你知道你的特征具有相同的“重要性”，那么应该使用这个方法。例如，在图像分类的情况下，所有的像素都具有相同的范围
    (0..255)，一个像素本身并没有比其他像素更重要，因此仅需进行均值减法就足够了。
- en: 'Common mistake:'
  id: totrans-575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见错误：
- en: '* * *'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: An important point to make about the preprocessing is that any preprocessing
    statistics (e.g. the data mean) must only be computed on the training data, and
    then applied to the validation / test data. Computing the mean and subtracting
    it from every image across the entire dataset and then splitting the data into
    train/val/test splits would be a mistake. Instead, the mean must be computed only
    over the training data and then subtracted equally from all splits (train/val/test).
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 关于预处理的一个重要观点是，任何预处理统计数据（例如数据均值）必须仅在训练数据上计算，然后应用于验证/测试数据。计算数据均值并从整个数据集中的每个图像中减去它，然后将数据分割成训练/验证/测试集是错误的。相反，均值必须仅在训练数据上计算，然后平均减去所有分割（训练/验证/测试）的数据。
- en: Next Chapter
  id: totrans-578
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: Next chapter we will learn about Neural Networks.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习神经网络。
- en: Model Initialization
  id: totrans-580
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型初始化
- en: Model Initialization
  id: totrans-581
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型初始化
- en: Introduction
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: One important topic we should learn before we start training our models, is
    about the weight initialization. Bad weight initialization, can lead to a "never
    convergence training" or a slow training.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练模型之前，我们应该学习的一个重要主题是权重初始化。糟糕的权重初始化可能导致“永不收敛的训练”或训练缓慢。
- en: Weight matrix format
  id: totrans-585
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重矩阵格式
- en: '* * *'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'As observed on previous chapters, the weight matrix has the following format:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前几章观察到的，权重矩阵具有以下格式：
- en: '![](9e63ed7d.png)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![](9e63ed7d.png)'
- en: 'Consider the number of outputs ![](64649e57.png), as rows and the number of
    inputs ![](5f0028c.png) as columns. You could also consider another format:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 将输出数量![](64649e57.png)视为行数，将输入数量![](5f0028c.png)视为列数。您还可以考虑另一种格式：
- en: '![](109a7901.png)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![](109a7901.png)'
- en: Here ![](64649e57.png) as columns and ![](5f0028c.png) as rows.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 这里![](64649e57.png)作为列，![](5f0028c.png)作为行。
- en: The whole point is that our weights is going to be a 2d matrix function of ![](5f0028c.png)
    and ![](64649e57.png)
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于我们的权重将是一个关于![](5f0028c.png)和![](64649e57.png)的二维矩阵函数
- en: Initialize all to zero
  id: totrans-593
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全部初始化为零
- en: '* * *'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you initialize your weights to zero, your gradient descent will never converge
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将权重初始化为零，您的梯度下降将永远不会收敛
- en: '![](all_zeros_initialization.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](all_zeros_initialization.png)'
- en: Initialize with small values
  id: totrans-597
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用小值初始化
- en: '* * *'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'A better idea is to initialize your weights with values close to zero (but
    not zero), ie: 0.01'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是用接近零的值（但不是零）初始化权重，即：0.01。
- en: '![](9979e35b.png)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](9979e35b.png)'
- en: Here randn gives random data with zero mean, unit standard deviation. ![](20895e70.png)
    are the number of input and outputs. The 0.01 term will keep the random weights
    small and close to zero.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 这里randn给出了均值为零，单位标准差的随机数据。![](20895e70.png)是输入和输出的数量。0.01项将保持随机权重较小且接近于零。
- en: '![](matlab_randn.png)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![](matlab_randn.png)'
- en: The problem with the previous way to do initialization is that the variance
    of the outputs will grow with the number of inputs. To solve this issue we can
    divide the random term by the square root of the number of inputs.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 以前进行初始化的问题是输出的方差会随着输入数量的增加而增加。要解决这个问题，我们可以将随机项除以输入数量的平方根。
- en: '![](fc90afec.png)'
  id: totrans-604
  prefs: []
  type: TYPE_IMG
  zh: '![](fc90afec.png)'
- en: '![](xavier_inits.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](xavier_inits.png)'
- en: Now it seems that we don't have dead neurons, the only problem with this approach
    is to use it with Relu neurons.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看来我们没有死神经元，这种方法的唯一问题是将其用于Relu神经元。
- en: '![](xavier_inits_relu.png)'
  id: totrans-607
  prefs: []
  type: TYPE_IMG
  zh: '![](xavier_inits_relu.png)'
- en: To solve this just add a simple (divide by 2) term....
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，只需添加一个简单的（除以2）项....
- en: '![](20772a44.png)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
  zh: '![](20772a44.png)'
- en: '![](xavier_inits_relu_fix.png)'
  id: totrans-610
  prefs: []
  type: TYPE_IMG
  zh: '![](xavier_inits_relu_fix.png)'
- en: So use this second form to initialize Relu layers.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 所以使用这种第二种形式来初始化Relu层。
- en: Bath Norm layer
  id: totrans-612
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批归一化层
- en: '* * *'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On future chapters we're going to learn a technique that make your model more
    resilient to specific initialization.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的章节中，我们将学习一种使您的模型对特定初始化更具弹性的技术。
- en: Next chapter
  id: totrans-615
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: '* * *'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Next chapter we start talk about convolutions.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们开始谈论卷积。
- en: Recurrent Neural Networks
  id: totrans-618
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Recurrent Neural Networks
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Introduction
  id: totrans-620
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On previous forward neural networks, our output was a function between the current
    input and a set of weights. On recurrent neural networks(RNN), the previous network
    state is also influence the output, so recurrent neural networks also have a "notion
    of time". This effect by a loop on the layer output to it's input.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 在以前的前向神经网络中，我们的输出是当前输入和一组权重之间的函数。在循环神经网络(RNN)中，先前的网络状态也会影响输出，因此循环神经网络也具有“时间概念”。这通过将层输出循环到其输入来实现。
- en: '![](recurrent.jpg)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![](recurrent.jpg)'
- en: In other words, the RNN will be a function with inputs ![](2683a688.png) (input
    vector) and previous state ![](93d89ee3.png). The new state will be ![](22b53607.png).
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，RNN将是一个具有输入![](2683a688.png)（输入向量）和上一个状态![](93d89ee3.png)的函数。新状态将是![](22b53607.png)。
- en: The recurrent function, ![](d9cebfcd.png), will be fixed after training and
    used to every time step.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 递归函数![](d9cebfcd.png)在训练后将被固定，并用于每个时间步。
- en: Recurrent Neural Networks are the best model for regression, because it take
    into account past values.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是回归的最佳模型，因为它考虑了过去的值。
- en: RNN are computation "Turing Machines" which means, with the correct set of weights
    it can compute anything, imagine this weights as a program.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是计算“图灵机”，这意味着，通过正确的权重集，它可以计算任何东西，想象这些权重就像一个程序。
- en: Just to not let you too overconfident on RNN, there is no automatic back-propagation
    algorithms, that will find this "perfect set of weights".
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了不让你对RNN过于自信，没有自动的反向传播算法，可以找到这个“完美的权重集”。
- en: 'Bellow we have a simple implementation of RNN recurrent function: (Vanilla
    version)'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是RNN循环函数的简单实现：（基础版本）
- en: '![](13091812.png)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
  zh: '![](13091812.png)'
- en: 'The code that calculate up to the next state ![](22b53607.png) looks like this:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 计算到下一个状态的代码如下：![](22b53607.png)
- en: '[PRE2]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Observe that in our case of RNN we are now more interested on the next state,
    ![](22b53607.png) not exactly the output, ![](4d21efaf.png)
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的RNN案例中，我们现在更感兴趣的是下一个状态，![](22b53607.png) 而不是确切的输出，![](4d21efaf.png)
- en: Before we start let's just make explicit how to backpropagate the tanh block.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们明确如何反向传播tanh块。
- en: '![](tanhBlock.png)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
  zh: '![](tanhBlock.png)'
- en: Now we can do the backpropagation step (For one single time-step)
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行反向传播步骤（对于单个时间步）
- en: '[PRE3]'
  id: totrans-637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A point to be noted is that the same function ![](4c48cbdd.png) and the same
    set of parameters will be applied to every time-step.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，相同的函数![](4c48cbdd.png) 和相同的参数集将应用于每个时间步。
- en: '![](RecurrentNeuralNetwork.png)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![](RecurrentNeuralNetwork.png)'
- en: A good initialization for the RNN states ![](22b53607.png) is zero. Again this
    is just the initial RNN state not it's weights.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: RNN状态的良好初始化为零。再次强调，这只是初始RNN状态，而不是其权重。
- en: These looping feature on RNNs can be confusing first but actually you can think
    as a normal neural network repeated(unrolled) multiple times. The number of times
    that you unroll can be consider how far in the past the network will remember.
    In other words each time is a time-step.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: RNN中的这种循环特性一开始可能会让人困惑，但实际上你可以将其看作是一个正常的神经网络多次重复（展开）。你展开的次数可以被认为是网络将记住多久以前的内容。换句话说，每次都是一个时间步。
- en: '![](RNN_Unrolling.png)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![](RNN_Unrolling.png)'
- en: Forward and backward propagation on each time-step
  id: totrans-643
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个时间步上的前向和后向传播
- en: From the previous examples we presented code for forward and backpropagation
    for one time-step only. As presented before the RNN are unroled for each time-step
    (finite). Now we present how to do the forward propagation for each time-step.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中，我们仅为一个时间步呈现了前向和反向传播的代码。如前所述，RNN被展开到每个时间步（有限）。现在我们展示如何为每个时间步进行前向传播。
- en: '[PRE4]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Bellow we show a diagram that present the multiple ways that you could use a
    recurrent neural network compared to the forward networks. Consider the inputs
    the red blocks, and the outputs the blue blocks.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们展示了一个图表，展示了与前向网络相比，您可以使用递归神经网络的多种方式。将红色块视为输入，蓝色块视为输出。
- en: '![](Recurrent_Forward.jpeg)'
  id: totrans-648
  prefs: []
  type: TYPE_IMG
  zh: '![](Recurrent_Forward.jpeg)'
- en: 'One to one: Normal Forward network, ie: Image on the input, label on the output'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一：正常的前向网络，即：输入为图像，输出为标签
- en: 'One to many(RNN): (Image captioning) Image in, words describing the scene out
    (CNN regions detected + RNN)'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多（RNN）：（图像字幕）输入为图像，输出为描述场景的单词（CNN检测到的区域 + RNN）
- en: 'Many to one(RNN): (Sentiment Analysis) Words on a phrase on the input, sentiment
    on the output (Good/Bad) product.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对一（RNN）：（情感分析）输入为短语中的单词，输出为情感（好/坏）产品。
- en: 'Many to many(RNN): (Translation), Words on English phrase on input, Portuguese
    on output.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（RNN）：（翻译）输入为英语短语中的单词，输出为葡萄牙语。
- en: 'Many to many(RNN): (Video Classification) Video in, description of video on
    output.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多（RNN）：（视��分类）视频输入，视频描述输出。
- en: Stacking RNNs
  id: totrans-654
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠RNN
- en: '* * *'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Bellow we describe how we add "depth" to RNN and also how to unroll RNNs to
    deal with time.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们描述如何为RNN添加“深度”，以及如何展开RNN以处理时间。
- en: Observe that the output of the RNNs are feed to deeper layers, while the state
    is feed for dealing with past states.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RNN的输出被馈送到更深层，而状态被馈送以处理过去的状态。
- en: '![](RNN_Stacking.png)'
  id: totrans-658
  prefs: []
  type: TYPE_IMG
  zh: '![](RNN_Stacking.png)'
- en: Simple regression example
  id: totrans-659
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单的回归示例
- en: Here we present a simple case where we want the RNN to complete the word, we
    give to the network the characters h,e,l,l , our vocabulary here is [h,e,l,o].
    Observe that after we input the first 'h' the network want's to output the wrong
    answer (right is on green), but near the end, after the second 'l' it want's to
    output the right answer 'o'. Here the order that the characters come in does matter.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了一个简单的案例，我们希望RNN完成单词，我们向网络提供字符h,e,l,l，我们的词汇表是[h,e,l,o]。请注意，在我们输入第一个'h'后，网络想要输出错误答案（正确答案在绿色上），但接近末尾，在第二个'l'后，它想要输出正确答案'o'。在这里，字符的顺序确实很重要。
- en: '![](RNNSampleWithDict.png)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
  zh: '![](RNNSampleWithDict.png)'
- en: Describing images
  id: totrans-662
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述图像
- en: '* * *'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you connect a convolution neural network, with pre-trained RNN. The RNN will
    be able to describe what it "see" on the image.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将卷积神经网络与预训练的 RNN 连接起来。RNN 将能够描述它在图像上“看到”的内容。
- en: '![](CNN_RNN.png)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
  zh: '![](CNN_RNN.png)'
- en: 'Basically we get a pre-trained CNN (ie: VGG) and connect the second-to-last
    FC layer and connect to a RNN. After this you train the whole thing end-to-end.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们得到一个预训练的 CNN（例如：VGG），连接到倒数第二个全连接层，然后连接到一个 RNN。之后你可以端对端地训练整个模型。
- en: '![](CNN_RNN_2.png)'
  id: totrans-667
  prefs: []
  type: TYPE_IMG
  zh: '![](CNN_RNN_2.png)'
- en: Long Short Term Memory networks(LSTM)
  id: totrans-668
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）
- en: '* * *'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: LSTM provides a different recurrent formula ![](d9cebfcd.png), it's more powefull
    thanvanilla RNN, due to it's complex ![](d9cebfcd.png) that add "residual information"
    to the next state instead of just transforming each state. Imagine LSTM are the
    "residual" version of RNNs.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 提供了一个不同的循环公式 ![](d9cebfcd.png)，它比普通 RNN 更强大，因为它的复杂性 ![](d9cebfcd.png) 会将“残余信息”添加到下一个状态，而不仅仅是转换每个状态。想象一下
    LSTM 是 RNN 的“残余”版本。
- en: In other words LSTM suffer much less from vanishing gradients than normal RNNs.
    Remember that the plus gates distribute the gradients.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，LSTM 比普通 RNN 更少受到梯度消失的影响。记住加法门会分配梯度。
- en: So by suffering less from vanishing gradients, the LSTMs can remember much more
    in the past. So from now just use LSTMs when you think about RNN.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于梯度消失的问题较少，LSTM 可以记住更多的过去信息。所以从现在开始，当你考虑 RNN 时只使用 LSTM。
- en: '![](residual_RNN.png)'
  id: totrans-673
  prefs: []
  type: TYPE_IMG
  zh: '![](residual_RNN.png)'
- en: Observe from the animation bellow how hast the gradients on the RNN disappear
    compared to LSTM.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面的动画中观察 RNN 上的梯度消失与 LSTM 相比是多么快。
- en: '![](rnn_LSTM_gradient.gif)'
  id: totrans-675
  prefs: []
  type: TYPE_IMG
  zh: '![](rnn_LSTM_gradient.gif)'
- en: The vanishing problem can be solved with LSTM, but another problem that can
    happen with all recurrent neural network is the exploding gradient problem.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题可以通过 LSTM 解决，但所有循环神经网络可能遇到的另一个问题是梯度爆炸问题。
- en: To fix the exploding gradient problem, people normally do a gradient clipping,
    that will allow only a maximum gradient value.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决梯度爆炸问题，人们通常会进行梯度裁剪，这将只允许最大梯度值。
- en: This highway for the gradients is called Cell-State, so one difference compared
    to the RNN that has only the state flowing, on LSTM we have states and the cell
    state.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用于梯度传递的通道被称为细胞状态，因此与只有状态流动的 RNN 相比，LSTM 有状态和细胞状态的区别。
- en: '![](Flow_LSTM_RNN.png)'
  id: totrans-679
  prefs: []
  type: TYPE_IMG
  zh: '![](Flow_LSTM_RNN.png)'
- en: LSTM Gate
  id: totrans-680
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM 门
- en: Doing a zoom on the LSTM gate. This also improves how to do the backpropagation.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 对 LSTM 门进行放大。这也改善了反向传播的方法。
- en: '![](LSTMBlockDiagram.png)'
  id: totrans-682
  prefs: []
  type: TYPE_IMG
  zh: '![](LSTMBlockDiagram.png)'
- en: Code for lstm forward propagation for one time-step
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 用于一个时间步的 LSTM 前向传播的代码
- en: '[PRE6]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now the backward propagation for one time-step
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是一个时间步的反向传播
- en: '[PRE7]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Deep Learning
  id: totrans-687
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: Introduction
  id: totrans-688
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](DLPic.png)'
  id: totrans-690
  prefs: []
  type: TYPE_IMG
  zh: '![](DLPic.png)'
- en: Deep learning is a branch of machine learning based on a set of algorithms that
    learn to represent the data. Bellow we list the most popular ones.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是基于一组算法的机器学习分支，这些算法学习表示数据。下面我们列出最流行的算法。
- en: Convolutional Neural Networks
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Deep Belief Networks
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: Deep Auto-Encoders
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度自动编码器
- en: Recurrent Neural Networks (LSTM)
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（LSTM）
- en: One of the promises of deep learning is that they will substitute hand-crafted
    feature extraction. The idea is that they will "learn" the best features needed
    to represent the given data.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个承诺是它们将取代手工制作的特征提取。其想法是它们将“学习”表示给定数据所需的最佳特征。
- en: '![](DeepLearning.png)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![](DeepLearning.png)'
- en: Layers and layers
  id: totrans-698
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层层叠加
- en: '* * *'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Deep learning models are formed by multiple layers. On the context of artificial
    neural networks the multi layer perceptron (MLP) with more than 2 hidden layers
    is already a Deep Model.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型由多个层组成。在人工神经网络的背景下，具有超过 2 个隐藏层的多层感知器（MLP）已经是一个深度模型。
- en: As a rule of thumb deeper models will perform better than shallow models, the
    problem is that more deep you go more data, you will need to avoid over-fitting.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个经验法则，深度模型的表现会比浅层模型更好，问题在于你深入的层数越多，你就需要更多的数据来避免过拟合。
- en: '![](DeepLearningModel.png)'
  id: totrans-702
  prefs: []
  type: TYPE_IMG
  zh: '![](DeepLearningModel.png)'
- en: Layer types
  id: totrans-703
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层类型
- en: Here we list some of the most used layers
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些最常用的层
- en: Convolution layer
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling Layer
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化层
- en: Dropout Layer
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃层
- en: Batch normalization layer
  id: totrans-708
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化层
- en: Fully Connected layer
  id: totrans-709
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全连接层
- en: Relu, Tanh, sigmoid layer
  id: totrans-710
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Relu、Tanh、sigmoid 层
- en: Softmax, Cross Entropy, SVM, Euclidean (Loss layers)
  id: totrans-711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax、交叉熵、SVM、欧几里得（损失层）
- en: Avoid over-fitting
  id: totrans-712
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免过拟合
- en: '* * *'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Besides getting more data, there are some techniques used to combat over-fitting,
    here is a list of the most common ones:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 除了获取更多数据，还有一些用于对抗过拟合的技术，以下是最常见的一些：
- en: Dropout
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辍学
- en: Regularization
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正规化
- en: Data Augmentation
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强
- en: Dropout
  id: totrans-718
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 辍学
- en: It's a technique that randomly turns off some neurons from the fully connected
    layer during training.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种在训练期间随机关闭一些全连接层神经元的技术。
- en: '![](Dropout.png)'
  id: totrans-720
  prefs: []
  type: TYPE_IMG
  zh: '![](Dropout.png)'
- en: The dropout forces the fully connected layers to learn the same concept in different
    ways
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 辍学强制全连接层以不同的方式学习相同的概念
- en: Regularization
  id: totrans-722
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正规化
- en: dasdaasdasadasdad
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: dasdaasdasadasdad
- en: Data Augmentation
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: Synthetically create new training examples by applying some transformations
    on the input data. For examples fliping images. During Imagenet Competition, Alex
    Krizhevesky (Alexnet) used data augmentation of a factor of 2048, where each class
    on imagenet has 1000 elements.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对输入数据应用一些变换来合成新的训练样本。例如翻转图像。在Imagenet比赛中，Alex Krizhevesky（Alexnet）使用了2048倍的数据增强，其中每个Imagenet类别有1000个元素。
- en: '![](Augmentation1.png)![](Augmentation2.png)'
  id: totrans-726
  prefs: []
  type: TYPE_IMG
  zh: '![](Augmentation1.png)![](Augmentation2.png)'
- en: '![](Augmentation3.png)'
  id: totrans-727
  prefs: []
  type: TYPE_IMG
  zh: '![](Augmentation3.png)'
- en: Automatic Hierarchical representation
  id: totrans-728
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动分层表示
- en: The idea is to let the learning algorithm to find the best representation that
    it can for every layer starting from the inputs to the more deepest ones.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 思想是让学习算法从输入到更深层逐层找到最佳表示。
- en: The shallow layers learn to represent data on it's simpler form and deepest
    layers learn to represent the data with the concepts learned from the previous
    ones.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 浅层学习以更简单的形式表示数据，最深层学习以从前一层学到的概念表示数据。
- en: '![](HieararchicalRepresentation.png)'
  id: totrans-731
  prefs: []
  type: TYPE_IMG
  zh: '![](HieararchicalRepresentation.png)'
- en: '![](DBN_Faces.png)'
  id: totrans-732
  prefs: []
  type: TYPE_IMG
  zh: '![](DBN_Faces.png)'
- en: Old vs New
  id: totrans-733
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 老 vs 新
- en: '* * *'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Actually the only new thing is the usage of something that will learn how to
    represent the data (feature selection) automatically and based on the dataset
    given. Is not about saying that SVM or decision trees are bad, actually some people
    use SVMs at the end of the deep neural network to do classificaion.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，唯一的新东西是使用某种东西自动学习如何表示数据（特征选择），并基于给定的数据集。不是说SVM或决策树不好，事实上有些人在深度神经网络的最后使用SVM来进行分类。
- en: The only point is that the feature selection can be easily adapted to new data.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是特征选择可以很容易地适应新数据。
- en: The biggest advantage on this is that if your problem get more complex you just
    make your model "deeper" and get more data (a lot) to train to your new problem.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的最大优势是，如果您的问题变得更加复杂，您只需使您的模型变得更加“深入”，并获得更多（大量）数据来训练您的新问题。
- en: Some guys from Deep Learning
  id: totrans-738
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习中的一些人
- en: '* * *'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](SomeGuysMachineLearning.png)'
  id: totrans-740
  prefs: []
  type: TYPE_IMG
  zh: '![](SomeGuysMachineLearning.png)'
- en: Next Chapter
  id: totrans-741
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: Next chapter we will learn about Convolution Neural Networks.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习卷积神经网络。
- en: Convolution
  id: totrans-743
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: Convolution
  id: totrans-744
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: Introduction
  id: totrans-745
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Convolution is a mathematical operation that does the integral of the product
    of 2 functions(signals), with one of the signals flipped. For example bellow we
    convolve 2 signals f(t) and g(t).
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一种数学运算，对两个函数（信号）的积进行积分，其中一个信号被翻转。例如，下面我们对两个信号 f(t) 和 g(t) 进行卷积。
- en: '![](Conv1.png)'
  id: totrans-748
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv1.png)'
- en: So the first thing to do is to flip (180 degrees) the signal g, then slide the
    flipped g over f, multiplying and accumulating all it's values.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 所以要做的第一件事是翻转（180度）信号 g，然后将翻转的 g 滑动到 f 上，乘以并累积所有它的值。
- en: The order that you convolve the signals does not matter for the end result,
    so conv(a,b)==conv(b,a)
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 对信号进行卷积的顺序对最终结果没有影响，因此 conv(a,b)==conv(b,a)
- en: On this case consider that the blue signal ![](6003a9f8.png) is our input signal
    and ![](e5429842.png) our kernel, the term kernel is used when you use convolutions
    to filter signals.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，考虑到蓝色信号 ![](6003a9f8.png) 是我们的输入信号， ![](e5429842.png) 是我们的卷积核，当您使用卷积来过滤信号时，术语卷积核就会被使用。
- en: Output signal size 1D
  id: totrans-752
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出信号大小1D
- en: 'On the case of 1d convolution the output size is calculated like this:'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 1维卷积的情况下，输出大小的计算方法如下：
- en: '![](f8e75e8d.png)'
  id: totrans-754
  prefs: []
  type: TYPE_IMG
  zh: '![](f8e75e8d.png)'
- en: Application of convolutions
  id: totrans-755
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积的应用
- en: People use convolution on signal processing for the following use cases
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在信号处理中使用卷积的用例如下
- en: Filter Signals (1d audio, 2d image processing)
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤信号（1维音频，2维图像处理）
- en: Check how much a signal is correlated to another
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查一个信号与另一个信号的相关性有多大
- en: Find patterns on signals
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在信号中找到模式
- en: Simple example in matlab and python(numpy)
  id: totrans-760
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Matlab和Python（numpy）的简单示例
- en: Bellow we convolve to signals x = (0,1,2,3,4) with w = (1,-1,2).
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将信号 x = (0,1,2,3,4) 与 w = (1,-1,2) 进行卷积。
- en: '![](SimpleConv.png)'
  id: totrans-762
  prefs: []
  type: TYPE_IMG
  zh: '![](SimpleConv.png)'
- en: '![](Convolve1d_Python.png)'
  id: totrans-763
  prefs: []
  type: TYPE_IMG
  zh: '![](Convolve1d_Python.png)'
- en: Doing by hand
  id: totrans-764
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动操作
- en: To understand better then concept of convolution let's do the example above
    by hand. Basically we're going to convolve 2 signals (x,w). The first thing is
    to flip W horizontally (Or rotate to left 180 degrees)
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解卷积的概念，让我们手动做上面的例子。基本上我们将对 2 个信号 (x, w) 进行卷积。第一件事是水平翻转 W（或向左旋转 180 度）
- en: '![](Conv1_ManualStart.png)'
  id: totrans-766
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv1_ManualStart.png)'
- en: After that we need to slide the flipped W over the input X
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 之后我们需要将翻转的 W 滑动到输入 X 上
- en: '![](Conv1d_Manual.png)'
  id: totrans-768
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv1d_Manual.png)'
- en: Observe that on steps 3,4,5 the flipped window is completely inside the input
    signal. Those results are called 'valid'. The cases where the flipped window is
    not fully inside the input window(X), we can consider to be zero, or calculate
    what is possible to be calculated, ex on step 1 we multiply 1 by zero, and the
    rest is simply ignored.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在步骤 3、4、5 中，翻转的窗口完全位于输入信号内。这些结果被称为“有效”。当翻转的窗口没有完全位于输入窗口(X)内时，我们可以考虑为零，或者计算可能计算的内容，例如在步骤
    1 中，我们将 1 乘以零，其余部分被简单地忽略。
- en: Input padding
  id: totrans-770
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入填充
- en: In order to keep the convolution result size the same size as the input, and
    to avoid an effect called circular convolution, we pad the signal with zeros.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持卷积结果的大小与输入大小相同，并避免称为循环卷积的效果，我们用零填充信号。
- en: 'Where you put the zeros depends on what you want to do, ie: on the 1d case
    you can concatenate them on the end, but on 2d is normally around the original
    signal'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 将零放在哪里取决于你想要做什么，即：在 1d 情况下，你可以将它们连接在末尾，但在 2d 情况下通常是在原始信号周围。
- en: '![](zeroPadding_0.png)'
  id: totrans-773
  prefs: []
  type: TYPE_IMG
  zh: '![](zeroPadding_0.png)'
- en: '![](zeroPadding.png)'
  id: totrans-774
  prefs: []
  type: TYPE_IMG
  zh: '![](zeroPadding.png)'
- en: 'On matlab you can use the command padarray to pad the input:'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 在 matlab 中，你可以使用 padarray 命令来填充输入：
- en: '[PRE8]'
  id: totrans-776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Transforming convolution to computation graph
  id: totrans-777
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将卷积转换为计算图
- en: In order to calculate partial derivatives of every node inputs and parameters,
    it's easier to transform it to a computational graph. Here I'm going to transform
    the previous 1d convolution, but this can be extended to 2d convolution as well.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算每个节点输入和参数的偏导数，将其转换为计算图会更容易。这里我将转换之前的 1d 卷积，但这也可以扩展到 2d 卷积。
- en: '![](Conv1d_Manual_symbolic.png)'
  id: totrans-779
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv1d_Manual_symbolic.png)'
- en: Here our graph will be created on the valid cases where the flipped kernel(weights)
    will be fully inserted on our input window.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们的图将在翻转的卷积核（权重）完全插入到我们的输入窗口的有效情况下创建。
- en: '![](Simple_1d_Conv_graph.png)'
  id: totrans-781
  prefs: []
  type: TYPE_IMG
  zh: '![](Simple_1d_Conv_graph.png)'
- en: We're going to use this graph in the future to infer the gradients of the inputs
    (x) and weights (w) of the convolution layer
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来使用这个图来推断卷积层的输入 (x) 和权重 (w) 的梯度
- en: 2d Convolution
  id: totrans-783
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2d 卷积
- en: '* * *'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Extending to the second dimension, 2d convolutions are used on image filters,
    and when you would like to find a specific patch on image.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展到第二维，2d 卷积用于图像滤波器，以及当你想要在图像上找到特定的补丁时。
- en: '![](Conv2dUsage1.png)'
  id: totrans-786
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv2dUsage1.png)'
- en: '![](PatternMatch.png)'
  id: totrans-787
  prefs: []
  type: TYPE_IMG
  zh: '![](PatternMatch.png)'
- en: Matlab and python examples
  id: totrans-788
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Matlab 和 python 示例
- en: '![](MatlabConv2_Example.png)'
  id: totrans-789
  prefs: []
  type: TYPE_IMG
  zh: '![](MatlabConv2_Example.png)'
- en: Doing by hand
  id: totrans-790
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动操作
- en: First we should flip the kernel, then slide the kernel on the input signal.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们应该翻转卷积核，然后将卷积核滑动到输入信号上。
- en: Before doing this operation by hand check out the animation showing how this
    sliding works
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动操作之前，看看展示滑动如何工作的动画
- en: '![](Convolution_schematic.gif)'
  id: totrans-793
  prefs: []
  type: TYPE_IMG
  zh: '![](Convolution_schematic.gif)'
- en: '![](Conv2d_Manual.png)'
  id: totrans-794
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv2d_Manual.png)'
- en: Stride
  id: totrans-795
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步幅
- en: By default when we're doing convolution we move our window one pixel at a time
    (stride=1), but some times in convolutional neural networks we move more than
    one pixel. Strides of 2 are used on pooling layers.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当我们进行卷积时，我们每次移动一个像素的窗口（步幅=1），但有时在卷积神经网络中我们会移动多个像素。步幅为 2 用于池化层。
- en: Observe that bellow the red window is moving much more than one pixel time.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在红色窗口下方移动的比一个像素时间更多。
- en: '![](Pooling_schematic.gif)'
  id: totrans-798
  prefs: []
  type: TYPE_IMG
  zh: '![](Pooling_schematic.gif)'
- en: Output size for 2d
  id: totrans-799
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2d 的输出大小
- en: 'If we consider the padding and stride, the output size of convolution is defined
    as:'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考虑填充和步幅，卷积的输出大小定义为：
- en: '![](200d3e57.png)'
  id: totrans-801
  prefs: []
  type: TYPE_IMG
  zh: '![](200d3e57.png)'
- en: F is the size of the kernel, normally we use square kernels, so F is both the
    width and height of the kernel
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: F 是卷积核的大小，通常我们使用方形卷积核，因此 F 既是卷积核的宽度又是高度
- en: Implementing convolution operation
  id: totrans-803
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现卷积操作
- en: '* * *'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The example bellow will convolve a 5x5x3 (WxHx3) input, with a conv layer with
    the following parameters Stride=2, Pad=1, F=3(3x3 kernel), and K=2 (two filters).
    Our input has 3 channels, so we need a 3x3x3 kernel weight. We have 2 filters
    (K=2) so we have 2 output activation (3x3x2). Calculating the output size we have:
    (5 - 3 + 2)/2 + 1 = 3'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子将用一个5x5x3（WxHx3）输入进行卷积，使用以下参数的卷积层 Stride=2, Pad=1, F=3（3x3内核），和 K=2（两个滤波器）。我们的输入有3个通道，所以我们需要一个3x3x3的内核权重。我们有2个滤波器（K=2），所以我们有2个输出激活（3x3x2）。计算输出大小我们有：(5
    - 3 + 2)/2 + 1 = 3
- en: '![](Conv_Example.png)'
  id: totrans-806
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv_Example.png)'
- en: So basically we need to calculate 2 convolutions, one for each 3x3x3 filter
    (w0,w1), and remembering to add the bias.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基本上我们需要计算2个卷积，一个用于每个3x3x3滤波器（w0，w1），并记得添加偏差。
- en: '![](Conv_Vanilla_1.png)'
  id: totrans-808
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv_Vanilla_1.png)'
- en: The code bellow (vanilla version) cannot be used on real life, because it will
    be slow. Usually deep learning libraries do the convolution as a matrix multiplication,
    using im2col/col2im.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码（基础版本）不能在实际生活中使用，因为它会很慢。通常，深度学习库会将卷积作为矩阵乘法进行，使用im2col/col2im。
- en: '[PRE9]'
  id: totrans-810
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next Chapter
  id: totrans-811
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: Next chapter we will learn about Deep Learning.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习有关深度学习的内容。
- en: Convolutional Neural Networks
  id: totrans-813
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Convolutional Neural Networks
  id: totrans-814
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '* * *'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](ConvnetDiagram.png)'
  id: totrans-816
  prefs: []
  type: TYPE_IMG
  zh: '![](ConvnetDiagram.png)'
- en: '![](Conv_Relu_pool_FC.png)'
  id: totrans-817
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv_Relu_pool_FC.png)'
- en: Introduction
  id: totrans-818
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A CNN is composed of layers that filters(convolve) the inputs to get usefull
    information. These convolutional layers have parameters(kernel) that are learned
    so that these filters are adjusted automatically to extract the most useful information
    for the task at hand without feature selection. CNN are better to work with images.
    Normal Neural networks does not fit well on image classification problems
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: CNN由过滤器（卷积）输入组成，以获取有用信息。这些卷积层有参数（内核），可以学习这些过滤器，使得这些过滤器自动调整以从手头的任务中提取最有用的信息，而不需要特征选择。CNN更适合处理图像。普通神经网络不太适合图像分类问题
- en: Comparison of Normal Neural network
  id: totrans-821
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通神经网络的比较
- en: '* * *'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On normal neural networks, we need to convert the image to a single 1d vector
    ![](49e0430d.png),then send this data to a hidden layer which is fully connected.
    On this scenario each neuron will have ![](51d4cd80.png) parameters per neuron.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通神经网络中，我们需要将图像转换为单个1d向量 ![](49e0430d.png)，然后将这些数据发送到完全连接的隐藏层。在这种情况下，每个神经元将每个神经元具有
    ![](51d4cd80.png) 参数。
- en: '![](WeightSharing.png)'
  id: totrans-824
  prefs: []
  type: TYPE_IMG
  zh: '![](WeightSharing.png)'
- en: Common architecture
  id: totrans-825
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的架构
- en: '* * *'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Normally the pattern [CONV->ReLU->Pool->CONV->ReLU->Pool->FC->Softmax_loss(during
    train)] is quite commom.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 通常模式 [CONV->ReLU->Pool->CONV->ReLU->Pool->FC->Softmax_loss(在训练期间)] 是相当常见的。
- en: '![](Common_CNN_Arch.png)'
  id: totrans-828
  prefs: []
  type: TYPE_IMG
  zh: '![](Common_CNN_Arch.png)'
- en: Main actor the convolution layer
  id: totrans-829
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要演员卷积层
- en: The most important operation on the convolutional neural network are the convolution
    layers, imagine a 32x32x3 image if we convolve this image with a 5x5x3 (The filter
    depth must have the same depth as the input), the result will be an activation
    map 28x28x1.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络中最重要的操作是卷积层，想象一下一个32x32x3图像，如果我们将这个图像与一个5x5x3卷积（滤波器的深度必须与输入的深度相同）卷积，结果将是一个激活地图28x28x1。
- en: '![](ezgif.com-optimize.gif)'
  id: totrans-831
  prefs: []
  type: TYPE_IMG
  zh: '![](ezgif.com-optimize.gif)'
- en: The filter will look for a particular thing on all the image, this means that
    it will look for a pattern in the whole image with just one filter.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器将在整个图像中寻找特定的东西，这意味着它将在整个图像中寻找一个模式，只需要一个滤波器。
- en: '![](Conv11.PNG)'
  id: totrans-833
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv11.PNG)'
- en: Now consider that we want our convolution layer to look for 6 different things.
    On this case our convolution layer will have 6 5x5x3 filters. Each one looking
    for a particular pattern on the image.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑我们希望我们的卷积层查找6个不同的东西。在这种情况下，我们的卷积层将有6个5x5x3滤波器。每一个都在图像上寻找一个特定的模式。
- en: '![](Conv2.PNG)'
  id: totrans-835
  prefs: []
  type: TYPE_IMG
  zh: '![](Conv2.PNG)'
- en: By the way the convolution by itself is a linear operation, if we don't want
    to suffer from the same problem of the linear classifers we need to add at the
    end of the convolution layer a non-linear layer. (Normally a Relu)
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，卷积本身是一种线性操作，如果我们不想遭受线性分类器的相同问题，我们需要在卷积层末尾添加一个非线性层。（通常是Relu）
- en: Another important point of using convolution as pattern match is that the position
    where the thing that we want to search on the image is irrelevant. On the case
    of neural networks the model/hypothesis will learn an object on the exact location
    where the object is located during training.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积作为模式匹配的另一个重要点是，我们要在图像上搜索的物体的位置是无关紧要的。在神经网络的情况下，模型/假设将在训练期间物体所在的确切位置学习一个对象。
- en: Convolution layer Hyper parameters
  id: totrans-838
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层超参数
- en: '* * *'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Those are the parameters that are used to configure a convolution layer
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是用于配置卷积层的参数
- en: 'Kernel size(K): Small is better (But if is on the first layer, takes a lot
    of memory)'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核大小（K）：越小越好（但如果是在第一层，会占用大量内存）
- en: 'Stride(S): How many pixels the kernel window will slide (Normally 1, in conv
    layers, and 2 on pooling layers)'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步幅（S）：核窗口将滑动的像素数（通常为1，在卷积层中，池化层为2）
- en: 'Zero Padding(pad): Put zeros on the image border to allow the conv output size
    be the same as the input size (F=1, PAD=0; F=3, PAD=1; F=5, PAD=2; F=7, PAD=3)'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零填充（pad）：在图像边界上放置零以使卷积输出大小与输入大小相同（F=1，PAD=0；F=3，PAD=1；F=5，PAD=2；F=7，PAD=3）
- en: 'Number of filters(F): Number of patterns that the conv layer will look for.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器数量（F）：卷积层将寻找的模式数量。
- en: Output size
  id: totrans-845
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出大小
- en: 'By default the convolution output will always have a result smaller than the
    input. To avoid this behaviour we need to use padding. To calculate the convolution
    output (activation map) size we need this formula:'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，卷积输出将始终比输入小。要避免这种行为，我们需要使用填充。要计算卷积输出（激活图）大小，我们需要使用以下公式：
- en: '![](36938e90.png)'
  id: totrans-847
  prefs: []
  type: TYPE_IMG
  zh: '![](36938e90.png)'
- en: '![](b60caaa0.png)'
  id: totrans-848
  prefs: []
  type: TYPE_IMG
  zh: '![](b60caaa0.png)'
- en: Vizualizing convolution
  id: totrans-849
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化卷积
- en: Here we will see some examples of the convolution window sliding on the input
    image and change some of it's hyper parameters.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将看到一些示例，演示卷积窗口在输入图像上滑动并更改一些超参数。
- en: Convolution with no padding and stride of 1
  id: totrans-851
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不带填充且步幅为1的卷积
- en: Here we have a input 4x4 convolved with a filter 3x3 (K=3) with stride (S=1)
    and padding (pad=0)
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有一个输入4x4与一个滤波器3x3（K=3）卷积，步幅（S=1）和填充（pad=0）
- en: '![](no_padding_no_strides.gif)'
  id: totrans-853
  prefs: []
  type: TYPE_IMG
  zh: '![](no_padding_no_strides.gif)'
- en: Convolution with padding and stride of 1
  id: totrans-854
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带填充和步幅为1的卷积
- en: Now we have an input 5x5 convolved with a filter 3x3 (k=3) with stride (S=1)
    and padding (pad=1). On some libraries there is a feature that always calculate
    the right amount of padding to keep the output spatial dimensions the "same" as
    the input dimensions.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个输入5x5与一个滤波器3x3（k=3）卷积，步幅（S=1）和填充（pad=1）。在某些库中，有一个功能总是计算正确数量的填充以保持输出空间维度与输入维度“相同”。
- en: '![](same_padding_no_strides.gif)'
  id: totrans-856
  prefs: []
  type: TYPE_IMG
  zh: '![](same_padding_no_strides.gif)'
- en: Number of parameters(weights)
  id: totrans-857
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数数量（权重）
- en: '* * *'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Here we show how to calculate the number of parameters used by one convolution
    layer. We will illustrate with a simple example:'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了如何计算一个卷积层使用的参数数量。我们将用一个简单的例子加以说明：
- en: 'Input: 32x32x3, 32x32 RGB image'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：32x32x3，32x32 RGB 图像
- en: 'CONV: Kernel(F):5x5, Stride:1, Pad:2, numFilters:10'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: CONV：核（F）：5x5，步幅：1，填充：2，滤波器数量：10
- en: '![](edc7a412.png)'
  id: totrans-862
  prefs: []
  type: TYPE_IMG
  zh: '![](edc7a412.png)'
- en: You can omit the "+1" parameter (Bias), to simplify calculations.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以省略“+1”参数（偏置），以简化计算。
- en: Amount of memory
  id: totrans-864
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存量
- en: '* * *'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Here we show how to calculate the amount of memory needed on the convolution
    layer.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了如何计算卷积层所需的内存量。
- en: 'Input: 32x32x3, 32x32 RGB image'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：32x32x3，32x32 RGB 图像
- en: 'CONV: Kernel(F):5x5, Stride:1, Pad:2, numFilters:10, as we use padding our
    output volume will be 32x32x10, so the ammount of memory in bytes is: 10240 bytes'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: CONV：核（F）：5x5，步幅：1，填充：2，滤波器数量：10，由于我们使用填充，所以我们的输出体积将是32x32x10，因此内存量为：10240字节
- en: So the amount of memory is basically just the product of the dimensions of the
    output volume which is a 4d tensor.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，内存量基本上只是输出体积尺寸的维度乘积，它是一个4维张量。
- en: '![](41e9054c.png)'
  id: totrans-870
  prefs: []
  type: TYPE_IMG
  zh: '![](41e9054c.png)'
- en: 'Where:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](e768559f.png): Output batch size'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](e768559f.png)：输出批量大小'
- en: '![](48a9667f.png): The outpt volume or on the case of convolution the number
    of filters'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](48a9667f.png)：输出体积或在卷积情况下滤波器的数量'
- en: '![](fdb9624c.png): The height of the output activation map'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](fdb9624c.png)：输出激活图的高度'
- en: '![](6b7ff93.png): The width of the output activation map'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](6b7ff93.png)：输出激活图的宽度'
- en: 1x1 Convolution
  id: totrans-876
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1x1卷积
- en: '* * *'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This type if convolution is normally used to adapt depths, by merging them,
    without changing the spatial information.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的卷积通常用于调整深度，通过合并它们而不改变空间信息。
- en: Substituting Big convolutions
  id: totrans-879
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替换大卷积
- en: '* * *'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Here we explain what is the effect of cascading several small convolutions,
    on the diagram bellow we have 2 3x3 convolution layers. If you start from the
    second layer on the right, one neuron on the second layer, has a 3x3 receptive
    field, and each neuron on the first layer create a 5x5 receptive field on the
    input.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们解释了级联几个小卷积的效果，在下面的图表中，我们有2个3x3卷积层。如果从最右边的第二层开始，第二层上的一个神经元具有3x3的感受野，而第一层上的每个神经元在输入上创建一个5x5的感受野。
- en: So in simpler words cascading can be used to represent bigger ones.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: '![](CascadingConvolutions.png)'
  id: totrans-883
  prefs: []
  type: TYPE_IMG
- en: The new trend on new successful models is to use smaller convolutions, for example
    a 7x7 convolution can be substituted with 3 3x3 convolutions with the same depth.
    This substitution cannot be done on the first conv layer due to the depth mismatch
    between the first conv layer and the input file depth (Unless if your first layer
    has only 3 filters).
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: '![](ConvLayer.png)'
  id: totrans-885
  prefs: []
  type: TYPE_IMG
- en: On the diagram above we substitute one 7x7 convolution by 3 3x3 convolutions,
    observe that between them we have relu layers, so we have more non-linearities.
    Also we have less weights and multiply-add operations so it will be faster to
    compute.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the substitution of a 7x7 convolution
  id: totrans-887
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine a 7x7 convolution, with C filters, being used on a input volume WxHxC
    we can calculate the number of weights as:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: '![](d1009d9b.png)'
  id: totrans-889
  prefs: []
  type: TYPE_IMG
- en: Now if we use 3 3x3 convolutions with C filters, we would have
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '![](f12a1879.png)'
  id: totrans-891
  prefs: []
  type: TYPE_IMG
- en: We still have less parameters, as we need to use Relu between the layers to
    break the linearity (otherwise the conv layers in cascade will appear as a single
    3x3 layer) we have more non-linearity, less parameters, and more performance.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
- en: Substitution on the first layer
  id: totrans-893
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, we cannot substitute large convolutions on the first layer.
    Actually small convolutions on the first layer cause a memory consume explosion.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the problem let's compare the first layer of a convolution neural
    network as been 3x3 with 64 filters and stride of 1 and the same depth with 7x7
    and stride of 2, consider the image size to be 256x256x3.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
- en: '![](6843150f.png)'
  id: totrans-897
  prefs: []
  type: TYPE_IMG
- en: 'TODO: How the stride and convolution size affect the memory consumption'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: Continuing on 3x3 substitution (Bottleneck)
  id: totrans-899
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
- en: It's also possible to simplify the 3x3 convolution with a mechanism called bottleneck.
    This again will have the same representation of a normal 3x3 convolution but with
    less parameters, and more non-linearities.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the substitution is made on the 3x3 convolution that has the same
    depth as the previous layer (On this case 50x50x64)
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Bottleneck.png)'
  id: totrans-903
  prefs: []
  type: TYPE_IMG
- en: Here we calculate how much parameters we use on the bottleneck, remember that
    on 3x3 is ![](3251489b.png)
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
- en: '![](Botleneck_Calc.PNG)'
  id: totrans-905
  prefs: []
  type: TYPE_IMG
- en: So the bottleneck uses ![](b94f9821.png), which is less.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck is also used on microsoft residual network.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
- en: '![](ResnetBottleneck.jpg)'
  id: totrans-908
  prefs: []
  type: TYPE_IMG
- en: Another option to break 3x3xC convolutions is to use 1x3xC, then 3x1xC, this
    has been used on residual googlenet inception layer.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: '![](Subs_3x3.PNG)'
  id: totrans-910
  prefs: []
  type: TYPE_IMG
- en: '![](NewInception.PNG)'
  id: totrans-911
  prefs: []
  type: TYPE_IMG
- en: FC -> Conv Layer Conversion
  id: totrans-912
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
- en: It's possible to convert Fully connected layers to convolution layers and vice-versa,
    but we are more interest on the FC->Conv conversion. This is done to improve performance.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: 'For example imagine a FC layer with output K=4096 and input 7x7x512, the conversion
    would be:'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: 'CONV: Kernel:7x7, Pad:0, Stride:1, numFilters:4096.'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the 2d convolution formula size: ![](26d0dd.png), which will be 1x1x4096.'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 2D 卷积公式大小：![](26d0dd.png)，将是 1x1x4096。
- en: 'In resume what you gain by converting the FC layers to convolution:'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，将 FC 层转换为卷积层所获得的好处：
- en: 'Performance: It''s faster to compute due to the weight sharing'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能：由于权重共享，计算速度更快
- en: You can use images larger than the ones that you trained, without changing nothing
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用比您训练的图像更大的图像，而不需要更改任何内容
- en: You will be able to detect 2 objects on the same image (If you use a bigger
    image) your final output will be bigger then a single row vector.
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将能够在同一图像上检测到2个对象（如果您使用更大的图像），您的最终输出将比单个行向量更大。
- en: '![](MultipleCharacters.gif)'
  id: totrans-922
  prefs: []
  type: TYPE_IMG
  zh: '![](MultipleCharacters.gif)'
- en: Next Chapter
  id: totrans-923
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: Next chapter we will learn about Fully Connected layers.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习关于全连接层的知识。
- en: Fully Connected Layer
  id: totrans-925
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: Fully Connected Layer
  id: totrans-926
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: Introduction
  id: totrans-927
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This chapter will explain how to implement in matlab and python the fully connected
    layer, including the forward and back-propagation.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将解释如何在 matlab 和 python 中实现全连接层，包括前向传播和反向传播。
- en: '![](FullyConnectedLayer.png)'
  id: totrans-930
  prefs: []
  type: TYPE_IMG
  zh: '![](FullyConnectedLayer.png)'
- en: 'First consider the fully connected layer as a black box with the following
    properties: On the forward propagation'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 首先将全连接层视为具有以下特性的黑匣子：在前向传播中
- en: Has 3 inputs (Input signal, Weights, Bias)
  id: totrans-932
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有3个输入（输入信号，权重，偏置）
- en: Has 1 output
  id: totrans-933
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有1个输出
- en: On the back propagation
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中
- en: Has 1 input (dout) which has the same size as output
  id: totrans-935
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有1个输入（dout），与输出大小相同
- en: Has 3 (dx,dw,db) outputs, that has the same size as the inputs
  id: totrans-936
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有3个（dx，dw，db）输出，与输入的大小相同
- en: Neural network point of view
  id: totrans-937
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从神经网络的角度来看
- en: '* * *'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](FC_Layer_NN.png)'
  id: totrans-939
  prefs: []
  type: TYPE_IMG
  zh: '![](FC_Layer_NN.png)'
- en: 'Just by looking the diagram we can infer the outputs:'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看图表，我们就可以推断出输出：
- en: '![](ce09aba1.png)'
  id: totrans-941
  prefs: []
  type: TYPE_IMG
  zh: '![](ce09aba1.png)'
- en: 'Now vectorizing (put on matrix form): (Observe 2 possible versions)'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行向量化（转换为矩阵形式）：（观察2种可能的版本）
- en: '![](9e63ed7d.png)'
  id: totrans-943
  prefs: []
  type: TYPE_IMG
  zh: '![](9e63ed7d.png)'
- en: Depending on the format that you choose to represent W attention to this because
    it can be confusing.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您选择表示 W 的格式，要注意这一点，因为可能会令人困惑。
- en: 'For example if we choose X to be a column vector, our matrix multiplication
    must be:'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们选择 X 为列向量，则我们的矩阵乘法必须是：
- en: '![](109a7901.png)'
  id: totrans-946
  prefs: []
  type: TYPE_IMG
  zh: '![](109a7901.png)'
- en: Computation graph point of view
  id: totrans-947
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从计算图的角度来看
- en: '* * *'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In order to discover how each input influence the output (backpropagation) is
    better to represent the algorithm as a computation graph.
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 为了发现每个输入如何影响输出（反向传播），最好将算法表示为计算图。
- en: '![](Graph_Fully_Connected_Layer.png)'
  id: totrans-950
  prefs: []
  type: TYPE_IMG
  zh: '![](Graph_Fully_Connected_Layer.png)'
- en: Now for the backpropagation let's focus in one of the graphs, and apply what
    we learned so far on backpropagation.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于反向传播，让我们专注于其中一个图表，并应用到目前为止学到的关于反向传播的知识。
- en: '![](Graph_Fully_Connected_Layer_Backprop.png)'
  id: totrans-952
  prefs: []
  type: TYPE_IMG
  zh: '![](Graph_Fully_Connected_Layer_Backprop.png)'
- en: Summarizing the calculation for the first output (y1), consider a global error
    L(loss) and ![](6b7d96f4.png)
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 总结计算第一个输出（y1），考虑全局误差 L（损失）和 ![](6b7d96f4.png)
- en: '![](a0d260d8.png)'
  id: totrans-954
  prefs: []
  type: TYPE_IMG
  zh: '![](a0d260d8.png)'
- en: '![](3f51c64e.png)'
  id: totrans-955
  prefs: []
  type: TYPE_IMG
  zh: '![](3f51c64e.png)'
- en: '![](34f8377a.png)'
  id: totrans-956
  prefs: []
  type: TYPE_IMG
  zh: '![](34f8377a.png)'
- en: Also extending to the second output (y2)
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 还扩展到第二个输出（y2）
- en: '![](c78583bb.png)'
  id: totrans-958
  prefs: []
  type: TYPE_IMG
  zh: '![](c78583bb.png)'
- en: '![](468a2295.png)'
  id: totrans-959
  prefs: []
  type: TYPE_IMG
  zh: '![](468a2295.png)'
- en: '![](51bc5114.png)'
  id: totrans-960
  prefs: []
  type: TYPE_IMG
  zh: '![](51bc5114.png)'
- en: 'Merging the results, for dx:'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 合并结果，对于 dx：
- en: '![](d269dc97.png)'
  id: totrans-962
  prefs: []
  type: TYPE_IMG
  zh: '![](d269dc97.png)'
- en: On the matrix form
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵形式中
- en: '![](7f5d0677.png), or ![](3fe949c9.png).'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: '![](7f5d0677.png)，或者 ![](3fe949c9.png)。'
- en: Depending on the format that you choose to represent X (as a row or column vector),
    attention to this because it can be confusing.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您选择表示 X（作为行向量或列向量）的格式，要注意这一点，因为可能会令人困惑。
- en: 'Now for dW It''s important to not that every gradient has the same dimension
    as it''s original value, for instance dW has the same dimension as W, in other
    words:'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于 dW，重要的是要注意每个梯度与其原始值具有相同的维度，例如 dW 与 W 具有相同的维度，换句话说：
- en: '![](db7dc2ed.png)'
  id: totrans-967
  prefs: []
  type: TYPE_IMG
  zh: '![](db7dc2ed.png)'
- en: '![](fca2e763.png)'
  id: totrans-968
  prefs: []
  type: TYPE_IMG
  zh: '![](fca2e763.png)'
- en: And dB ![](c88de6e2.png)
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 以及 dB ![](c88de6e2.png)
- en: Expanding for bigger batches
  id: totrans-970
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展为更大的批次
- en: '* * *'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: All the examples so far, deal with single elements on the input, but normally
    we deal with much more than one example at a time. For instance on GPUs is common
    to have batches of 256 images at the same time. The trick is to represent the
    input signal as a 2d matrix [NxD] where N is the batch size and D the dimensions
    of the input signal. So if you consider the CIFAR dataset where each digit is
    a 28x28x1 (grayscale) image D will be 784, so if we have 10 digits on the same
    batch our input will be [10x784].
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of argument, let''s consider our previous samples where the vector
    X was represented like ![](8a28794f.png), if we want to have a batch of 4 elements
    we will have:'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
- en: '![](4ffc17e6.png)'
  id: totrans-974
  prefs: []
  type: TYPE_IMG
- en: In this case W must be represented in a way that support this matrix multiplication,
    so depending how it was created it may need to be transposed
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: '![](eaba624a.png)'
  id: totrans-976
  prefs: []
  type: TYPE_IMG
- en: 'Continuing the forward propagation will be computed as:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: '![](3c17fe53.png)'
  id: totrans-978
  prefs: []
  type: TYPE_IMG
- en: One point to observe here is that the bias has repeated 4 times to accommodate
    the product X.W that in this case will generate a matrix [4x2]. On matlab the
    command "repmat" does the job. On python it does automatically.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: '![](MatlabRepmat.PNG)'
  id: totrans-980
  prefs: []
  type: TYPE_IMG
- en: Using Symbolic engine
  id: totrans-981
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping to implementation is good to verify the operations on Matlab
    or Python (sympy) symbolic engine. This will help visualize and explore the results
    before acutally coding the functions.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic forward propagation on Matlab
  id: totrans-984
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: Here after we defined the variables which will be symbolic, we create the matrix
    W,X,b then calculate ![](c5476bc3.png), compare the final result with what we
    calculated before.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: '![](SymbForwardMatlab.PNG)'
  id: totrans-987
  prefs: []
  type: TYPE_IMG
- en: Symbolic backward propagation on Matlab
  id: totrans-988
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: Now we also confirm the backward propagation formulas. Observe the function
    "latex" that convert an expression to latex on matlab
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: '![](SymbBackwardMatlab.PNG) Here I''ve just copy and paste the latex result
    of dW or " ![](a06dcfa1.png) " from matlab'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '![](d485c17d.png)'
  id: totrans-992
  prefs: []
  type: TYPE_IMG
- en: Input Tensor
  id: totrans-993
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: Our library will be handling images, and most of the time we will be handling
    matrix operations on hundreds of images at the same time. So we must find a way
    to represent them, here we will represent batch of images as a 4d tensor, or an
    array of 3d matrices. Bellow we have a batch of 4 rgb images (width:160, height:120).
    We're going to load them on matlab/python and organize them one a 4d matrix
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: '![](ImgsBatch.png)'
  id: totrans-996
  prefs: []
  type: TYPE_IMG
- en: Observe that in matlab the image becomes a matrix 120x160x3\. Our tensor will
    be 120x160x3x4 ![](MatlabLoadImages.png)
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: On Python before we store the image on the tensor we do a transpose to convert
    out image 120x160x3 to 3x120x160, then to store on a tensor 4x3x120x160
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
- en: '![](PythonLoadImages.png)'
  id: totrans-999
  prefs: []
  type: TYPE_IMG
- en: Python Implementation
  id: totrans-1000
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forward Propagation
  id: totrans-1001
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_FC_Forward.png)'
  id: totrans-1003
  prefs: []
  type: TYPE_IMG
- en: Backward Propagation
  id: totrans-1004
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_FC_Backward.png)'
  id: totrans-1006
  prefs: []
  type: TYPE_IMG
- en: Matlab Implementation
  id: totrans-1007
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
- en: One special point to pay attention is the way that matlab represent high-dimension
    arrays in contrast with matlab. Also another point that may cause confusion is
    the fact that matlab represent data on col-major order and numpy on row-major
    order.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 要特别注意 Matlab 与 Python 在表示高维数组方面的方式。还有一个可能会引起混淆的点是，Matlab 将数据表示为列主序，而 numpy 则为行主序。
- en: '![](Row_Col_Major.jpg)'
  id: totrans-1010
  prefs: []
  type: TYPE_IMG
  zh: '![](Row_Col_Major.jpg)'
- en: Multidimensional arrays in python and matlab
  id: totrans-1011
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 和 Matlab 中的多维数组
- en: '* * *'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](MultiMatlabArray.png)'
  id: totrans-1013
  prefs: []
  type: TYPE_IMG
  zh: '![](MultiMatlabArray.png)'
- en: One difference on how matlab and python represent multidimensional arrays must
    be noticed. We want to create a 4 channel matrix 2x3\. So in matlab you need to
    create a array (2,3,4) and on python it need to be (4,2,3)
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意 Matlab 和 Python 表示多维数组的一点差异。我们想要创建一个 2x3 的 4 通道矩阵。因此，在 Matlab 中，您需要创建一个数组
    (2,3,4)，在 Python 中，它需要是 (4,2,3)。
- en: '![](Matlab_MultiDim.png)'
  id: totrans-1015
  prefs: []
  type: TYPE_IMG
  zh: '![](Matlab_MultiDim.png)'
- en: Matlab Reshape order
  id: totrans-1016
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Matlab 重塑顺序
- en: '* * *'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As mentioned before matlab will run the command reshape one column at a time,
    so if you want to change this behavior you need to transpose first the input matrix.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，Matlab 将按列一次重塑一列，因此如果您想更改此行为，您需要首先转置输入矩阵。
- en: '![](Matlab_Reshape.png)'
  id: totrans-1019
  prefs: []
  type: TYPE_IMG
  zh: '![](Matlab_Reshape.png)'
- en: If you are dealing with more than 2 dimensions you need to use the "permute"
    command to transpose. Now on Python the default of the reshape command is one
    row at a time, or if you want you can also change the order (This options does
    not exist in matlab)
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您处理的是多于 2 个维度的情况，您需要使用 "permute" 命令进行转置。现在在 Python 中，reshape 命令的默认值是一次一行，或者如果您愿意，您也可以更改顺序（在
    Matlab 中不存在此选项）。
- en: '![](Python_Reshape.png)'
  id: totrans-1021
  prefs: []
  type: TYPE_IMG
  zh: '![](Python_Reshape.png)'
- en: 'Bellow we have a reshape on the row-major order as a new function:'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们将按行主序重塑为一个新函数：
- en: '![](Matlab_reshape_row.png)'
  id: totrans-1023
  prefs: []
  type: TYPE_IMG
  zh: '![](Matlab_reshape_row.png)'
- en: 'The other option would be to avoid this permutation reshape is to have the
    weight matrix on a different order and calculate the forward propagation like
    this:'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种排列重塑的另一种选择是将权重矩阵放在不同的顺序上，并像这样计算前向传播：
- en: '![](20ffcef4.png)'
  id: totrans-1025
  prefs: []
  type: TYPE_IMG
  zh: '![](20ffcef4.png)'
- en: With x as a column vector and the weights organized row-wise, on the example
    that is presented we keep using the same order as the python example.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 当 x 作为列向量而权重按行组织时，在所展示的例子中，我们保持与 Python 示例相同的顺序。
- en: Forward Propagation
  id: totrans-1027
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播
- en: '* * *'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](Matlab_Forward.png)'
  id: totrans-1029
  prefs: []
  type: TYPE_IMG
  zh: '![](Matlab_Forward.png)'
- en: Backward Propagation
  id: totrans-1030
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: '* * *'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](Matlab_Backward.png)'
  id: totrans-1032
  prefs: []
  type: TYPE_IMG
  zh: '![](Matlab_Backward.png)'
- en: Next Chapter
  id: totrans-1033
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一章
- en: '* * *'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Next chapter we will learn about Relu layers
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章我们将学习关于 ReLU 层的内容
- en: Relu Layer
  id: totrans-1036
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU 层
- en: Rectified-Linear unit Layer
  id: totrans-1037
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整流线性单元层
- en: Introduction
  id: totrans-1038
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We will start this chapter explaining how to implement in Python/Matlab the
    ReLU layer.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从解释如何在 Python/Matlab 中实现 ReLU 层开始。
- en: '![](ReluLayer.png)'
  id: totrans-1041
  prefs: []
  type: TYPE_IMG
  zh: '![](ReluLayer.png)'
- en: In simple words, the ReLU layer will apply the function ![](13304fde.png) in
    all elements on a input tensor, without changing it's spatial or depth information.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，ReLU 层将在输入张量的所有元素上应用函数 ![](13304fde.png)，而不改变其空间或深度信息。
- en: '![](ReluMatlabSimpleExample.png) ![](Relu.jpeg)'
  id: totrans-1043
  prefs: []
  type: TYPE_IMG
  zh: '![](ReluMatlabSimpleExample.png) ![](Relu.jpeg)'
- en: From the picture above, observe that all positive elements remain unchanged
    while the negatives become zero. Also the spatial information and depth are the
    same.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以看出，所有正元素保持不变，而负元素变为零。同时空间信息和深度信息保持不变。
- en: 'Thinking about neural networks, it''s just a new type of Activation function,
    but with the following features:'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑神经网络，这只是一种新型的激活函数，但具有以下特征：
- en: Easy to compute (forward/backward propagation)
  id: totrans-1046
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容易计算（前向/反向传播）
- en: Suffer much less from vanishing gradient on deep models
  id: totrans-1047
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在深度模型上遭受的梯度消失要少得多
- en: A bad point is that they can irreversibly die if you use a big learning rate
  id: totrans-1048
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不好的一点是，如果你使用较大的学习率，它们可能会不可逆地死亡。
- en: Forward propagation
  id: totrans-1049
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: '* * *'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Change all negative elements to zero while retaining the value of the positive
    elements. No spatial/depth information is changed.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有负元素更改为零，同时保留正元素的值。不改变空间/深度信息。
- en: Python forward propagation
  id: totrans-1052
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 前向传播
- en: '* * *'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](Python_forward_relu.png)'
  id: totrans-1054
  prefs: []
  type: TYPE_IMG
  zh: '![](Python_forward_relu.png)'
- en: Matlab forward propagation
  id: totrans-1055
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Matlab 前向传播
- en: '* * *'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Backward propagation
  id: totrans-1057
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: '* * *'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Basically we're just applying the max(0,x) function to every ![](bbe484e5.png)
    input element. From the back-propagation chapter we can notice that the gradient
    dx will be zero if the element ![](aa4b6abf.png)is negative or ![](8166a298.png)
    if the element is positive.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
- en: '![](ReluGraph.png)'
  id: totrans-1060
  prefs: []
  type: TYPE_IMG
- en: Python backward propagation
  id: totrans-1061
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_ReLU_Backward_Propagation.png)'
  id: totrans-1063
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  id: totrans-1064
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Dropout layers
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: Dropout Layer
  id: totrans-1067
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dropout Layer
  id: totrans-1068
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1069
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is a technique used to improve over-fit on neural networks, you should
    use Dropout along with other techniques like L2 Regularization.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: '![](DropoutLayers.png)'
  id: totrans-1072
  prefs: []
  type: TYPE_IMG
- en: Bellow we have a classification error (Not including loss), observe that the
    test/validation error is smaller using dropout
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: '![](With_Without_Dropout.png)'
  id: totrans-1074
  prefs: []
  type: TYPE_IMG
- en: As other regularization techniques the use of dropout also make the training
    loss error a little worse. But that's the idea, basically we want to trade training
    performance for more generalization. Remember that's more capacity you add on
    your model (More layers, or more neurons) more prone to over-fit it becomes.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we have a plot showing both training, and validation loss with and without
    dropout
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout_loss_val_train.jpeg)'
  id: totrans-1077
  prefs: []
  type: TYPE_IMG
- en: How it works
  id: totrans-1078
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: Basically during training half of neurons on a particular layer will be deactivated.
    This improve generalization because force your layer to learn with different neurons
    the same "concept".
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
- en: During the prediction phase the dropout is deactivated.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout.jpeg)'
  id: totrans-1082
  prefs: []
  type: TYPE_IMG
- en: Where to use Dropout layers
  id: totrans-1083
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: Normally some deep learning models use Dropout on the fully connected layers,
    but is also possible to use dropout after the max-pooling layers, creating some
    kind of image noise augmentation.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-1086
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement this neuron deactivation, we create a mask(zeros and ones)
    during forward propagation. This mask is applied to the layer outputs during training
    and cached for future use on back-propagation. As explained before this dropout
    mask is used only during training.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: 'On the backward propagation we''re interested on the neurons that was activated
    (we need to save mask from forward propagation). Now with those neurons selected
    we just back-propagate dout. The dropout layer has no learnable parameters, just
    it''s input (X). During back-propagation we just return "dx". In other words:
    ![](a6e8a0fe.png)'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: Python Forward propagation
  id: totrans-1090
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout_forward_python_code.png)'
  id: totrans-1092
  prefs: []
  type: TYPE_IMG
- en: Python Backward propagation
  id: totrans-1093
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: '![](dropout_backward_python_code.png)'
  id: totrans-1095
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  id: totrans-1096
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Convolution layer
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
- en: Convolution Layer
  id: totrans-1099
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution Layer
  id: totrans-1100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain how to implement the convolution layer on python and
    matlab.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
- en: '![](ConvolutionLayer.png)'
  id: totrans-1104
  prefs: []
  type: TYPE_IMG
- en: 'In simple terms the convolution layer, will apply the convolution operator
    on all images on the input tensor, and also transform the input depth to match
    the number of filters. Bellow we explain it''s parameters and signals:'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: 'N: Batch size (Number of images on the 4d tensor)'
  id: totrans-1106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'F: Number of filters on the convolution layer'
  id: totrans-1107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'kW/kH: Kernel Width/Height (Normally we use square images, so kW=kH)'
  id: totrans-1108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H/W: Image height/width (Normally H=W)'
  id: totrans-1109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H''/W'': Convolved image height/width (Remains the same as input if proper
    padding is used)'
  id: totrans-1110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stride: Number of pixels that the convolution sliding window will travel.'
  id: totrans-1111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Padding: Zeros added to the border of the image to keep the input and output
    size the same.'
  id: totrans-1112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Depth: Volume input depth (ie if the input is a RGB image depth will be 3)'
  id: totrans-1113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output depth: Volume output depth (same as F)'
  id: totrans-1114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward propagation
  id: totrans-1115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: On the forward propagation, you must remember, that we're going to "convolve"
    each input depth with a different filter, and each filter will look for something
    different on the image.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '![](filters_per_layer.png)'
  id: totrans-1118
  prefs: []
  type: TYPE_IMG
- en: Here observe that all neurons(flash-lights) from layer 1 share the same set
    of weights, other filters will look for different patterns on the image.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: Matlab Forward propagation
  id: totrans-1120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: Basically we can consider the previous "convn_vanilla" function on the [Convolution
    chapter](convolution_split_000.html) and apply for each depth on the input and
    output.
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Forward_CONV.png)'
  id: totrans-1123
  prefs: []
  type: TYPE_IMG
- en: Python Forward propagation
  id: totrans-1124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: The only point to observe here is that due to the way the multidimensional arrays
    are represented in python our tensors will have different order.
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_Forward_CONV.png)'
  id: totrans-1127
  prefs: []
  type: TYPE_IMG
- en: Back-propagation
  id: totrans-1128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: In order to derive the convolution layer back-propagation it's easier to think
    on the 1d convolution, the results will be the same for 2d.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: So doing a 1d convolution, between a signal ![](666fe02f.png) and ![](678ee8ba.png),
    and without padding we will have ![](e271041e.png), where ![](a751091f.png). Here
    flip can be consider as a 180 degrees rotation.
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Manual_symbolic.png)'
  id: totrans-1132
  prefs: []
  type: TYPE_IMG
- en: Now we convert all the "valid cases" to a computation graph, observe that for
    now we're adding the bias because it is used on the convolution layer.
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the graphs are basically the same as the fully connected layer,
    the only difference is that we have shared weights.
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: '![](Simple_1d_Conv_Bias.png)'
  id: totrans-1135
  prefs: []
  type: TYPE_IMG
- en: Now changing to the back-propagation
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: '![](Simple_1d_Conv_Back.png)'
  id: totrans-1137
  prefs: []
  type: TYPE_IMG
- en: If you follow the computation graphs backward, as was presented on the [Backpropagation
    chapter](backpropagation_split_000.html) we will have the following formulas for
    ![](a537b4ba.png), which means how the loss will change with the input X ![](5b05ec2e.png)
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider some things:'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
- en: dX must have the same size of X, so we need padding
  id: totrans-1140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dout must have the same size of Y, which in this case is 3 (Gradient input)
  id: totrans-1141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To save programming effort we want to calculate the gradient as a convolution
  id: totrans-1142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On dX gradient all elements are been multiplied by W so we're probably convolving
    W and dout
  id: totrans-1143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following the output size rule for the 1d convolution: ![](34481da6.png) Our
    desired size is 3, our original input size is 3, and we''re going to convolve
    with the W matrix that also have 3 elements. So we need to pad our input with
    2 zeros.'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Backprop_dX.png)'
  id: totrans-1145
  prefs: []
  type: TYPE_IMG
- en: 'The convolution above implement all calculations needed for ![](a537b4ba.png),
    so in terms of convolution: ![](1af9a957.png)'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: Now let's continue for ![](7086761.png), considering that they must have the
    same size as W. ![](85992f4e.png)
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: Again by just looking to the expressions that we took from the graph we can
    see that is possible to represent them as a convolution between dout and X. Also
    as the output will be 3 elements, there is no need to do padding.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv1d_Backprop_dW.png)'
  id: totrans-1149
  prefs: []
  type: TYPE_IMG
- en: 'So in terms of convolution the calculations for ![](a06dcfa1.png) will be:
    ![](85ad1578.png) Just one point to remember, if you consider X to be the kernel,
    and dout the signal, X will be automatically flipped. ![](2bd929bc.png)'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: Now for the bias, the calculation will be similar to the Fully Connected layer.
    Basically we have one bias per filter (depth) ![](ce8280a3.png)
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Notes
  id: totrans-1152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: 'Before jumping to the code some points need to be reviewed:'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use some parameter (ie: Stride/Pad) during forward propagation you need
    to apply them on the backward propagation.'
  id: totrans-1155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On Python our multidimensional tensor will be "input=[N x Depth x H x W]" on
    matlab they will be "input=[H x W x Depth x N]"
  id: totrans-1156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As mentioned before the gradients of a input, has the same size as the input
    itself "size(x)==size(dx)"
  id: totrans-1157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matlab Backward propagation
  id: totrans-1158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: '![](Matlab_Backward_CONV.png)'
  id: totrans-1160
  prefs: []
  type: TYPE_IMG
- en: Python Backward propagation
  id: totrans-1161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_Backward_CONV.png)'
  id: totrans-1163
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  id: totrans-1164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Pooling layer
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: Making faster
  id: totrans-1167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making faster
  id: totrans-1168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we show a way to convert your convolution operation into a matrix
    multiplication. This has the advantage to compute faster, at the expense of more
    memory usage. We employ the **im2col** operation that will transform the input
    image or batch into a matrix, then we multiply this matrix with a reshaped version
    of our kernel. Then at the end we reshape this multiplied matrix back to an image
    with the **col2im** operation.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: Im2col
  id: totrans-1172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: As shown on previous source code, we use a lot for for-loops to implement the
    convolutions, while this is useful for learning purpose, it's not fast enough.
    On this section we will learn how to implement convolutions on a vectorized fashion.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: First, if we inspect closer the code for convolution is basically a dot-product
    between the kernel filter and the local regions selected by the moving window,
    that sample a patch with the same size as our kernel.
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
- en: What would happens if we expand all possible windows on memory and perform the
    dot product as a matrix multiplication. Answer 200x or more speedups, at the expense
    of more memory consumption.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: '![](Convolution_With_Im2col.png)'
  id: totrans-1177
  prefs: []
  type: TYPE_IMG
- en: For example, if the input is [227x227x3] and it is to be convolved with 11x11x3
    filters at stride 4 and padding 0, then we would take [11x11x3] blocks of pixels
    in the input and stretch each block into a column vector of size ![](ce877a37.png)
    = 363.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: Calculating with input 227 with stride 4 and padding 0, gives ((227-11)/4)+1
    = 55 locations along both width and height, leading to an output matrix X_col
    of size [363 x 3025].
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: Here every column is a stretched out receptive field (patch with depth) and
    there are 55*55 = 3025 of them in total.
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize how we calculate the im2col output sizes:'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-1182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The weights of the CONV layer are similarly stretched out into rows. For example,
    if there are 96 filters of size [11x11x3] this would give a matrix W_row of size
    [96 x 363], where 11x11x3=363
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: '![](im2col_operation.png)'
  id: totrans-1184
  prefs: []
  type: TYPE_IMG
- en: After the image and the kernel are converted, the convolution can be implemented
    as a simple matrix multiplication, in our case it will be W_col[96 x 363] multiplied
    by X_col[363 x 3025] resulting as a matrix [96 x 3025], that need to be reshaped
    back to [55x55x96].
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: This final reshape can also be implemented as a function called col2im.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
- en: Notice that some implementations of im2col will have this result transposed,
    if this is the case then the order of the matrix multiplication must be changed.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
- en: '![](Im2Col_cs231n.png)'
  id: totrans-1188
  prefs: []
  type: TYPE_IMG
- en: Forward graph
  id: totrans-1189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
- en: In order to help the usage of im2col with convolution and also to derive the
    back-propagation, let's show the convolution with im2col as a graph. Here the
    input tensor is single a 3 channel 4x4 image. That will pass to a convolution
    layer with S:1 P:0 K:2 and F:1 (Output volume).
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Graph_Im2col.png)'
  id: totrans-1192
  prefs: []
  type: TYPE_IMG
- en: Backward graph
  id: totrans-1193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
- en: Using the im2col technique the computation graph resembles the FC layer with
    the same format ![](479bdf40.png), the difference that now we have a bunch of
    reshapes, transposes and the im2col block.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
- en: About the reshapes and transposes during back propagation you just need to invert
    their operations using again another reshape or transpose, the only important
    thing to remember is that if you use a reshape row major during forward propagation
    you need to use a reshape row major on the backpropagation.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
- en: The only point to pay attention is the im2col backpropagation operation. The
    issue is that it cannot be implemented as a simple reshape. This is because the
    patches could actually overlap (depending on the stride), so you need to sum the
    gradients where the patches intersect.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Graph_Im2col_Backward.png)'
  id: totrans-1198
  prefs: []
  type: TYPE_IMG
- en: Matlab forward propagation
  id: totrans-1199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-1201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Matlab backward propagation
  id: totrans-1202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-1203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Matlab im2col
  id: totrans-1204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-1206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Matlab im2col backward propagation
  id: totrans-1207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-1209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Python example for forward propagation
  id: totrans-1210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-1212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Python example for backward propagation
  id: totrans-1213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-1215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Im2col and Col2im sources in python
  id: totrans-1216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This implementation will receive a image on the format of a 3 dimension tensor
    [channels, rows, cols] and will create a 2d matrix on the format [rows=(new_h*new_w),
    cols=(kw*kw*C)] notice that this algorithm will output the transposed version
    of the diagram above.
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-1218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-1219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-1220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Smaller example
  id: totrans-1221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
- en: To make things simpler on our heads, follow the simple example of convolving
    X[3x3] with W[2x2]
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
- en: '![](simple_im2col.png)'
  id: totrans-1224
  prefs: []
  type: TYPE_IMG
- en: '![](im2col_matlab_example.png)'
  id: totrans-1225
  prefs: []
  type: TYPE_IMG
- en: Pooling Layer
  id: totrans-1226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling Layer
  id: totrans-1227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
- en: '![](MaxPoolingLayer.png)'
  id: totrans-1230
  prefs: []
  type: TYPE_IMG
- en: 'The pooling layer, is used to reduce the spatial dimensions, but not depth,
    on a convolution neural network, model, basically this is what you gain:'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
- en: By having less spatial information you gain computation performance
  id: totrans-1232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Less spatial information also means less parameters, so less chance to over-fit
  id: totrans-1233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You get some translation invariance
  id: totrans-1234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some projects don't use pooling, specially when they want to "learn" some object
    specific position. Learn how to play atari games.
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: On the diagram bellow we show the most common type of pooling the max-pooling
    layer, which slides a window, like a normal convolution, and get the biggest value
    on the window as the output.
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
- en: '![](Pooling_Simple_max.png)'
  id: totrans-1237
  prefs: []
  type: TYPE_IMG
- en: 'The most important parameters to play:'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: H1 x W1 x Depth_In x N'
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stride: Scalar that control the amount of pixels that the window slide.'
  id: totrans-1240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: Kernel size'
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regarding it''s Output H2 x W2 x Depth_Out x N:'
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
- en: '![](d8085eaa.png)'
  id: totrans-1243
  prefs: []
  type: TYPE_IMG
- en: It's also valid to point out that there is no learnable parameters on the pooling
    layer. So it's backpropagation is simpler.
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  id: totrans-1245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
- en: The window movement mechanism on pooling layers is the same as convolution layer,
    the only change is that we will select the biggest value on the window.
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
- en: Python Forward propagation
  id: totrans-1248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_maxpool_forward.png)'
  id: totrans-1250
  prefs: []
  type: TYPE_IMG
- en: Matlab Forward propagation
  id: totrans-1251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
- en: Backward Propagation
  id: totrans-1253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
- en: From the [backpropagation chapter](backpropagation_split_000.html) we learn
    that the max node simply act as a router, giving the input gradient "dout" to
    the input that has value bigger than zero.
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
- en: '![](MaxGate.png) You can consider that the max pooling use a series of max
    nodes, on it''s computation graph. So consider the backward propagation of the
    max pooling layer as a product between a mask containing all elements that were
    selected during the forward propagation and dout.'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: '![](BackPropagation_MaxPool.png)'
  id: totrans-1257
  prefs: []
  type: TYPE_IMG
- en: In other words the gradient with respect to the input of the max pooling layer
    will be a tensor make of zeros except on the places that was selected during the
    forward propagation.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: Python Backward propagation
  id: totrans-1259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
- en: '![](Python_maxpool_backward.png)'
  id: totrans-1261
  prefs: []
  type: TYPE_IMG
- en: Improving performance
  id: totrans-1262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
- en: On future chapter we will learn a technique that improves the convolution performance,
    until them we will stick with the naive implementation.
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
- en: Next Chapter
  id: totrans-1265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about Batch Norm layer
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: Batch Norm layer
  id: totrans-1268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch Norm layer
  id: totrans-1269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we will learn about the batch norm layer. Previously we said
    that [feature scaling](https://www.gitbook.com/book/leonardoaraujosantos/artificial-inteligence/edit#/edit/master/feature_scaling.md)
    make the job of the gradient descent easier. Now we will extend this idea and
    normalize the activation of every Fully Connected layer or Convolution layer during
    training. This also means that while we're training we will select an batch calculate
    it's mean and standard deviation.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
- en: You can think that the batch-norm will be some kind of adaptive (or learnable)
    pre-processing block with trainable parameters. Which also means that we need
    to back-propagate them.
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
- en: The original batch-norm paper can be found [here](http://arxiv.org/pdf/1502.03167v3.pdf).
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the list of advantages of using Batch-Norm:'
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
- en: Improves gradient flow, used on very deep models (Resnet need this)
  id: totrans-1276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Allow higher learning rates
  id: totrans-1277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce dependency on initialization
  id: totrans-1278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gives some kind of regularization (Even make Dropout less important but keep
    using it)
  id: totrans-1279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a rule of thumb if you use Dropout+BatchNorm you don't need L2 regularization
  id: totrans-1280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It basically force your activations (Conv,FC ouputs) to be unit standard deviation
    and zero mean.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
- en: To each learning batch of data we apply the following normalization.
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: '![](f4932fb.png)'
  id: totrans-1283
  prefs: []
  type: TYPE_IMG
- en: '![](Compute_BatchNorm.png)'
  id: totrans-1284
  prefs: []
  type: TYPE_IMG
- en: The output of the batch norm layer, has the ![](87a87a8c.png) are parameters.
    Those parameters will be learned to best represent your activations. Those parameters
    allows a learnable (scale and shift) factor ![](dd16f1ee.png)
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: 'Now summarizing the operations:'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_fp.png)'
  id: totrans-1287
  prefs: []
  type: TYPE_IMG
- en: Here, ![](9fe1930b.png) is a small number, 1e-5.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
- en: Where to use the Batch-Norm layer
  id: totrans-1289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
- en: 'The batch norm layer is used after linear layers (ie: FC, conv), and before
    the non-linear layers (relu). There is actually 2 batch norm implementations one
    for FC layer and the other for conv layers.'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
- en: '![](BatchNorm_Placement.png)'
  id: totrans-1292
  prefs: []
  type: TYPE_IMG
- en: Test time
  id: totrans-1293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: At prediction time that batch norm works differently. The mean/std are not computed
    based on the batch. Instead, we need to build a estimate during training of the
    mean/std of the whole dataset(population) for each batch norm layer on your model.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
- en: One approach to estimating the population mean and variance during training
    is to use an [exponential moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average).
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
- en: '![](749e2654.png)'
  id: totrans-1297
  prefs: []
  type: TYPE_IMG
- en: 'Where: ![](c5ae95b1.png): Current and previous estimation ![](de87fcce.png):
    Represents the degree of weighting decrease, a constant smoothing factor between
    0 and 1 ![](c03f5ce1.png): Current value (could be mean or std) that we''re trying
    to estimate'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
- en: Normally when we implement this layer we have some kind of flag that detects
    if we're on training or testing.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: As reference we can find some tutorials with [Tensorflow](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html)
    or [manually on python](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html).
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-1301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier we need to know how to backpropagate on the batch-norm
    layer, first as we did with other layers we need to create the computation graph.
    After this step we need to calculate the derivative of each node with respect
    to it's inputs.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: Computation Graph
  id: totrans-1304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to find the partial derivatives on back-propagation is better to visualize
    the algorithm as a computation graph:'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_computation_graph.png)'
  id: totrans-1306
  prefs: []
  type: TYPE_IMG
- en: New nodes
  id: totrans-1307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By inspecting this graph we have some new nodes (![](48a4007c.png), ![](cf0feaac.png),
    ![](f52524b5.png), ![](59e67941.png)). To simplify things you can use [Wolfram
    alpha](https://www.wolframalpha.com/) to find the derivatives. For backpropagate
    other nodes refer to the [Back-propagation chapter](https://www.gitbook.com/book/leonardoaraujosantos/artificial-inteligence/edit#/edit/master/backpropagation.md)
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
- en: '[Block 1/x](https://www.wolframalpha.com/input/?i=derivative+1%2Fx)'
  id: totrans-1309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_1_over_x.png)'
  id: totrans-1310
  prefs: []
  type: TYPE_IMG
- en: 'In other words:'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: '![](7b972ff1.png)'
  id: totrans-1312
  prefs: []
  type: TYPE_IMG
- en: 'Where: ![](b8367cc3.png) means the cached (or saved) input from the forward
    propagation. ![](9a616dbf.png) means the previous block gradient'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
- en: '[Block sqrt(x-epsilon)](https://www.wolframalpha.com/input/?i=derivative+of+sqrt(x-epsilon)'
  id: totrans-1314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_sqrt_x.png)'
  id: totrans-1315
  prefs: []
  type: TYPE_IMG
- en: 'In other words:'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
- en: '![](496a1ddb.png)'
  id: totrans-1317
  prefs: []
  type: TYPE_IMG
- en: 'Where: ![](b8367cc3.png): the cached (or saved) input from the forward propagation.
    ![](9a616dbf.png): the previous block gradient ![](9fe1930b.png): Some small number
    0.00005'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
- en: '[Block x^2](https://www.wolframalpha.com/input/?i=derivative+of+x%5E2)'
  id: totrans-1319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_x2.png)'
  id: totrans-1320
  prefs: []
  type: TYPE_IMG
- en: 'In other words:'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: '![](5d7daa76.png)'
  id: totrans-1322
  prefs: []
  type: TYPE_IMG
- en: Block Summation
  id: totrans-1323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![](BlockBackprop_SUM.png)'
  id: totrans-1324
  prefs: []
  type: TYPE_IMG
- en: Like the SUM block this block will copy the input gradient dout equally to all
    it's inputs. So for all elements in X we will divide by N and multiply by dout.
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-1326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python Forward Propagation
  id: totrans-1327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_fp_python.png)'
  id: totrans-1329
  prefs: []
  type: TYPE_IMG
- en: Python Backward Propagation
  id: totrans-1330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
- en: '![](batch_norm_bp_python.png)'
  id: totrans-1332
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  id: totrans-1333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
- en: Next chapter we will learn about how to optimize our model weights.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
- en: Model Solver
  id: totrans-1336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Solver
  id: totrans-1337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
- en: The mission of the model solver is to find the best set of parameters, that
    minimize the train/accuracy errors. On this chapter we will give a UML description
    with some piece of python/matlab code that allows you implement it yourself.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
- en: '![](ClassDiagram.png)'
  id: totrans-1341
  prefs: []
  type: TYPE_IMG
- en: 'From the UML description we can infer some information about the Solver class:'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
- en: It uses the training set, and has a reference to your model
  id: totrans-1343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Uses different type of optimizers(ex: SGD, ADAM, SGD with momentum)'
  id: totrans-1344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep tracks of all the loss, accuracy during the training phase
  id: totrans-1345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the set of parameters, that achieved best validation performance
  id: totrans-1346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usage example
  id: totrans-1347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
- en: '![](Solver_Usage.png)'
  id: totrans-1349
  prefs: []
  type: TYPE_IMG
- en: Train operation
  id: totrans-1350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the method called when you actually want to start a model training,
    the methods Step, Check_Accuracy are called inside the Train method:'
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
- en: Calculate number of iterations per epoch, based on number of epochs, train size,
    and batch size
  id: totrans-1353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call step, for each iteration
  id: totrans-1354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decay the learning rate
  id: totrans-1355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the validation accuracy
  id: totrans-1356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cache the best parameters based on validation accuracy
  id: totrans-1357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step operation
  id: totrans-1358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically during the step operation the following operations are done:'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
- en: Extract a batch from the training set.
  id: totrans-1361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the model loss and gradients
  id: totrans-1362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a parameter update with one of the optimizers.
  id: totrans-1363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check Accuracy
  id: totrans-1364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
- en: This method basically is called at the end of each epoch. Basically it uses
    the current set of parameters, and predict the whole validation set. The objective
    is at the end get the accuracy. ![](7f8bb141.png)
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
- en: Model loss operation
  id: totrans-1367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned during the "Step" operation that we get the model loss and gradients.
    This operation is implemented by the "getLoss" method. Consider the following
    basic model.
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleModelLoss.png) Bellow we have the "getLoss" function for the previous
    simple model.'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
- en: '![](getLoss.png)'
  id: totrans-1371
  prefs: []
  type: TYPE_IMG
- en: Also bellow we have the "softmax_loss" function including "dout", ![](50803af0.png)
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
- en: '![](Softmax_Loss_Layer.png)'
  id: totrans-1373
  prefs: []
  type: TYPE_IMG
- en: Object Localization and Detection
  id: totrans-1374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object Localization and Detection
  id: totrans-1375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On this chapter we're going to learn about using convolution neural networks
    to localize and detect objects on images
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
- en: '![](LocalizationDetection.png)'
  id: totrans-1379
  prefs: []
  type: TYPE_IMG
- en: RCNN
  id: totrans-1380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast RCNN
  id: totrans-1381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster RCNN
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yolo
  id: totrans-1383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSD
  id: totrans-1384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Localize objects with regression
  id: totrans-1385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression is about returning a number instead of a class, in our case we're
    going to return 4 numbers (x0,y0,width,height) that are related to a bounding
    box. You train this system with an image an a ground truth bounding box, and use
    L2 distance to calculate the loss between the predicted bounding box and the ground
    truth.
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
- en: '![](LocalizationRegression1.png)'
  id: totrans-1387
  prefs: []
  type: TYPE_IMG
- en: Normally what you do is attach another fully connected layer on the last convolution
    layer
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
- en: '![](LocalizationRegression2.png)'
  id: totrans-1389
  prefs: []
  type: TYPE_IMG
- en: This will work only for one object at a time.
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
- en: Some people attach the regression part after the last convolution (Overfeat)
    layer, while others attach after the fully connected layer (RCNN). Both works.
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
- en: Comparing bounding box prediction accuracy
  id: totrans-1392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
- en: Basically we need to compare if the Intersect Over Union (ioU) between the prediction
    and the ground truth is bigger than some threshold (ex > 0.5)
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
- en: '![](Intersection-over-Union-IoU-of.png)'
  id: totrans-1395
  prefs: []
  type: TYPE_IMG
- en: RCNN
  id: totrans-1396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RCNN (Regions + CNN) is a method that relies on a external region proposal system.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
- en: '![](RCNNSimple.png)'
  id: totrans-1398
  prefs: []
  type: TYPE_IMG
- en: 'The problem of RCNN is that it''s never made to be fast, for instance the steps
    to train the network are these:'
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: Take a pre-trained imagenet cnn (ex Alexnet)
  id: totrans-1400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-train the last fully connected layer with the objects that need to be detected
    + "no-object" class
  id: totrans-1401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get all proposals(=~2000 p/image), resize them to match the cnn input, then
    save to disk.
  id: totrans-1402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train SVM to classify between object and background (One binary SVM for each
    class)
  id: totrans-1403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'BB Regression: Train a linear regression classifier that will output some correction
    factor'
  id: totrans-1404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 3 Save and pre-process proposals
  id: totrans-1405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](Step3RCNN.png)'
  id: totrans-1406
  prefs: []
  type: TYPE_IMG
- en: Step 5 (Adjust bounding box)
  id: totrans-1407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](Step5RCNN.png)'
  id: totrans-1408
  prefs: []
  type: TYPE_IMG
- en: Fast RCNN
  id: totrans-1409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fast RCNN method receive region proposals from some external system (Selective
    search). This proposals will sent to a layer (Roi Pooling) that will resize all
    regions with their data to a fixed size. This step is needed because the fully
    connected layer expect that all the vectors will have same size
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
- en: '![](Fast_RCNN.png)'
  id: totrans-1411
  prefs: []
  type: TYPE_IMG
- en: Proposals example, boxes=[r, x1, y1, x2, y2]
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
- en: '![](Proposals.png)'
  id: totrans-1413
  prefs: []
  type: TYPE_IMG
- en: '![](Fast_RCnn_Caffe_LastPart.png)'
  id: totrans-1414
  prefs: []
  type: TYPE_IMG
- en: Still depends on some external system to give the region proposals (Selective
    search)
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
- en: Roi Pooling layer
  id: totrans-1416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
- en: '![](RoiPoolingLayer.png)'
  id: totrans-1418
  prefs: []
  type: TYPE_IMG
- en: It's a type of max-pooling with a pool size dependent on the input, so that
    the output always has the same size. This is done because fully connected layer
    always expected the same input size.
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
- en: '![](RoiPoolingLayerCaffe.png)'
  id: totrans-1420
  prefs: []
  type: TYPE_IMG
- en: The inputs of the Roi layer will be the proposals and the last convolution layer
    activations. For example consider the following input image, and it's proposals.
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
- en: Input image
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
- en: '![](InImage.png)'
  id: totrans-1423
  prefs: []
  type: TYPE_IMG
- en: Two proposed regions
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
- en: '![](InImageRegions.png)'
  id: totrans-1425
  prefs: []
  type: TYPE_IMG
- en: 'Now the activations on the last convolution layer (ex: conv5)'
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv5_Activations.png)'
  id: totrans-1427
  prefs: []
  type: TYPE_IMG
- en: For each convolution activation (each cell from the image above) the Roi Pooling
    layer will resize, the region proposals (in red) to the same resolution expected
    on the fully connected layer. For example consider the selected cell in green.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv5_Activation_and_Proposals.png)'
  id: totrans-1429
  prefs: []
  type: TYPE_IMG
- en: 'Here the output will be:'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
- en: '![](Roi_PoolingLayer1.png)'
  id: totrans-1431
  prefs: []
  type: TYPE_IMG
- en: '![](Roi_PoolingLayer2.png)'
  id: totrans-1432
  prefs: []
  type: TYPE_IMG
- en: Faster RCNN
  id: totrans-1433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
- en: '![](Faster_Rcnn.png)'
  id: totrans-1435
  prefs: []
  type: TYPE_IMG
- en: The main idea is use the last (or deep) conv layers to infer region proposals.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
- en: Faster-RCNN consists of two modules.
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
- en: 'RPN (Region proposals): Gives a set of rectangles based on deep convolution
    layer'
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fast-RCNN Roi Pooling layer: Classify each proposal, and refining proposal
    location'
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Region proposal Network
  id: totrans-1440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
- en: Here we break on a block diagram how Faster RCNN works.
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
- en: Get a trained (ie imagenet) convolution neural network
  id: totrans-1443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get feature maps from the last (or deep) convolution layer
  id: totrans-1444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a region proposal network that will decide if there is an object or not
    on the image, and also propose a box location
  id: totrans-1445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give results to a custom (python) layer
  id: totrans-1446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give proposals to a ROI pooling layer (like Fast RCNN)
  id: totrans-1447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After all proposals get reshaped to a fix size, send to a fully connected layer
    to continue the classification
  id: totrans-1448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](RegionProposalNetwork.png)'
  id: totrans-1449
  prefs: []
  type: TYPE_IMG
- en: '![](RPN_Network.png)'
  id: totrans-1450
  prefs: []
  type: TYPE_IMG
- en: How it works
  id: totrans-1451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Basically the RPN slides a small window (3x3) on the feature map, that classify
    what is under the window as object or not object, and also gives some bounding
    box location.
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: For every slidding window center it creates fixed k anchor boxes, and classify
    those boxes as been object or not.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
- en: '![](RPN_Sliding.png)'
  id: totrans-1454
  prefs: []
  type: TYPE_IMG
- en: Faster RCNN training
  id: totrans-1455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the paper, each network was trained separately, but we also can train it
    jointly. Just consider the model having 4 losses.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
- en: RPN Classification (Object or not object)
  id: totrans-1457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RPN Bounding box proposal
  id: totrans-1458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast RCNN Classification (Normal object classification)
  id: totrans-1459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast RCNN Bounding-box regression (Improve previous BB proposal)
  id: totrans-1460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](FasterRCNNTrain.png)'
  id: totrans-1461
  prefs: []
  type: TYPE_IMG
- en: Faster RCNN results
  id: totrans-1462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best result now is Faster RCNN with a resnet 101 layer.
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
- en: '![](FasterRCNNSpeedComparison.png)'
  id: totrans-1464
  prefs: []
  type: TYPE_IMG
- en: Complete Faster RCNN diagram
  id: totrans-1465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This diagram represents the complete structure of the Faster RCNN using VGG16,
    I've found on a github project [here](https://github.com/mitmul/chainer-faster-rcnn).
    It uses a framework called [Chainer](https://github.com/pfnet/chainer) which is
    a complete framework using only python (Sometimes cython).
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
- en: '![](Faster%20R-CNN.png)'
  id: totrans-1467
  prefs:
  - PREF_H2
  type: TYPE_IMG
- en: Next Chapter
  id: totrans-1468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will discuss a different type of object detector called
    single shot detectors.
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
- en: Single Shot Detectors
  id: totrans-1471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Single Shot detectors
  id: totrans-1472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous methods of object detection all share one thing in common: they
    have one part of their network dedicated to providing region proposals followed
    by a high quality classifier to classify these proposals. These methods are very
    accurate but come at a big computational cost (low frame-rate), in other words
    they are not fit to be used on embedded devices.'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
- en: Another way of doing object detection is by combining these two tasks into one
    network. We can do this by instead of having a network produce proposals we instead
    have a set of pre defined boxes in which to look for objects.
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
- en: Using convolutional features maps from later layers of a network we run small
    conv filters over these features maps to predict class scores and bounding box
    offsets.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the family of object detectors that follow this strategy:'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD: Uses different activation maps (multiple-scales) for prediction of classes
    and bounding boxes'
  id: totrans-1479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'YOLO: Uses a single activation map for prediction of classes and bounding boxes'
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R-FCN(Region based Fully-Convolution Neural Networks): Like Faster Rcnn, but
    faster due to less computation ber box.'
  id: totrans-1481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multibox: asdasdas'
  id: totrans-1482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these multiple scales helps to achieve a higher mAP(mean average precision)
    by being able to detect objects with different sizes on the image.
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
- en: Summarising the strategy of these methods
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
- en: Train a CNN with regression(box) and classification objective (loss function).
  id: totrans-1485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use sliding window (conv) and non-maxima suppression during prediction on the
    conv feature maps (output of conv-relu)
  id: totrans-1486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On this kind of detectors it is typical to have a collection of boxes overlaid
    on the image at different spatial locations, scales and aspect ratios that act
    as “anchors” (sometimes called “priors” or “default boxes”).
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
- en: 'A model is then trained to make two predictions for each anchor:'
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
- en: A discrete class prediction for each anchor
  id: totrans-1489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A continuous prediction of an offset by which the anchor needs to be shifted
    to fit the ground-truth bounding box.
  id: totrans-1490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also the loss used on this methods are a combination of the 2 objectives, localization(regression)
    and classification.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
- en: Image Segmentation
  id: totrans-1492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Segmentation
  id: totrans-1493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to learn how to classify each pixel on the image, the idea is
    to create a map of all detected object areas on the image. Basically what we want
    is the image bellow where every pixel has a label.
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we're going to learn how convolutional neural networks (CNN)
    can do the job.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
- en: '![](ImageSegmentation.PNG)'
  id: totrans-1498
  prefs: []
  type: TYPE_IMG
- en: Fully Convolutional network for segmentation
  id: totrans-1499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
- en: A Fully Convolutional neural network (FCN) is a normal CNN, where the last fully
    connected layer is substituted by another convolution layer with a large "receptive
    field". The idea is to capture the global context of the scene (Tell what we have
    on the image and also give some rude location where it is).
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
- en: '![](Fully_Convolutional_Network_Semantic.PNG)'
  id: totrans-1502
  prefs: []
  type: TYPE_IMG
- en: Just remember that when we convert our last fully connected (FC) layer to a
    convolutional layer we gain some form of localization if we look where we have
    more activations.
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN.jpg)'
  id: totrans-1504
  prefs: []
  type: TYPE_IMG
- en: The idea is that if we choose our new last conv layer to be big enough we will
    have this localization effect scaled up to our input image size.
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from normal CNN to FCN
  id: totrans-1506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we convert a normal CNN used for classification, ie: Alexnet to
    a FCN used for segmentation.'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to remember this is how Alexnet looks like:'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexNet_2.png)'
  id: totrans-1510
  prefs: []
  type: TYPE_IMG
- en: Bellow is also show the parameters for each layer
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexNet_1.jpg)'
  id: totrans-1512
  prefs: []
  type: TYPE_IMG
- en: On Alexnet the inputs are fixed to be 227x227, so all the pooling effects will
    scale down the image from 227x227 to 55x55, 27x27, 13x13, then finally a single
    row vector on the FC layers.
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexNet_0.jpg)'
  id: totrans-1514
  prefs: []
  type: TYPE_IMG
- en: Now let's look on the steps needed to do the conversion.
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
- en: 1) We start with a normal CNN for classification with
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_1.png)'
  id: totrans-1517
  prefs: []
  type: TYPE_IMG
- en: 2) The second step is to convert all the FC layers to convolution layers 1x1
    we don't even need to change the weights at this point. (This is already a fully
    convolutional neural network). The nice property of FCN networks is that we can
    now use any image size.
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_2.png)'
  id: totrans-1519
  prefs: []
  type: TYPE_IMG
- en: Observe here that with a FCN we can use a different size H x N.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_3.png)'
  id: totrans-1521
  prefs: []
  type: TYPE_IMG
- en: 3) The last step is to use a "deconv or transposed convolution" layer to recover
    the activation positions to something meaningful related to the image size. Imagine
    that we're just scaling up the activation size to the same image size.
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
- en: This last "upsampling" layer also have lernable parameters.
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
- en: '![](FCN_CONV_4.png)'
  id: totrans-1524
  prefs: []
  type: TYPE_IMG
- en: 'Now with this structure we just need to find some "ground truth" and to end
    to end learning, starting from e pre-trainned network ie: Imagenet.'
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this approach is that we loose some resolution by just doing
    this because the activations were downscaled on a lot of steps.
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
- en: '![](FirstResultFCN_No_Skips.png)'
  id: totrans-1527
  prefs: []
  type: TYPE_IMG
- en: To solve this problem we also get some activation from previous layers and sum
    them together. This process is called "skip" from the creators of this algorithm.
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
- en: Even today (2016) the winners on Imagenet on the Segmentation category, used
    an ensemble of FCN to win the competition.
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
- en: Those up-sampling operations used on skip are also learn-able.
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
- en: '![](Skip_Layers_FCN.png)'
  id: totrans-1531
  prefs: []
  type: TYPE_IMG
- en: Bellow we show the effects of this "skip" process notice how the resolution
    of the segmentation improves after some "skips"
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
- en: '![](AllSkips_FCN.png)'
  id: totrans-1533
  prefs: []
  type: TYPE_IMG
- en: '![](SkipConnections.png)'
  id: totrans-1534
  prefs: []
  type: TYPE_IMG
- en: Transposed convolution layer (deconvolution "bad name")
  id: totrans-1535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
- en: Basically the idea is to scale up, the scale down effect made on all previous
    layers.
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
- en: '![](Conv_Deconv.PNG)'
  id: totrans-1538
  prefs: []
  type: TYPE_IMG
- en: '![](Deconv_exp.PNG)'
  id: totrans-1539
  prefs: []
  type: TYPE_IMG
- en: '![](animUpsampling.gif)'
  id: totrans-1540
  prefs: []
  type: TYPE_IMG
- en: It has this bad name because the upsamping forward propagation is the convolution
    backpropagation and the upsampling backpropagation is the convolution forward
    propagation.
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
- en: Also in caffe source code it is wrong called "deconvolution"
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
- en: Extreme segmentation
  id: totrans-1543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
- en: There is another thing that we can do to avoid those "skiping" steps and also
    give better segmentation.
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
- en: '![](Deconvnet.png)'
  id: totrans-1546
  prefs: []
  type: TYPE_IMG
- en: This architechture is called "Deconvnet" which is basically another network
    but now with all convolution and pooling layers reversed. As you may suspect this
    is heavy, it takes 6 days to train on a TitanX. But the results are really good.
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is that the trainning is made in 2 stages.
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
- en: Also Deconvnets suffer less than FCN when there are small objects on the scene.
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
- en: The deconvolution network output a probability map with the same size as the
    input.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
- en: '![](DeconvnetResults.png)'
  id: totrans-1551
  prefs: []
  type: TYPE_IMG
- en: Unpooling
  id: totrans-1552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
- en: Besides the deconvolution layer we also need now the unpooling layer. The max-pooling
    operation is non-invertible, but we can approximate, by recording the positions
    (Max Location switches) where we located the biggest values (during normal max-pool),
    then use this positions to reconstruct the data from the layer above (on this
    case a deconvolution)
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
- en: '![](UnPoolinDiagram.png)'
  id: totrans-1555
  prefs: []
  type: TYPE_IMG
- en: '![](Unpooling_1.png)'
  id: totrans-1556
  prefs: []
  type: TYPE_IMG
- en: '![](UnpoolResults.png)'
  id: totrans-1557
  prefs: []
  type: TYPE_IMG
- en: Next Chapter
  id: totrans-1558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will discuss some libraries that support deep learning
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
- en: GoogleNet
  id: totrans-1561
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GoogleNet
  id: totrans-1562
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1563
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
- en: '![](GoogleNet.png)'
  id: totrans-1565
  prefs: []
  type: TYPE_IMG
- en: On this chapter you will learn about the googleNet (Winning architecture on
    ImageNet 2014) and it's inception layers.
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
- en: '![](ImagenetTable.png)'
  id: totrans-1567
  prefs: []
  type: TYPE_IMG
- en: googleNet has 22 layer, and almost 12x less parameters (So faster and less then
    Alexnet and much more accurate.
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
- en: Their idea was to make a model that also could be used on a smart-phone (Keep
    calculation budget around 1.5 billion multiply-adds on prediction).
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
- en: Inception Layer
  id: totrans-1570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the inception layer is to cover a bigger area, but also keep a fine
    resolution for small information on the images. So the idea is to convolve in
    parallel different sizes from the most accurate detailing (1x1) to a bigger one
    (5x5).
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that a series of gabor filters with different sizes, will handle
    better multiple objects scales. With the advantage that all filters on the inception
    layer are learnable.
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward way to improve performance on deep learning is to use
    more layers and more data, googleNet use 9 inception modules. The problem is that
    more parameters also means that your model is more prone to overfit. So to avoid
    a parameter explosion on the inception layers, all bottleneck techniques are exploited.
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
- en: '![](Naive_Version.png)'
  id: totrans-1575
  prefs: []
  type: TYPE_IMG
- en: '![](InceptionModules.png)'
  id: totrans-1576
  prefs: []
  type: TYPE_IMG
- en: '![](inception_1x1.png)'
  id: totrans-1577
  prefs: []
  type: TYPE_IMG
- en: Using the bottleneck approaches we can rebuild the inception module with more
    non-linearities and less parameters. Also a max pooling layer is added to summarize
    the content of the previous layer. All the results are concatenated one after
    the other, and given to the next layer.
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
- en: Caffe Example
  id: totrans-1579
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we present 2 inception layers on cascade from the original googleNet.
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
- en: '![](caffe_inception.png)'
  id: totrans-1582
  prefs: []
  type: TYPE_IMG
- en: Residual Net
  id: totrans-1583
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Residual Net
  id: totrans-1584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will present the 2016 state of the art on object classification.
    The ResidualNet it's basically a 150 deep convolution neural network made by equal
    "residual" blocks.
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
- en: The problem is for real deep networks (more than 30 layers), all the known techniques
    (Relu, dropout, batch-norm, etc...) are not enough to do a good end-to-end training.
    This contrast with the common "empirical proven knowledge" that deeper is better.
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the residual network is use blocks that re-route the input, and
    add to the concept learned from the previous layer. The idea is that during learning
    the next layer will learn the concepts of the previous layer plus the input of
    that previous layer. This would work better than just learn a concept without
    a reference that was used to learn that concept.
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
- en: Another way to visualize their solution is remember that the back-propagation
    of a sum node will replicate the input gradient with no degradation.
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_building_block.png)'
  id: totrans-1591
  prefs: []
  type: TYPE_IMG
- en: Bellow we show an example of a 34-deep residual net.
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
- en: '![](residualnet_34.png)'
  id: totrans-1593
  prefs: []
  type: TYPE_IMG
- en: The ResidualNet creators proved empiricaly that it's easier to train a 34-layer
    residual compared to a 34-layer cascaded (Like VGG).
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
- en: Observe that on the end of the residual net there is only one fully connected
    layer followed by a previous average pool.
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
- en: Residual Block
  id: totrans-1596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
- en: At it's core the residual net is formed by the following structure.
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_block.png)'
  id: totrans-1599
  prefs: []
  type: TYPE_IMG
- en: Basically this jump and adder creates a path for back-propagation, allowing
    even really deep models to be trained.
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
- en: As mention before the Batch-Norm block alleviate the network initialization,
    but it can be omitted for not so deep models (less than 50 layers).
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
- en: Again like googlenet we must use bottlenecks to avoid a parameter explosion.
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
- en: '![](residual_bottleneck.png)'
  id: totrans-1603
  prefs: []
  type: TYPE_IMG
- en: Just to remember for the bottleneck to work the previous layer must have same
    depth.
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
- en: Caffe Example
  id: totrans-1605
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
- en: Here we show 2 cascaded residual blocks form residual net, due to difficulties
    with batch-norm layers, they were omitted but still residual net gives good results.
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
- en: '![](caffe_residual.png)'
  id: totrans-1608
  prefs: []
  type: TYPE_IMG
- en: Deep Learning Libraries
  id: totrans-1609
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Libraries
  id: totrans-1610
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1611
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Discussion, and some examples on the most common deep learning libraries:'
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
- en: Caffe
  id: totrans-1613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch
  id: totrans-1614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  id: totrans-1615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theano
  id: totrans-1616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNTK
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe
  id: totrans-1618
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
- en: One of the most basic characteristic of caffe is that is easy to train simple
    non recurrent models.
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
- en: 'Most cool features:'
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
- en: Good Performance, allows training with multiple GPUs
  id: totrans-1622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation for CPU and GPU
  id: totrans-1623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code is easy to read
  id: totrans-1624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow layer definition in Python
  id: totrans-1625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has bidings for Python and Matlab
  id: totrans-1626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows network definition with text language (No need to write code)
  id: totrans-1627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast dataset access through LMDB
  id: totrans-1628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows network vizualization
  id: totrans-1629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has web interface (Digits)
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caffe Main classes:'
  id: totrans-1631
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](CaffeOverview.jpg)'
  id: totrans-1632
  prefs: []
  type: TYPE_IMG
- en: 'Blob: Used to store data and diffs(Derivatives)'
  id: totrans-1633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Layer: Some operation that transform a bottom blob(input) to top blobs(outputs)'
  id: totrans-1634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Net: Set of connected layers'
  id: totrans-1635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solver: Call Net forward and backward propagation, update weights using gradient
    methods (Gradient descent, SGD, adagrad, etc...)'
  id: totrans-1636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe training/validation files
  id: totrans-1637
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: path/to/image/1.jpg [label]
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
- en: Simple example
  id: totrans-1639
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here a logistic regression classifier. Imagine as a neural network with one
    layer and a sigmoid (cross-entropy softmax) non-linearity.
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
- en: '![](Caffe_Logistic.jpg)'
  id: totrans-1641
  prefs: []
  type: TYPE_IMG
- en: '![](Caffe_Proto_Logistic.jpg)'
  id: totrans-1642
  prefs: []
  type: TYPE_IMG
- en: Caffe Cons
  id: totrans-1643
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Need to write C++ / Cuda code for new layers
  id: totrans-1644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad to write protofiles for big networks (Resnet, googlenet)
  id: totrans-1645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad to experience new architectures (Mainstream version does not support Fast
    RCNN)
  id: totrans-1646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch
  id: totrans-1647
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
- en: Really good for research, the problem is that use a new language called Lua.
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
- en: Torch Pros
  id: totrans-1650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flexible
  id: totrans-1651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very easy source code
  id: totrans-1652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy biding with C/C++
  id: totrans-1653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web interface (Digits)
  id: totrans-1654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch Cons
  id: totrans-1655
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: New language Lua
  id: totrans-1656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to load data from directories
  id: totrans-1657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Matlab bidings
  id: totrans-1658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less Plug and play than caffe
  id: totrans-1659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not easy for RNN
  id: totrans-1660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theano
  id: totrans-1661
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
- en: Theano Cons
  id: totrans-1663
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More manual
  id: totrans-1664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No matlab biding
  id: totrans-1665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slower than other frameworks
  id: totrans-1666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No much pre-trained models
  id: totrans-1667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorflow
  id: totrans-1668
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow Pros
  id: totrans-1670
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flexible
  id: totrans-1671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good for RNN
  id: totrans-1672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow distributed training
  id: totrans-1673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorboard for signal visualization
  id: totrans-1674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Numpy
  id: totrans-1675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorflow Cons
  id: totrans-1676
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not much pre-trained models
  id: totrans-1677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Support for new object detection features (Ex Roi pooling)
  id: totrans-1678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No support for datasets like Caffe
  id: totrans-1679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slower than Caffe for single GPU training
  id: totrans-1680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNTK
  id: totrans-1681
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
- en: CNTK Pros
  id: totrans-1683
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flexible
  id: totrans-1684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good for RNN
  id: totrans-1685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows distributed training
  id: totrans-1686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNTK Cons
  id: totrans-1687
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: No visualization
  id: totrans-1688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any error CNTK crash
  id: totrans-1689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No simple source code to read
  id: totrans-1690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New language (ndl) to describe networks
  id: totrans-1691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No current matlab or python bindings
  id: totrans-1692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1693
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For research use Torch or Tensorflow (Last option Theano)
  id: totrans-1694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training convnets or use pre-trained models use Caffe
  id: totrans-1695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CS231n Deep learning course summary
  id: totrans-1696
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](DeepLibrariesOverview.jpg)'
  id: totrans-1697
  prefs: []
  type: TYPE_IMG
- en: 'Get features from known model (Alexnet, Googlenet, Vgg): Use caffe'
  id: totrans-1698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine tune known models (Alexnet, Googlenet, Vgg): Use Caffe'
  id: totrans-1699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Captioning: Torch or Tensorflow'
  id: totrans-1700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Segmentation: Caffe, Torch'
  id: totrans-1701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object Detection: Caffe with python layers, Torch (More work)'
  id: totrans-1702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language Modelling: Torch, Theano'
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implement Bath Norm: Torch, Theano or Tensorflow'
  id: totrans-1704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally Tensorflow can be used in all cased that torch can, but if you need
    to understand what a specific layer does, or if you need to create a new layer,
    use torch instead of tensorflow. Torch is preferable on those cases, because the
    layer source code is more easy to read in torch.
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
- en: Next Chapter
  id: totrans-1706
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
- en: On the next chapter we will discuss Distributed Learning
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  id: totrans-1709
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1710
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned on previous chapters, unsupervised learning is about learning information
    without the label information.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
- en: Here the term information means, "structure" for instance you would like to
    know how many groups exist in your dataset, even if you don't know what those
    groups mean.
  id: totrans-1713
  prefs: []
  type: TYPE_NORMAL
- en: Also we use unsupervised learning to visualize your dataset, in order to try
    to learn some insight from the data.
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
- en: Unlabeled data example
  id: totrans-1715
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following dataset ![](750c80ea.png) (X has 2 features)
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleDataUnlabeled.png)'
  id: totrans-1718
  prefs: []
  type: TYPE_IMG
- en: One type of unsupervised learning algorithm called "clustering" is used to infer
    how many distinct groups exist on your dataset.
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
- en: '![](SimpleDataClustered.png)'
  id: totrans-1720
  prefs: []
  type: TYPE_IMG
- en: Here we still don't know what those groups means, but we know that there are
    4 groups that seems very distinct. On this case we choose a low dimensional dataset
    ![](6a67a3f7.png) but on real life it could be thousands of dimensions, ie ![](fe46b6da.png)
    for a grayscale 28x28 image.
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  id: totrans-1722
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to improve classification response time (not prediction performance)
    and sometimes for visualizing your high dimension dataset (2D, 3D), we use dimesionality
    reduction techniques (ie: PCA, T-Sne).'
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
- en: For example the [MNIST datset](http://yann.lecun.com/exdb/mnist/) is composed
    with 60,000 training examples of (0..9) digits, each one with 784 dimensions.
    This high dimensionality is due to the fact that each digit is a 28x28 grayscale
    image.
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
- en: '![](MNIST_samples.png)'
  id: totrans-1726
  prefs: []
  type: TYPE_IMG
- en: It would be difficult to vizualize this dataset, so one option is to reduce
    it's dimensions to something visible on the monitor (2D,3D).
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
- en: '![](mnist_tSNE.jpg)'
  id: totrans-1728
  prefs: []
  type: TYPE_IMG
- en: Here is easy to observe that a classifier could have problems to differentiate
    the digit 1 and 7.
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is that this gives us some hint on how good is our current
    set of features.
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-1731
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
- en: We can also use neural networks to do dimensionality reduction the idea is that
    we have a neural network topology that approximate the input on the output layer.
    On the middle the autoencoder has smaller layer. After training the middle layer
    has a compressed version (lossy) of the input.
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
- en: '![](AutoEncoder.png)'
  id: totrans-1734
  prefs: []
  type: TYPE_IMG
- en: Convolution Neural network pre-train
  id: totrans-1735
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we don't need the label information to train autoencoders, we can use them
    as a pre-trainer to our convolution neural network. So in the future we can start
    your training with the weights initialized from unsupervised training.
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
- en: '![](conv_autoencoder.png)'
  id: totrans-1737
  prefs: []
  type: TYPE_IMG
- en: 'Some examples of this technique can be found here:'
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
- en: '[With Python](https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/)'
  id: totrans-1739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[With Torch](https://siavashk.github.io/2016/02/22/autoencoder-imagenet/)'
  id: totrans-1740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Manifold
  id: totrans-1741
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
- en: Manifold Learning pursuits the goal to embed data that originally lies in a
    high dimensional space in a lower dimensional space, while preserving characteristic
    properties. This is possible because for any high dimensional data to be interesting,
    it must be intrinsically low dimensional.
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
- en: For example, images of faces might be represented as points in a high dimensional
    space (let’s say your camera has 5MP -- so your images, considering each pixel
    consists of three values [r,g,b], lie in a 15M dimensional space), but not every
    5MP image is a face. Faces lie on a sub-manifold in this high dimensional space.
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
- en: A sub-manifold is locally Euclidean, i.e. if you take two very similar points,
    for example two images of identical twins they will be close on the euclidian
    space
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
- en: '![](ManifoldSubspace.jpg)'
  id: totrans-1746
  prefs: []
  type: TYPE_IMG
- en: For example on the dataset above we have a high dimension manifold, but the
    faces sit's on a much lower dimension space (almost euclidian). So on this subspace
    things like distance has a meaning.
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
- en: 'With the increase of more features, the data distribution will not be linear,
    so simpler linear techniques (ex: PCA) will not be useful for dimensionality reduction.
    On those cases we need other stuff like T-Sne, Autoencoders, etc..'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
- en: By the way dimensionality reduction on non-linear manifolds is sometimes called
    manifold learning.
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
- en: '![](LinearNonLinear.png)'
  id: totrans-1750
  prefs: []
  type: TYPE_IMG
- en: '![](ScatterPlotsMatlab.png)'
  id: totrans-1751
  prefs: []
  type: TYPE_IMG
- en: 'Bellow we have a diagram that guide you depending on the type of problem:'
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
- en: '![](DimensionalityReduction.png)'
  id: totrans-1753
  prefs: []
  type: TYPE_IMG
- en: Here is a comparison between the T-SNE method against PCA on MNIST dataset
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_TSNE.png)'
  id: totrans-1755
  prefs: []
  type: TYPE_IMG
- en: Principal Component Analysis
  id: totrans-1756
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis
  id: totrans-1757
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1758
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we're going to learn about Principal Component Analysis (PCA)
    which is a tool used to make dimensionality reduction. This is usefull because
    it make the job of classifiers easier in terms of speed, or to aid data visualization.
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
- en: So what are principal components then? They're the underlying structure in the
    data. They are the directions where there is the most variance on your data, the
    directions where the data is most spread out.
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
- en: The only limitation if this algorithm is that it works better only when we have
    a linear manifold.
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
- en: The PCA algorithm will try to fit a plane that minimize a projection error (sum
    of all red-line sizes)
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the PCA will try to rotate your data looking for a angle where
    it see more variances.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCAAnimation.gif)'
  id: totrans-1765
  prefs: []
  type: TYPE_IMG
- en: As mentioned before you can use PCA when your data has a linear data manifold.
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_3d.png)'
  id: totrans-1767
  prefs: []
  type: TYPE_IMG
- en: But for non linear manifolds we're going to have a lot of projection errors.
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
- en: '![](SwissRoll_DataManifold.png) ![](PCA_SwissRoll.png)'
  id: totrans-1769
  prefs: []
  type: TYPE_IMG
- en: Calculating PCA
  id: totrans-1770
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocess the data: ![](106312a2.png)'
  id: totrans-1772
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the covariance matrix: ![](655dd0e4.png), ![](dfaeb377.png) is the
    number of elements, X is a matrix ![](ca7f258c.png) where n is experiment number
    and p the features'
  id: totrans-1773
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the eigenvectors of the covariance matrix ![](27af2486.png), here the U
    matrix will be a nxn matrix where every column of U will be the principal components,
    if we want to reduce our data from n dimensions to k, we choose k columns from
    U.
  id: totrans-1774
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preprocessing part sometimes includes a division by the standard deviation
    of each collumn, but there are cases that this is not needed. (The mean subtraction
    is more important)
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
- en: Reducing input data
  id: totrans-1776
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
- en: Now that we calculate our principal components, which are stored on the matrix
    U, we will reduce our input data ![](aecd381d.png) from n dimensions to k dimensions
    ![](a86cb084.png). Here k is the number of columns of U. Depending on how you
    organized the data we can have 2 different formats for Z
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
- en: '![](49ca84d0.png)'
  id: totrans-1779
  prefs: []
  type: TYPE_IMG
- en: Get the data back
  id: totrans-1780
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
- en: 'To reverse the transformation we do the following: ![](a800fa4.png)'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
- en: Example in Matlab
  id: totrans-1783
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the whole process we're going to calculate the PCA from an image,
    and then restore it with less dimensions.
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
- en: Get some data example
  id: totrans-1786
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
- en: Here our data is a matrix with 15 samples of 3 measurements [15x3]
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
- en: '![](inputData_PCA.png)'
  id: totrans-1789
  prefs: []
  type: TYPE_IMG
- en: Data pre-processing
  id: totrans-1790
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to subtract the mean of each experiment from every column, then
    divide also each element by the standard deviation of each column.
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
- en: '![](Prep_InputPCA.png)'
  id: totrans-1793
  prefs: []
  type: TYPE_IMG
- en: mean and std will work on all columns of X
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the covariance matrix
  id: totrans-1795
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
- en: '![](cov_matrix_pca.png)'
  id: totrans-1797
  prefs: []
  type: TYPE_IMG
- en: Get the principal components
  id: totrans-1798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
- en: Now we use "svd" to get the principal components, which are the eigen-vectors
    and eigen-values of the covariance matrix
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
- en: '![](Pca_svd.png)'
  id: totrans-1801
  prefs: []
  type: TYPE_IMG
- en: There are different ways to calculate the PCA, for instance matlab gives already
    a function pca or princomp, which could give different signs on the eigenvectors
    (U) but they all represent the same components.
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
- en: '![](other_pca.png)'
  id: totrans-1803
  prefs: []
  type: TYPE_IMG
- en: The one thing that you should pay attention is the order of the input matrix,
    because some methods to find the PCA, expect that your samples and measurements,
    are in some pre-defined order.
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
- en: Recover original data
  id: totrans-1805
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
- en: Now to recover the original data we use all the components, and also reverse
    the preprocessing.
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
- en: '![](X_Recover_PCA.png)'
  id: totrans-1808
  prefs: []
  type: TYPE_IMG
- en: Reducing our data
  id: totrans-1809
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
- en: Actually normally we do something before we Now that we have our principal components
    let's apply for instance k=2
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_Reduce.png)'
  id: totrans-1812
  prefs: []
  type: TYPE_IMG
- en: We can use the principal components Z to recreate the data X, but with some
    loss. The idea is that the data in Z is smaller than X, but with similar variance.
    On this case we have ![](21509b2a.png) awe could reproduce the data X_loss with
    ![](d2d11d37.png), so one dimension less.
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
- en: '![](X_loss_PCA.png)'
  id: totrans-1814
  prefs: []
  type: TYPE_IMG
- en: Using PCA on images
  id: totrans-1815
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
- en: Before finish the chapter we're going to use PCA on images.
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
- en: '![](PCA_Image_Compression_Code.png)'
  id: totrans-1818
  prefs: []
  type: TYPE_IMG
- en: '![](PCA_image_compression.png)'
  id: totrans-1819
  prefs: []
  type: TYPE_IMG
- en: Generative Models
  id: totrans-1820
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Models
  id: totrans-1821
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of generative models, is to be able to learn the probability distribution
    of the training set. By doing this the generative model can create more data from
    the original data. Imagine as been the perfect dataset augmentation system. So
    basically it can be used as a unsupervised way to generate samples to train other
    networks better.
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
- en: Basically this done by having 2 neural networks playing against each other.
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Learning
  id: totrans-1824
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1825
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
- en: Learn how the training of deep models can be distributed across multiple machines.
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
- en: Map Reduce
  id: totrans-1828
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
- en: 'Map Reduce can be described on the following steps:'
  id: totrans-1830
  prefs: []
  type: TYPE_NORMAL
- en: 'Split your training set, in batches (ex: divide by the number of workers on
    your farm: 4)'
  id: totrans-1831
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give each machine of your farm 1/4th of the data
  id: totrans-1832
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform Forward/Backward propagation, on each computer node (All nodes share
    the same model)
  id: totrans-1833
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine results of each machine and perform gradient descent
  id: totrans-1834
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update model version on all nodes.
  id: totrans-1835
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](MapReduceSimple.png)'
  id: totrans-1836
  prefs: []
  type: TYPE_IMG
- en: Example Linear Regression model
  id: totrans-1837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the batch gradient descent formula, which is the gradient descent
    applied on all training set:'
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
- en: '![](bd56965d.png)'
  id: totrans-1840
  prefs: []
  type: TYPE_IMG
- en: 'Each machine will deal with 100 elements (After splitting the dataset), calculating
    ![](6a936391.png), then:'
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
- en: '![](3bc3ebcb.png)'
  id: totrans-1842
  prefs: []
  type: TYPE_IMG
- en: Each machine is calculating the back-propagation and error for it's own split
    of data. Remember that all machines have the same copy of the model.
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
- en: After each machine calculated their respective ![](706202b.png). Another machine
    will combine those gradients, calculate the new weights and update the model in
    all machines.
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
- en: '![](215027ae.png)'
  id: totrans-1845
  prefs: []
  type: TYPE_IMG
- en: The whole point of this procedure is to check if we can combine the calculations
    of all nodes and still make sense, in terms of the final calculation.
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
- en: Who use this approach
  id: totrans-1847
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
- en: Caffe
  id: totrans-1849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch (Parallel layer)
  id: totrans-1850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems
  id: totrans-1851
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This approach has some problems:'
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
- en: The complete model must fit on every machine
  id: totrans-1853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is to big it will take time to update all machines with the same
    model
  id: totrans-1854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split weights
  id: totrans-1855
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
- en: Another approach whas used on google DistBelief project where they use a normal
    neural network model with weights separated between multiple machines.
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
- en: '![](DistNeuralNetwork.png)'
  id: totrans-1858
  prefs: []
  type: TYPE_IMG
- en: On this approach only the weights (thick edges) that cross machines need to
    be synchronized between the workers. This technique could only be used on fully
    connected layers.
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
- en: If you mix both techniques (reference on Alexnet) paper, you do this share fully
    connected processing (Just a matrix multiplication), then when you need to the
    the convolution part, each convolution layer get one part of the batch.
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
- en: '![](AlexnetDistribution.png)'
  id: totrans-1861
  prefs: []
  type: TYPE_IMG
- en: Google approach (old)
  id: totrans-1862
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
- en: '![](GoogleDistOldApproach.png)'
  id: totrans-1864
  prefs: []
  type: TYPE_IMG
- en: Here each model replica is trained independently with pieces of data and a parameter
    server that synchronize the parameters between the workers.
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
- en: Google new approach
  id: totrans-1866
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
- en: Now google offer on Tensorflow some automation on choosing which strategy to
    follow depending on your work.
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
- en: '![](TensorflowDistributes.png)'
  id: totrans-1869
  prefs: []
  type: TYPE_IMG
- en: Asynchronous Stochastic Gradient Descent
  id: totrans-1870
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Methodology for usage
  id: totrans-1871
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1872
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will give a [recipe](https://www.youtube.com/watch?v=NKiwFF_zBu4&t=997s)
    on how to tackle Machine learning problems
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
- en: 3 Step process
  id: totrans-1875
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define what you need to do and how to measure(metrics) how well/bad you are
    going.
  id: totrans-1876
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with a very simple model (Few layers)
  id: totrans-1877
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add more complexity when needed.
  id: totrans-1878
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define goals
  id: totrans-1879
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normally by being slightly better than human performance(in terms of accuracy
    or prediction time), is already enough for a good product.
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  id: totrans-1881
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accuracy: % of correct examples'
  id: totrans-1882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coverage: Number of processed examples per unit of time'
  id: totrans-1883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: Number of detections that are correct'
  id: totrans-1884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error: Amount of error'
  id: totrans-1885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with simplest model
  id: totrans-1886
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Look if available for the state-of-the-art method for solving that problem.
    If not available use the following recipe.
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
- en: 'Lots of noise and now much structure(ex: House price from features like number
    of rooms,kitchen size, etc...): Don''t use deep learning'
  id: totrans-1888
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Few noise but lot''s of structure (ex: Images, video, text): Use deep learning'
  id: totrans-1889
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Examples for Non-deep methods:'
  id: totrans-1890
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Logistic Regression
  id: totrans-1891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  id: totrans-1892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosted decision trees (Previous favorite "default" algorithm), used a lot in
    robotics
  id: totrans-1893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of deep
  id: totrans-1894
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Few structure: Use only Fully-Connected layers 2-3 layers (Relu, Dropout, SGD+Momentum).
    Need at least few thousand examples per class.'
  id: totrans-1895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spatial structure: Use CONV layers (Inception/Residual, Relu, Droput, Batch-Norm,
    SGD+Momentum).'
  id: totrans-1896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequencial structure (text, market movement): Use Recurrent networks (LSTM,
    SGD, Gradient clipping).'
  id: totrans-1897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Residual/Inception networks start with the shallowest example possible.
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
- en: Solving High train errors
  id: totrans-1899
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Do the following actions on this order:'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
- en: Inspect for defects on the data. (Need human intervention)
  id: totrans-1901
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check for software bugs on your library. (Use gradient check, it's probably
    a backprop error)
  id: totrans-1902
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune learning rate (Make it smaller)
  id: totrans-1903
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make network deeper. (You should start with a shallow network on the beginning)
  id: totrans-1904
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solving High test errors
  id: totrans-1905
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Do the following actions on this order:'
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
- en: Do more data augmentation (Also try generative models to create more data)
  id: totrans-1907
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add dropout and batch-norm
  id: totrans-1908
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get more data (More data has more influence on Accuracy then anything else)
  id: totrans-1909
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some trends
  id: totrans-1910
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the late 2016 Andrew Ng [lecture](https://www.youtube.com/watch?v=F1ka6a13S9I&t=564s)
    these are the topics that we need to pay attention.
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Scalability
  id: totrans-1912
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a computing system that scale well for more data and more model complexity.
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Team
  id: totrans-1914
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have on your team divided with with AI people and HPC (Cuda, OpenCl, etc...).
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Data first
  id: totrans-1916
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data is more important than your model, always try to get more quality data
    before trying to change your model.
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data Augmentation
  id: totrans-1918
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use normal data augmentation techniques plus Generative models(Unsupervised).
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Make sure that Validation Set and Test set come from same distribution
  id: totrans-1920
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This will avoid having a test or validation set that does not tell the reality.
    Also helps to check if your training is valid.
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Have Human level performance metric
  id: totrans-1922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a team of experts to compare with your current system performance. Also
    it drives decisions between getting more data, or making model more complex.
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Data server
  id: totrans-1924
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Have a unified data-warehouse. All team must have access to data, with SSD quality
    access speed.
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Using Games
  id: totrans-1926
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using games are cool to help augment datasets, but attention because games does
    not have the same variants of the same class as real life. For example GTA does
    not have enough car models compared to real life.
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Ensembles always help
  id: totrans-1928
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training separately different networks and averaging their end results always
    gives some extra 2% accuracy. (Imagenet 2016 best results were simple ensambles)
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
- en: 10\. What to do if you have more than 1000 classes
  id: totrans-1930
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use hierarchical Softmax to increase performance.
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
- en: 11\. How many samples per class do we need to have good result
  id: totrans-1932
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If training from scratch, use the same number of parameters. For example Model
    has 1000 parameters, so use 1000 samples per class.
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
- en: If doing transfer learning is much less (Much is not defined yet, more is better).
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
