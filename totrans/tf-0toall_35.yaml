- en: lab 10.6 mnist nn batchnorm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lab 10.6 Batchnormalization Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a batchnormalization layer?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a layer that normalize the output before the activation layer. [The original
    paper](https://arxiv.org/abs/1502.03167) was proposed by Sergey Ioffe in 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch Normalization Layer looks like this: ![](b0e213f4.PNG)'
  prefs: []
  type: TYPE_NORMAL
- en: Why batchnormalization?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distribution of each layer's input changes because the weights of the previous
    layer change as we update weights by the gradient descent. This is called a covariance
    shift, which makes the network training difficult.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the activation layer is a relu layer and the input of the activation
    layer is shifted to less than zeros, no weights will be activated!
  prefs: []
  type: TYPE_NORMAL
- en: One thing also worth mentioning is that $\gamma$ and $\beta$ parameters in $$
    y = \gamma \hat{x} + \beta $$ are also trainable.
  prefs: []
  type: TYPE_NORMAL
- en: '**What it means is that if we don''t need the batchnormalization, its parameters
    will be updated such that it offsets the normalization step.**'
  prefs: []
  type: TYPE_NORMAL
- en: For example, assume that
  prefs: []
  type: TYPE_NORMAL
- en: \begin{align} \gamma &= \sqrt{\sigma^2_B + \epsilon}\ \beta &= \mu_B \end{align}
  prefs: []
  type: TYPE_NORMAL
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: $$ y_i = \gamma \hat{x_i} + \beta = x_i $$
  prefs: []
  type: TYPE_NORMAL
- en: Also note that $\mu$ and $\sigma$ are computed using moving averages during
    the training step. However, during the test time, the computed $\mu$ and $\sigma$
    will be used as fixed
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Always use the batch normalization!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enough Talk: how to implement in Tensorflow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Load Library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the famous MNIST data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Define Model & Solver Class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object-Oriented-Programming allows to define multiple model easily
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we separate model and solver classes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can just swap out the model class in the Solver class when we need a different
    network architecture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually we need one solver class
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Instantiate Model/Solver classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Run the train step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the batchnormalization, the loss is lower and it's more accurate too!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![png](lab-10-6-mnist_nn_batchnorm_17_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![png](lab-10-6-mnist_nn_batchnorm_18_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![png](lab-10-6-mnist_nn_batchnorm_19_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![png](lab-10-6-mnist_nn_batchnorm_20_0.png)'
  prefs: []
  type: TYPE_IMG
