- en: Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[index](genindex.html "General Index")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[previous](index.html "A Little Book of Python for Multivariate Analysis")
    |'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Little Book of Python for Multivariate Analysis 0.1 documentation](index.html)
    »'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '# A Little Book of Python for Multivariate Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: This booklet tells you how to use the Python ecosystem to carry out some simple
    multivariate analyses, with a focus on principal components analysis (PCA) and
    linear discriminant analysis (LDA).
  prefs: []
  type: TYPE_NORMAL
- en: This booklet assumes that the reader has some basic knowledge of multivariate
    analyses, and the principal focus of the booklet is not to explain multivariate
    analyses, but rather to explain how to carry out these analyses using Python.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to multivariate analysis, and want to learn more about any of
    the concepts presented here, there are a number of good resources, such as for
    example Multivariate Data Analysis by Hair et. al. or Applied Multivariate Data
    Analysis by Everitt and Dunn.
  prefs: []
  type: TYPE_NORMAL
- en: In the examples in this booklet, I will be using data sets from the [UCI Machine
    Learning Repository](http://archive.ics.uci.edu/ml) [http://archive.ics.uci.edu/ml].
  prefs: []
  type: TYPE_NORMAL
- en: '## Setting up the python environment'
  prefs: []
  type: TYPE_NORMAL
- en: '### Install Python'
  prefs: []
  type: TYPE_NORMAL
- en: Although there are a number of ways of getting Python to your system, for a
    hassle free install and quick start using, I highly recommend downloading and
    installing [Anaconda](https://www.continuum.io/downloads) [https://www.continuum.io/downloads]
    by [Continuum](https://www.continuum.io) [https://www.continuum.io], which is
    a Python distribution that contains the core packages plus a large number of packages
    for scientific computing and tools to easily update them, install new ones, create
    virtual environments, and provide IDEs such as this one, the [Jupyter notebook](https://jupyter.org)
    [https://jupyter.org] (formerly known as ipython notebook).
  prefs: []
  type: TYPE_NORMAL
- en: 'This notebook was created with python 2.7 version. For exact details, including
    versions of the other libraries, see the `%watermark` directive below.  ### Libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '[Python](https://en.wikipedia.org/wiki/Python_%28programming_language%29) [https://en.wikipedia.org/wiki/Python_%28programming_language%29]
    can typically do less out of the box than other languages, and this is due to
    being a genaral programming language taking a more modular approach, relying on
    other packages for specialized tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following libraries are used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[pandas](http://pandas.pydata.org) [http://pandas.pydata.org]: The Python Data
    Analysis Library is used for storing the data in dataframes and manipulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[numpy](http://www.numpy.org) [http://www.numpy.org]: Python scientific computing
    library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[matplotlib](http://matplotlib.org) [http://matplotlib.org]: Python plotting
    library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[seaborn](http://stanford.edu/~mwaskom/software/seaborn/) [http://stanford.edu/~mwaskom/software/seaborn/]:
    Statistical data visualization based on matplotlib.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[scikit-learn](http://scikit-learn.org/stable/) [http://scikit-learn.org/stable/]:
    Sklearn is a machine learning library for Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html) [http://docs.scipy.org/doc/scipy/reference/stats.html]:
    Provides a number of probability distributions and statistical functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These should have been installed for you if you have installed the Anaconda
    Python distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The libraries versions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]  ### Importing the libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]  ### Python console'
  prefs: []
  type: TYPE_NORMAL
- en: A useful tool to have aside a notebook for quick experimentation and data visualization
    is a python console attached. Uncomment the following line if you wish to have
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]  ## Reading Multivariate Analysis Data into Python'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that you will want to do to analyse your multivariate data will
    be to read it into Python, and to plot the data. For data analysis an I will be
    using the [Python Data Analysis Library](http://pandas.pydata.org) [http://pandas.pydata.org]
    (pandas, imported as `pd`), which provides a number of useful functions for reading
    and analyzing the data, as well as a `DataFrame` storage structure, similar to
    that found in other popular data analytics languages, such as R.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the file http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data
    contains data on concentrations of 13 different chemicals in wines grown in the
    same region in Italy that are derived from three different cultivars. The data
    set looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: There is one row per wine sample. The first column contains the cultivar of
    a wine sample (labelled 1, 2 or 3), and the following thirteen columns contain
    the concentrations of the 13 different chemicals in that sample. The columns are
    separated by commas, i.e. it is a comma-separated (csv) file without a header
    row.
  prefs: []
  type: TYPE_NORMAL
- en: The data can be read in a pandas dataframe using the `read_csv()` function.
    The argument `header=None` tells the function that there is no header in the beginning
    of the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.640000
    | 1.04 | 3.92 | 1065 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.380000
    | 1.05 | 3.40 | 1050 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.680000
    | 1.03 | 3.17 | 1185 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.800000
    | 0.86 | 3.45 | 1480 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.320000
    | 1.04 | 2.93 | 735 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 | 14.20 | 1.76 | 2.45 | 15.2 | 112 | 3.27 | 3.39 | 0.34 | 1.97 | 6.750000
    | 1.05 | 2.85 | 1450 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 14.39 | 1.87 | 2.45 | 14.6 | 96 | 2.50 | 2.52 | 0.30 | 1.98 | 5.250000
    | 1.02 | 3.58 | 1290 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1 | 14.06 | 2.15 | 2.61 | 17.6 | 121 | 2.60 | 2.51 | 0.31 | 1.25 | 5.050000
    | 1.06 | 3.58 | 1295 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 14.83 | 1.64 | 2.17 | 14.0 | 97 | 2.80 | 2.98 | 0.29 | 1.98 | 5.200000
    | 1.08 | 2.85 | 1045 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1 | 13.86 | 1.35 | 2.27 | 16.0 | 98 | 2.98 | 3.15 | 0.22 | 1.85 | 7.220000
    | 1.01 | 3.55 | 1045 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...
    | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 168 | 3 | 13.58 | 2.58 | 2.69 | 24.5 | 105 | 1.55 | 0.84 | 0.39 | 1.54 |
    8.660000 | 0.74 | 1.80 | 750 |'
  prefs: []
  type: TYPE_TB
- en: '| 169 | 3 | 13.40 | 4.60 | 2.86 | 25.0 | 112 | 1.98 | 0.96 | 0.27 | 1.11 |
    8.500000 | 0.67 | 1.92 | 630 |'
  prefs: []
  type: TYPE_TB
- en: '| 170 | 3 | 12.20 | 3.03 | 2.32 | 19.0 | 96 | 1.25 | 0.49 | 0.40 | 0.73 | 5.500000
    | 0.66 | 1.83 | 510 |'
  prefs: []
  type: TYPE_TB
- en: '| 171 | 3 | 12.77 | 2.39 | 2.28 | 19.5 | 86 | 1.39 | 0.51 | 0.48 | 0.64 | 9.899999
    | 0.57 | 1.63 | 470 |'
  prefs: []
  type: TYPE_TB
- en: '| 172 | 3 | 14.16 | 2.51 | 2.48 | 20.0 | 91 | 1.68 | 0.70 | 0.44 | 1.24 | 9.700000
    | 0.62 | 1.71 | 660 |'
  prefs: []
  type: TYPE_TB
- en: '| 173 | 3 | 13.71 | 5.65 | 2.45 | 20.5 | 95 | 1.68 | 0.61 | 0.52 | 1.06 | 7.700000
    | 0.64 | 1.74 | 740 |'
  prefs: []
  type: TYPE_TB
- en: '| 174 | 3 | 13.40 | 3.91 | 2.48 | 23.0 | 102 | 1.80 | 0.75 | 0.43 | 1.41 |
    7.300000 | 0.70 | 1.56 | 750 |'
  prefs: []
  type: TYPE_TB
- en: '| 175 | 3 | 13.27 | 4.28 | 2.26 | 20.0 | 120 | 1.59 | 0.69 | 0.43 | 1.35 |
    10.200000 | 0.59 | 1.56 | 835 |'
  prefs: []
  type: TYPE_TB
- en: '| 176 | 3 | 13.17 | 2.59 | 2.37 | 20.0 | 120 | 1.65 | 0.68 | 0.53 | 1.46 |
    9.300000 | 0.60 | 1.62 | 840 |'
  prefs: []
  type: TYPE_TB
- en: '| 177 | 3 | 14.13 | 4.10 | 2.74 | 24.5 | 96 | 2.05 | 0.76 | 0.56 | 1.35 | 9.200000
    | 0.61 | 1.60 | 560 |'
  prefs: []
  type: TYPE_TB
- en: 178 rows × 14 columns
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case the data on 178 samples of wine has been read into the variable
    `data`.  ## Plotting Multivariate Data'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have read a multivariate data set into python, the next step is usually
    to make a plot of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '### A Matrix Scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: One common way of plotting multivariate data is to make a *matrix scatterplot*,
    showing each pair of variables plotted against each other. We can use the `scatter_matrix()`
    function from the `pandas.tools.plotting` package to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `scatter_matrix()` function, you need to give it as its input the
    variables that you want included in the plot. Say for example, that we just want
    to include the variables corresponding to the concentrations of the first five
    chemicals. These are stored in columns V2-V6 of the variable `data`. The parameter
    `diagonal` allows us to specify whether to plot a histogram (`"hist"`) or a Kernel
    Density Estimation (`"kde"`) for the variable. We can extract just these columns
    from the variable `data` by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V2 | V3 | V4 | V5 | V6 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 14.23 | 1.71 | 2.43 | 15.6 | 127 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 13.16 | 2.36 | 2.67 | 18.6 | 101 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 14.37 | 1.95 | 2.50 | 16.8 | 113 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 13.24 | 2.59 | 2.87 | 21.0 | 118 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 14.20 | 1.76 | 2.45 | 15.2 | 112 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 14.39 | 1.87 | 2.45 | 14.6 | 96 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 14.06 | 2.15 | 2.61 | 17.6 | 121 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 14.83 | 1.64 | 2.17 | 14.0 | 97 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 13.86 | 1.35 | 2.27 | 16.0 | 98 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 168 | 13.58 | 2.58 | 2.69 | 24.5 | 105 |'
  prefs: []
  type: TYPE_TB
- en: '| 169 | 13.40 | 4.60 | 2.86 | 25.0 | 112 |'
  prefs: []
  type: TYPE_TB
- en: '| 170 | 12.20 | 3.03 | 2.32 | 19.0 | 96 |'
  prefs: []
  type: TYPE_TB
- en: '| 171 | 12.77 | 2.39 | 2.28 | 19.5 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| 172 | 14.16 | 2.51 | 2.48 | 20.0 | 91 |'
  prefs: []
  type: TYPE_TB
- en: '| 173 | 13.71 | 5.65 | 2.45 | 20.5 | 95 |'
  prefs: []
  type: TYPE_TB
- en: '| 174 | 13.40 | 3.91 | 2.48 | 23.0 | 102 |'
  prefs: []
  type: TYPE_TB
- en: '| 175 | 13.27 | 4.28 | 2.26 | 20.0 | 120 |'
  prefs: []
  type: TYPE_TB
- en: '| 176 | 13.17 | 2.59 | 2.37 | 20.0 | 120 |'
  prefs: []
  type: TYPE_TB
- en: '| 177 | 14.13 | 4.10 | 2.74 | 24.5 | 96 |'
  prefs: []
  type: TYPE_TB
- en: 178 rows × 5 columns
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a matrix scatterplot of just these 5 variables using the `scatter_matrix()`
    function we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_17_0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this matrix scatterplot, the diagonal cells show histograms of each of the
    variables, in this case the concentrations of the first five chemicals (variables
    V2, V3, V4, V5, V6).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the off-diagonal cells is a scatterplot of two of the five chemicals,
    for example, the second cell in the first row is a scatterplot of V2 (y-axis)
    against V3 (x-axis).  ### A Scatterplot with the Data Points Labelled by their
    Group'
  prefs: []
  type: TYPE_NORMAL
- en: If you see an interesting scatterplot for two variables in the matrix scatterplot,
    you may want to plot that scatterplot in more detail, with the data points labelled
    by their group (their cultivar in this case).
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the matrix scatterplot above, the cell in the third column of
    the fourth row down is a scatterplot of V5 (x-axis) against V4 (y-axis). If you
    look at this scatterplot, it appears that there may be a positive relationship
    between V5 and V4.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may therefore decide to examine the relationship between `V5` and `V4` more
    closely, by plotting a scatterplot of these two variables, with the data points
    labelled by their group (their cultivar). To plot a scatterplot of two variables,
    we can use the `lmplot` function from the `seaborn` package. The V4 and V5 variables
    are stored in the columns V4 and V5 of the variable `data`. The first two parameters
    in the `lmplot()` function are the columns to be plotted against each other in
    x-y, the third parameter specifies the data, the `hue` parameter is the column
    name used for the labels of the datapoints, i.e. the classes they belong to, lastly,
    the `fit_reg` parameter is set to `False` when we do not want to plot a regression
    model relating to the x-y variables. Therefore, to plot the scatterplot, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_20_0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see from the scatterplot of V4 versus V5 that the wines from cultivar
    2 seem to have lower values of V4 compared to the wines of cultivar 1.  ### A
    Profile Plot'
  prefs: []
  type: TYPE_NORMAL
- en: Another type of plot that is useful is a *profile plot*, which shows the variation
    in each of the variables, by plotting the value of each of the variables for each
    of the samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be achieved using `pandas` plot facilities, which are built upon `matplotlib`,
    by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_23_0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is clear from the profile plot that the mean and standard deviation for
    V6 is quite a lot higher than that for the other variables.  ## Calculating Summary
    Statistics for Multivariate Data'
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that you are likely to want to do is to calculate summary statistics
    such as the mean and standard deviation for each of the variables in your multivariate
    data set.
  prefs: []
  type: TYPE_NORMAL
- en: This is easy to do, using the `mean()` and `std()` functions in `numpy` and
    applying them to the dataframe using its member function `apply`.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas allows to do simple operations directly calling them as methods, for
    example we could do compute the means of a dataframe `df` by calling `df.mean()`.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative option is to use the `apply` method of the `pandas.DataFrame`
    class, which applies the passed argument function along the input axis of the
    DataFrame. This method is powerful as it allows passing any function we want to
    be applied in our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say we want to calculate the mean and standard deviations of each
    of the 13 chemical concentrations in the wine samples. These are stored in columns
    V2-V14 of the variable `data`, which has been previously assigned to `X` for convenience.
    So we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that the mean of variable V2 is 13.000618, the mean of V3 is 2.336348,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, to get the standard deviations of the 13 chemical concentrations,
    we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can see here that it would make sense to standardise in order to compare
    the variables because the variables have very different standard deviations -
    the standard deviation of V14 is 314.021657, while the standard deviation of V9
    is just 0.124103\. Thus, in order to compare the variables, we need to standardise
    each variable so that it has a sample variance of 1 and sample mean of 0\. We
    will explain below how to standardise the variables.
  prefs: []
  type: TYPE_NORMAL
- en: '### Means and Variances Per Group'
  prefs: []
  type: TYPE_NORMAL
- en: It is often interesting to calculate the means and standard deviations for just
    the samples from a particular group, for example, for the wine samples from each
    cultivar. The cultivar is stored in the column V1 of the variable `data`, which
    has been previously assigned to `y` for convenience.
  prefs: []
  type: TYPE_NORMAL
- en: 'To extract out the data for just cultivar 2, we can type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then calculate the mean and standard deviations of the 13 chemicals’
    concentrations, for just the cultivar 2 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You can calculate the mean and standard deviation of the 13 chemicals’ concentrations
    for just cultivar 1 samples, or for just cultivar 3 samples, in a similar way.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for convenience, you might want to use the function `printMeanAndSdByGroup()`
    below, which prints out the mean and standard deviation of the variables for each
    group in your data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments of the function are the variables that you want to calculate
    means and standard deviations for (`X`), and the variable containing the group
    of each sample (`y`). For example, to calculate the mean and standard deviation
    for each of the 13 chemical concentrations, for each of the three different wine
    cultivars, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 13.744746 | 2.010678 | 2.455593 | 17.037288 | 106.338983 | 2.840169 |
    2.982373 | 0.290000 | 1.899322 | 5.528305 | 1.062034 | 3.157797 | 1115.711864
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 12.278732 | 1.932676 | 2.244789 | 20.238028 | 94.549296 | 2.258873 |
    2.080845 | 0.363662 | 1.630282 | 3.086620 | 1.056282 | 2.785352 | 519.507042 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 13.153750 | 3.333750 | 2.437083 | 21.416667 | 99.312500 | 1.678750 |
    0.781458 | 0.447500 | 1.153542 | 7.396250 | 0.682708 | 1.683542 | 629.895833 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.458192 | 0.682689 | 0.225233 | 2.524651 | 10.409595 | 0.336077 | 0.394111
    | 0.069453 | 0.408602 | 1.228032 | 0.115491 | 0.354038 | 219.635449 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.534162 | 1.008391 | 0.313238 | 3.326097 | 16.635097 | 0.541507 | 0.700713
    | 0.123085 | 0.597813 | 0.918393 | 0.201503 | 0.493064 | 156.100173 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.524689 | 1.076514 | 0.182756 | 2.234515 | 10.776433 | 0.353233 | 0.290431
    | 0.122840 | 0.404555 | 2.286743 | 0.113243 | 0.269262 | 113.891805 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|  | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 71 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 48 |'
  prefs: []
  type: TYPE_TB
- en: 'The function `printMeanAndSdByGroup()` also prints out the number of samples
    in each group. In this case, we see that there are 59 samples of cultivar 1, 71
    of cultivar 2, and 48 of cultivar 3.  ### Between-groups Variance and Within-groups
    Variance for a Variable'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to calculate the within-groups variance for a particular variable
    (for example, for a particular chemical’s concentration), we can use the function
    `calcWithinGroupsVariance()` below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `variable` parameter of the function `calcWithinGroupsVariance()` is the
    input variable for which we wish to compute its within-groups variance for the
    groups given in `groupvariable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So for example, to calculate the within-groups variance of the variable V2
    (the concentration of the first chemical), we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the within-groups variance for V2 is 0.2620525.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the between-groups variance for a particular variable (eg.
    V2) using the function `calcBetweenGroupsVariance()` below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Similarly to the parameters of the function `calcWithinGroupsVariance()`, the
    `variable` parameter of the function `calcBetweenGroupsVariance()` is the input
    variable for which we wish to compute its between-groups variance for the groups
    given in `groupvariable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So for example, to calculate the between-groups variance of the variable V2
    (the concentration of the first chemical), we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the between-groups variance of V2 is 35.397425.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the *separation* achieved by a variable as its between-groups
    variance devided by its within-groups variance. Thus, the separation achieved
    by V2 is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to calculate the separations achieved by all of the variables in
    a multivariate data set, you can use the function `calcSeparations()` below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to calculate the separations for each of the 13 chemical concentrations,
    we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the individual variable which gives the greatest separations between
    the groups (the wine cultivars) is V8 (separation 233.9). As we will discuss below,
    the purpose of linear discriminant analysis (LDA) is to find the linear combination
    of the individual variables that will give the greatest separation between the
    groups (cultivars here). This hopefully will give a better separation than the
    best separation achievable by any individual variable (233.9 for V8 here).  ###
    Between-groups Covariance and Within-groups Covariance for Two Variables'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a multivariate data set with several variables describing sampling
    units from different groups, such as the wine samples from different cultivars,
    it is often of interest to calculate the within-groups covariance and between-groups
    variance for pairs of the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done using the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to calculate the within-groups covariance for variables V8 and
    V11, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to calculate the between-groups covariance for variables V8 and
    V11, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, for V8 and V11, the between-groups covariance is -60.41 and the within-groups
    covariance is 0.29\. Since the within-groups covariance is positive (0.29), it
    means V8 and V11 are positively related within groups: for individuals from the
    same group, individuals with a high value of V8 tend to have a high value of V11,
    and vice versa. Since the between-groups covariance is negative (-60.41), V8 and
    V11 are negatively related between groups: groups with a high mean value of V8
    tend to have a low mean value of V11, and vice versa.  ### Calculating Correlations
    for Multivariate Data¶'
  prefs: []
  type: TYPE_NORMAL
- en: It is often of interest to investigate whether any of the variables in a multivariate
    data set are significantly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the linear (Pearson) correlation coefficient for a pair of variables,
    you can use the `pearsonr()` function from `scipy.stats` package. For example,
    to calculate the correlation coefficient for the first two chemicals’ concentrations,
    V2 and V3, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that the correlation coefficient is about 0.094, which is a very
    weak correlation. Furthermore, the *p-value* for the statistical test of whether
    the correlation coefficient is significantly different from zero is 0.21\. This
    is much greater than 0.05 (which we can use here as a cutoff for statistical significance),
    so there is very weak evidence that that the correlation is non-zero.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a lot of variables, you can use the `pandas.DataFrame` method `corr()`
    to calculate a correlation matrix that shows the correlation coefficient for each
    pair of variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V2 | 1.000000 | 0.094397 | 0.211545 | -0.310235 | 0.270798 | 0.289101 | 0.236815
    | -0.155929 | 0.136698 | 0.546364 | -0.071747 | 0.072343 | 0.643720 |'
  prefs: []
  type: TYPE_TB
- en: '| V3 | 0.094397 | 1.000000 | 0.164045 | 0.288500 | -0.054575 | -0.335167 |
    -0.411007 | 0.292977 | -0.220746 | 0.248985 | -0.561296 | -0.368710 | -0.192011
    |'
  prefs: []
  type: TYPE_TB
- en: '| V4 | 0.211545 | 0.164045 | 1.000000 | 0.443367 | 0.286587 | 0.128980 | 0.115077
    | 0.186230 | 0.009652 | 0.258887 | -0.074667 | 0.003911 | 0.223626 |'
  prefs: []
  type: TYPE_TB
- en: '| V5 | -0.310235 | 0.288500 | 0.443367 | 1.000000 | -0.083333 | -0.321113 |
    -0.351370 | 0.361922 | -0.197327 | 0.018732 | -0.273955 | -0.276769 | -0.440597
    |'
  prefs: []
  type: TYPE_TB
- en: '| V6 | 0.270798 | -0.054575 | 0.286587 | -0.083333 | 1.000000 | 0.214401 |
    0.195784 | -0.256294 | 0.236441 | 0.199950 | 0.055398 | 0.066004 | 0.393351 |'
  prefs: []
  type: TYPE_TB
- en: '| V7 | 0.289101 | -0.335167 | 0.128980 | -0.321113 | 0.214401 | 1.000000 |
    0.864564 | -0.449935 | 0.612413 | -0.055136 | 0.433681 | 0.699949 | 0.498115 |'
  prefs: []
  type: TYPE_TB
- en: '| V8 | 0.236815 | -0.411007 | 0.115077 | -0.351370 | 0.195784 | 0.864564 |
    1.000000 | -0.537900 | 0.652692 | -0.172379 | 0.543479 | 0.787194 | 0.494193 |'
  prefs: []
  type: TYPE_TB
- en: '| V9 | -0.155929 | 0.292977 | 0.186230 | 0.361922 | -0.256294 | -0.449935 |
    -0.537900 | 1.000000 | -0.365845 | 0.139057 | -0.262640 | -0.503270 | -0.311385
    |'
  prefs: []
  type: TYPE_TB
- en: '| V10 | 0.136698 | -0.220746 | 0.009652 | -0.197327 | 0.236441 | 0.612413 |
    0.652692 | -0.365845 | 1.000000 | -0.025250 | 0.295544 | 0.519067 | 0.330417 |'
  prefs: []
  type: TYPE_TB
- en: '| V11 | 0.546364 | 0.248985 | 0.258887 | 0.018732 | 0.199950 | -0.055136 |
    -0.172379 | 0.139057 | -0.025250 | 1.000000 | -0.521813 | -0.428815 | 0.316100
    |'
  prefs: []
  type: TYPE_TB
- en: '| V12 | -0.071747 | -0.561296 | -0.074667 | -0.273955 | 0.055398 | 0.433681
    | 0.543479 | -0.262640 | 0.295544 | -0.521813 | 1.000000 | 0.565468 | 0.236183
    |'
  prefs: []
  type: TYPE_TB
- en: '| V13 | 0.072343 | -0.368710 | 0.003911 | -0.276769 | 0.066004 | 0.699949 |
    0.787194 | -0.503270 | 0.519067 | -0.428815 | 0.565468 | 1.000000 | 0.312761 |'
  prefs: []
  type: TYPE_TB
- en: '| V14 | 0.643720 | -0.192011 | 0.223626 | -0.440597 | 0.393351 | 0.498115 |
    0.494193 | -0.311385 | 0.330417 | 0.316100 | 0.236183 | 0.312761 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: A better graphical representation of the correlation matrix is via a correlation
    matrix plot in the form of a *heatmap*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_68_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Or an alternative nice visualization is via a Hinton diagram. The color of the
    boxes determines the sign of the correlation, in this case red for positive and
    blue for negative correlations; while the size of the boxes determines their magnitude,
    the bigger the box the higher the magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_70_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Although the correlation matrix and diagrams are useful for quickly looking
    to identify the strongest correlations, they still require labor work to find
    the top `N` strongest correlations. For this you can use the function `mosthighlycorrelated()`
    below.
  prefs: []
  type: TYPE_NORMAL
- en: The function `mosthighlycorrelated()` will print out the linear correlation
    coefficients for each pair of variables in your data set, in order of the correlation
    coefficient. This lets you see very easily which pair of variables are most highly
    correlated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The arguments of the function are the variables that you want to calculate the
    correlations for, and the number of top correlation coefficients to print out
    (for example, you can tell it to print out the largest 10 correlation coefficients,
    or the largest 20).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to calculate correlation coefficients between the concentrations
    of the 13 chemicals in the wine samples, and to print out the top 10 pairwise
    correlation coefficients, you can type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '|  | FirstVariable | SecondVariable | Correlation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | V7 | V8 | 0.864564 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | V8 | V13 | 0.787194 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | V7 | V13 | 0.699949 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | V8 | V10 | 0.652692 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | V2 | V14 | 0.643720 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | V7 | V10 | 0.612413 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | V12 | V13 | 0.565468 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | V3 | V12 | -0.561296 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | V2 | V11 | 0.546364 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | V8 | V12 | 0.543479 |'
  prefs: []
  type: TYPE_TB
- en: 'This tells us that the pair of variables with the highest linear correlation
    coefficient are V7 and V8 (correlation = 0.86 approximately).  ### Standardising
    Variables'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to compare different variables that have different units, are very
    different variances, it is a good idea to first standardise the variables.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we found above that the concentrations of the 13 chemicals in the
    wine samples show a wide range of standard deviations, from 0.124103 for V9 (variance
    0.015402) to 314.021657 for V14 (variance 98609.60). This is a range of approximately
    6,402,389-fold in the variances.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, it is not a good idea to use the unstandardised chemical concentrations
    as the input for a principal component analysis (PCA, see below) of the wine samples,
    as if you did that, the first principal component would be dominated by the variables
    which show the largest variances, such as V14.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it would be a better idea to first standardise the variables so that they
    all have variance 1 and mean 0, and to then carry out the principal component
    analysis on the standardised data. This would allow us to find the principal components
    that provide the best low-dimensional representation of the variation in the original
    data, without being overly biased by those variables that show the most variance
    in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: You can standardise variables by using the `scale()` function from the package
    `sklearn.preprocessing`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to standardise the concentrations of the 13 chemicals in the wine
    samples, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]  ## Principal Component Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of principal component analysis is to find the best low-dimensional
    representation of the variation in a multivariate data set. For example, in the
    case of the wine data set, we have 13 chemical concentrations describing wine
    samples from three different cultivars. We can carry out a principal component
    analysis to investigate whether we can capture most of the variation between samples
    using a smaller number of new variables (principal components), where each of
    these new variables is a linear combination of all or some of the 13 chemical
    concentrations.
  prefs: []
  type: TYPE_NORMAL
- en: To carry out a principal component analysis (PCA) on a multivariate data set,
    the first step is often to standardise the variables under study using the `scale()`
    function (see above). This is necessary if the input variables have very different
    variances, which is true in this case as the concentrations of the 13 chemicals
    have very different variances (see above).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have standardised your variables, you can carry out a principal component
    analysis using the `PCA` class from `sklearn.decomposition` package and its `fit`
    method, which fits the model with the data `X`. The default `solver` is Singular
    Value Decomposition (“svd”). For more information you can type `help(PCA)` in
    the python console.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to standardise the concentrations of the 13 chemicals in the wine
    samples, and carry out a principal components analysis on the standardised concentrations,
    we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get a summary of the principal component analysis results using the
    `pca_summary()` function below, which simulates the output of R’s `summary` function
    on a PCA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the `print_pca_summary` function are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pca`: A PCA object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`standardised_data`: The standardised data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out (True)`: Print to standard output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '|  | sdev | varprop | cumprop |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Standard deviation | Proportion of Variance | Cumulative Proportion |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PC1 | 2.169297 | 0.361988 | 0.361988 |'
  prefs: []
  type: TYPE_TB
- en: '| PC2 | 1.580182 | 0.192075 | 0.554063 |'
  prefs: []
  type: TYPE_TB
- en: '| PC3 | 1.202527 | 0.111236 | 0.665300 |'
  prefs: []
  type: TYPE_TB
- en: '| PC4 | 0.958631 | 0.070690 | 0.735990 |'
  prefs: []
  type: TYPE_TB
- en: '| PC5 | 0.923704 | 0.065633 | 0.801623 |'
  prefs: []
  type: TYPE_TB
- en: '| PC6 | 0.801035 | 0.049358 | 0.850981 |'
  prefs: []
  type: TYPE_TB
- en: '| PC7 | 0.742313 | 0.042387 | 0.893368 |'
  prefs: []
  type: TYPE_TB
- en: '| PC8 | 0.590337 | 0.026807 | 0.920175 |'
  prefs: []
  type: TYPE_TB
- en: '| PC9 | 0.537476 | 0.022222 | 0.942397 |'
  prefs: []
  type: TYPE_TB
- en: '| PC10 | 0.500902 | 0.019300 | 0.961697 |'
  prefs: []
  type: TYPE_TB
- en: '| PC11 | 0.475172 | 0.017368 | 0.979066 |'
  prefs: []
  type: TYPE_TB
- en: '| PC12 | 0.410817 | 0.012982 | 0.992048 |'
  prefs: []
  type: TYPE_TB
- en: '| PC13 | 0.321524 | 0.007952 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'This gives us the standard deviation of each component, and the proportion
    of variance explained by each component. The standard deviation of the components
    is stored in a named row called `sdev` of the output variable made by the `pca_summary`
    function and stored in the `summary` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Standard deviation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PC1 | 2.169297 |'
  prefs: []
  type: TYPE_TB
- en: '| PC2 | 1.580182 |'
  prefs: []
  type: TYPE_TB
- en: '| PC3 | 1.202527 |'
  prefs: []
  type: TYPE_TB
- en: '| PC4 | 0.958631 |'
  prefs: []
  type: TYPE_TB
- en: '| PC5 | 0.923704 |'
  prefs: []
  type: TYPE_TB
- en: '| PC6 | 0.801035 |'
  prefs: []
  type: TYPE_TB
- en: '| PC7 | 0.742313 |'
  prefs: []
  type: TYPE_TB
- en: '| PC8 | 0.590337 |'
  prefs: []
  type: TYPE_TB
- en: '| PC9 | 0.537476 |'
  prefs: []
  type: TYPE_TB
- en: '| PC10 | 0.500902 |'
  prefs: []
  type: TYPE_TB
- en: '| PC11 | 0.475172 |'
  prefs: []
  type: TYPE_TB
- en: '| PC12 | 0.410817 |'
  prefs: []
  type: TYPE_TB
- en: '| PC13 | 0.321524 |'
  prefs: []
  type: TYPE_TB
- en: 'The total variance explained by the components is the sum of the variances
    of the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we see that the total variance is 13, which is equal to the number
    of standardised variables (13 variables). This is because for standardised data,
    the variance of each standardised variable is 1\. The total variance is equal
    to the sum of the variances of the individual variables, and since the variance
    of each standardised variable is 1, the total variance should be equal to the
    number of variables (13 here).
  prefs: []
  type: TYPE_NORMAL
- en: '### Deciding How Many Principal Components to Retain'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to decide how many principal components should be retained, it is
    common to summarise the results of a principal components analysis by making a
    scree plot, which we can do using the `screeplot()` function below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_92_0.png)'
  prefs: []
  type: TYPE_IMG
- en: The most obvious change in slope in the scree plot occurs at component 4, which
    is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis
    of the scree plot that the first three components should be retained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of deciding how many components to retain is to use *Kaiser’s criterion*:
    that we should only retain principal components for which the variance is above
    1 (when principal component analysis was applied to standardised data). We can
    check this by finding the variance of each of the principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Standard deviation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PC1 | 4.705850 |'
  prefs: []
  type: TYPE_TB
- en: '| PC2 | 2.496974 |'
  prefs: []
  type: TYPE_TB
- en: '| PC3 | 1.446072 |'
  prefs: []
  type: TYPE_TB
- en: '| PC4 | 0.918974 |'
  prefs: []
  type: TYPE_TB
- en: '| PC5 | 0.853228 |'
  prefs: []
  type: TYPE_TB
- en: '| PC6 | 0.641657 |'
  prefs: []
  type: TYPE_TB
- en: '| PC7 | 0.551028 |'
  prefs: []
  type: TYPE_TB
- en: '| PC8 | 0.348497 |'
  prefs: []
  type: TYPE_TB
- en: '| PC9 | 0.288880 |'
  prefs: []
  type: TYPE_TB
- en: '| PC10 | 0.250902 |'
  prefs: []
  type: TYPE_TB
- en: '| PC11 | 0.225789 |'
  prefs: []
  type: TYPE_TB
- en: '| PC12 | 0.168770 |'
  prefs: []
  type: TYPE_TB
- en: '| PC13 | 0.103378 |'
  prefs: []
  type: TYPE_TB
- en: We see that the variance is above 1 for principal components 1, 2, and 3 (which
    have variances 4.71, 2.50, and 1.45, respectively). Therefore, using Kaiser’s
    criterion, we would retain the first three principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'A third way to decide how many principal components to retain is to decide
    to keep the number of components required to explain at least some minimum amount
    of the total variance. For example, if it is important to explain at least 80%
    of the variance, we would retain the first five principal components, as we can
    see from cumulative proportions (`summary.cumprop`) that the first five principal
    components explain 80.2% of the variance (while the first four components explain
    just 73.6%, so are not sufficient).  ### Loadings for the Principal Components'
  prefs: []
  type: TYPE_NORMAL
- en: The loadings for the principal components are stored in a named element `components_`
    of the variable returned by `PCA().fit()`. This contains a matrix with the loadings
    of each principal component, where the first column in the matrix contains the
    loadings for the first principal component, the second column contains the loadings
    for the second principal component, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to obtain the loadings for the first principal component in our
    analysis of the 13 chemical concentrations in wine samples, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the first principal component is a linear combination of the
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: where Z2, Z3, Z4, ..., Z14 are the standardised versions of the variables V2,
    V3, V4, ..., V14 (that each have mean of 0 and variance of 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the square of the loadings sum to 1, as this is a constraint used
    in calculating the loadings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the values of the first principal component, we can define our
    own function to calculate a principal component given the loadings and the input
    variables’ values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the function to calculate the values of the first principal
    component for each sample in our wine data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, the values of the first principal component are computed with the
    following, so we can compare those values to the ones that we calculated, and
    they should agree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We see that they do agree.
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component has highest (in absolute value) loadings for V8
    (-0.423), V7 (-0.395), V13 (-0.376), V10 (-0.313), V12 (-0.297), V14 (-0.287),
    V9 (0.299), V3 (0.245), and V5 (0.239). The loadings for V8, V7, V13, V10, V12
    and V14 are negative, while those for V9, V3, and V5 are positive. Therefore,
    an interpretation of the first principal component is that it represents a contrast
    between the concentrations of V8, V7, V13, V10, V12, and V14, and the concentrations
    of V9, V3 and V5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can obtain the loadings for the second principal component by
    typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the second principal component is a linear combination of the
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: where Z1, Z2, Z3, ..., Z14 are the standardised versions of variables V2, V3,
    ..., V14 that each have mean 0 and variance 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the square of the loadings sum to 1, as above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The second principal component has highest loadings for V11 (0.530), V2 (0.484),
    V14 (0.365), V4 (0.316), V6 (0.300), V12 (-0.279), and V3 (0.225). The loadings
    for V11, V2, V14, V4, V6 and V3 are positive, while the loading for V12 is negative.
    Therefore, an interpretation of the second principal component is that it represents
    a contrast between the concentrations of V11, V2, V14, V4, V6 and V3, and the
    concentration of V12\. Note that the loadings for V11 (0.530) and V2 (0.484) are
    the largest, so the contrast is mainly between the concentrations of V11 and V2,
    and the concentration of V12.  ### Scatterplots of the Principal Components'
  prefs: []
  type: TYPE_NORMAL
- en: The values of the principal components can be computed by the `transform()`
    (or `fit_transform()`) method of the `PCA` class. It returns a matrix with the
    principal components, where the first column in the matrix contains the first
    principal component, the second column the second component, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in our example, `pca.transform(standardisedX)[:, 0]` contains the first
    principal component, and `pca.transform(standardisedX)[:, 1]` contains the second
    principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make a scatterplot of the first two principal components, and label
    the data points with the cultivar that the wine samples come from, by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_112_0.png)'
  prefs: []
  type: TYPE_IMG
- en: The scatterplot shows the first principal component on the x-axis, and the second
    principal component on the y-axis. We can see from the scatterplot that wine samples
    of cultivar 1 have much lower values of the first principal component than wine
    samples of cultivar 3\. Therefore, the first principal component separates wine
    samples of cultivars 1 from those of cultivar 3.
  prefs: []
  type: TYPE_NORMAL
- en: We can also see that wine samples of cultivar 2 have much higher values of the
    second principal component than wine samples of cultivars 1 and 3\. Therefore,
    the second principal component separates samples of cultivar 2 from samples of
    cultivars 1 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the first two principal components are reasonably useful for distinguishing
    wine samples of the three different cultivars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Above, we interpreted the first principal component as a contrast between the
    concentrations of V8, V7, V13, V10, V12, and V14, and the concentrations of V9,
    V3 and V5\. We can check whether this makes sense in terms of the concentrations
    of these chemicals in the different cultivars, by printing out the means of the
    standardised concentration variables in each cultivar, using the `printMeanAndSdByGroup()`
    function (see above):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.919195 | -0.292342 | 0.325604 | -0.737997 | 0.463226 | 0.873362 | 0.956884
    | -0.578985 | 0.540383 | 0.203401 | 0.458847 | 0.771351 | 1.174501 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -0.891720 | -0.362362 | -0.444958 | 0.223137 | -0.364567 | -0.058067
    | 0.051780 | 0.014569 | 0.069002 | -0.852799 | 0.433611 | 0.245294 | -0.724110
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.189159 | 0.895331 | 0.257945 | 0.577065 | -0.030127 | -0.987617 | -1.252761
    | 0.690119 | -0.766287 | 1.011418 | -1.205382 | -1.310950 | -0.372578 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '|  | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | V10 | V11 | V12 | V13 | V14 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.565989 | 0.612825 | 0.823302 | 0.758115 | 0.730892 | 0.538506 | 0.395674
    | 0.559639 | 0.715905 | 0.531210 | 0.506699 | 0.500058 | 0.699428 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.659832 | 0.905196 | 1.144991 | 0.998777 | 1.168006 | 0.867674 | 0.703493
    | 0.991797 | 1.047418 | 0.397269 | 0.884060 | 0.696425 | 0.497100 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.648130 | 0.966347 | 0.668036 | 0.670991 | 0.756649 | 0.565996 | 0.291583
    | 0.989818 | 0.708814 | 0.989176 | 0.496834 | 0.380317 | 0.362688 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '|  | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 71 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 48 |'
  prefs: []
  type: TYPE_TB
- en: Does it make sense that the first principal component can separate cultivar
    1 from cultivar 3? In cultivar 1, the mean values of V8 (0.954), V7 (0.871), V13
    (0.769), V10 (0.539), V12 (0.458) and V14 (1.171) are very high compared to the
    mean values of V9 (-0.577), V3 (-0.292) and V5 (-0.736). In cultivar 3, the mean
    values of V8 (-1.249), V7 (-0.985), V13 (-1.307), V10 (-0.764), V12 (-1.202) and
    V14 (-0.372) are very low compared to the mean values of V9 (0.688), V3 (0.893)
    and V5 (0.575). Therefore, it does make sense that principal component 1 is a
    contrast between the concentrations of V8, V7, V13, V10, V12, and V14, and the
    concentrations of V9, V3 and V5; and that principal component 1 can separate cultivar
    1 from cultivar 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Above, we intepreted the second principal component as a contrast between the
    concentrations of V11, V2, V14, V4, V6 and V3, and the concentration of V12\.
    In the light of the mean values of these variables in the different cultivars,
    does it make sense that the second principal component can separate cultivar 2
    from cultivars 1 and 3? In cultivar 1, the mean values of V11 (0.203), V2 (0.917),
    V14 (1.171), V4 (0.325), V6 (0.462) and V3 (-0.292) are not very different from
    the mean value of V12 (0.458). In cultivar 3, the mean values of V11 (1.009),
    V2 (0.189), V14 (-0.372), V4 (0.257), V6 (-0.030) and V3 (0.893) are also not
    very different from the mean value of V12 (-1.202). In contrast, in cultivar 2,
    the mean values of V11 (-0.850), V2 (-0.889), V14 (-0.722), V4 (-0.444), V6 (-0.364)
    and V3 (-0.361) are much less than the mean value of V12 (0.432). Therefore, it
    makes sense that principal component is a contrast between the concentrations
    of V11, V2, V14, V4, V6 and V3, and the concentration of V12; and that principal
    component 2 can separate cultivar 2 from cultivars 1 and 3.  ## Linear Discriminant
    Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of principal component analysis is to find the best low-dimensional
    representation of the variation in a multivariate data set. For example, in the
    wine data set, we have 13 chemical concentrations describing wine samples from
    three cultivars. By carrying out a principal component analysis, we found that
    most of the variation in the chemical concentrations between the samples can be
    captured using the first two principal components, where each of the principal
    components is a particular linear combination of the 13 chemical concentrations.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of linear discriminant analysis (LDA) is to find the linear combinations
    of the original variables (the 13 chemical concentrations here) that gives the
    best possible separation between the groups (wine cultivars here) in our data
    set. *Linear discriminant analysis* is also known as *canonical discriminant analysis*,
    or simply *discriminant analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to separate the wines by cultivar, the wines come from three different
    cultivars, so the number of groups (G) is 3, and the number of variables is 13
    (13 chemicals’ concentrations; p = 13). The maximum number of useful discriminant
    functions that can separate the wines by cultivar is the minimum of G-1 and p,
    and so in this case it is the minimum of 2 and 13, which is 2\. Thus, we can find
    at most 2 useful discriminant functions to separate the wines by cultivar, using
    the 13 chemical concentration variables.
  prefs: []
  type: TYPE_NORMAL
- en: You can carry out a linear discriminant analysis by using the `LinearDiscriminantAnalysis`
    class model from the module `sklearn.discriminant_analysis` and using its method
    `fit()` to fit our `X, y` data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to carry out a linear discriminant analysis using the 13 chemical
    concentrations in the wine samples, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '### Loadings for the Discriminant Functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'The values of the loadings of the discriminant functions for the wine data
    are stored in the `scalings_` member of the `lda` object model. For a pretty print
    we can type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '|  | LD1 | LD2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V2 | -0.403400 | 0.871793 |'
  prefs: []
  type: TYPE_TB
- en: '| V3 | 0.165255 | 0.305380 |'
  prefs: []
  type: TYPE_TB
- en: '| V4 | -0.369075 | 2.345850 |'
  prefs: []
  type: TYPE_TB
- en: '| V5 | 0.154798 | -0.146381 |'
  prefs: []
  type: TYPE_TB
- en: '| V6 | -0.002163 | -0.000463 |'
  prefs: []
  type: TYPE_TB
- en: '| V7 | 0.618052 | -0.032213 |'
  prefs: []
  type: TYPE_TB
- en: '| V8 | -1.661191 | -0.491998 |'
  prefs: []
  type: TYPE_TB
- en: '| V9 | -1.495818 | -1.630954 |'
  prefs: []
  type: TYPE_TB
- en: '| V10 | 0.134093 | -0.307088 |'
  prefs: []
  type: TYPE_TB
- en: '| V11 | 0.355056 | 0.253231 |'
  prefs: []
  type: TYPE_TB
- en: '| V12 | -0.818036 | -1.515634 |'
  prefs: []
  type: TYPE_TB
- en: '| V13 | -1.157559 | 0.051184 |'
  prefs: []
  type: TYPE_TB
- en: '| V14 | -0.002691 | 0.002853 |'
  prefs: []
  type: TYPE_TB
- en: 'This means that the first discriminant function is a linear combination of
    the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: where V2, V3, ..., V14 are the concentrations of the 14 chemicals found in the
    wine samples. For convenience, the value for each discriminant function (eg. the
    first discriminant function) are scaled so that their mean value is zero (see
    below).
  prefs: []
  type: TYPE_NORMAL
- en: Note that these loadings are calculated so that the within-group variance of
    each discriminant function for each group (cultivar) is equal to 1, as will be
    demonstrated below.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned above, these scalings are stored in the named member `scalings_`
    of the object variable returned by `LinearDiscriminantAnalysis().fit(X, y)`. This
    element contains a numpy array, in which the first column contains the loadings
    for the first discriminant function, the second column contains the loadings for
    the second discriminant function and so on. For example, to extract the loadings
    for the first discriminant function, we can type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Or for “prettier” print, use the dataframe variable created above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the values of the first discriminant function, we can define our
    own function `calclda()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `calclda()` simply calculates the value of a discriminant function
    for each sample in the data set, for example, for the first disriminant function,
    for each sample we calculate the value using the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, the `scale()` command is used within the `calclda()` function in
    order to standardise the value of a discriminant function (eg. the first discriminant
    function) so that its mean value (over all the wine samples) is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the function `calclda()` to calculate the values of the first discriminant
    function for each sample in our wine data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, the values of the first linear discriminant function can be calculated
    using the `transform(X)` or `fit_transform(X, y)` methods of the LDA object, so
    we can compare those to the ones that we calculated, and they should agree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We see that they do agree.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t matter whether the input variables for linear discriminant analysis
    are standardised or not, unlike for principal components analysis in which it
    is often necessary to standardise the input variables. However, using standardised
    variables in linear discriminant analysis makes it easier to interpret the loadings
    in a linear discriminant function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In linear discriminant analysis, the standardised version of an input variable
    is defined so that it has mean zero and within-groups variance of 1\. Thus, we
    can calculate the “group-standardised” variable by subtracting the mean from each
    value of the variable, and dividing by the within-groups standard deviation. To
    calculate the group-standardised version of a set of variables, we can use the
    function `groupStandardise()` below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, we can use the `groupStandardise()` function to calculate the
    group-standardised versions of the chemical concentrations in wine samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the `LinearDiscriminantAnalysis().fit()` method to perform
    linear disriminant analysis on the group-standardised variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '|  | LD1 | LD2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V2 | -0.206505 | 0.446280 |'
  prefs: []
  type: TYPE_TB
- en: '| V3 | 0.155686 | 0.287697 |'
  prefs: []
  type: TYPE_TB
- en: '| V4 | -0.094869 | 0.602989 |'
  prefs: []
  type: TYPE_TB
- en: '| V5 | 0.438021 | -0.414204 |'
  prefs: []
  type: TYPE_TB
- en: '| V6 | -0.029079 | -0.006220 |'
  prefs: []
  type: TYPE_TB
- en: '| V7 | 0.270302 | -0.014088 |'
  prefs: []
  type: TYPE_TB
- en: '| V8 | -0.870673 | -0.257869 |'
  prefs: []
  type: TYPE_TB
- en: '| V9 | -0.163255 | -0.178004 |'
  prefs: []
  type: TYPE_TB
- en: '| V10 | 0.066531 | -0.152364 |'
  prefs: []
  type: TYPE_TB
- en: '| V11 | 0.536701 | 0.382783 |'
  prefs: []
  type: TYPE_TB
- en: '| V12 | -0.128011 | -0.237175 |'
  prefs: []
  type: TYPE_TB
- en: '| V13 | -0.464149 | 0.020523 |'
  prefs: []
  type: TYPE_TB
- en: '| V14 | -0.463854 | 0.491738 |'
  prefs: []
  type: TYPE_TB
- en: It makes sense to interpret the loadings calculated using the group-standardised
    variables rather than the loadings for the original (unstandardised) variables.
  prefs: []
  type: TYPE_NORMAL
- en: In the first discriminant function calculated for the group-standardised variables,
    the largest loadings (in absolute) value are given to V8 (-0.871), V11 (0.537),
    V13 (-0.464), V14 (-0.464), and V5 (0.438). The loadings for V8, V13 and V14 are
    negative, while those for V11 and V5 are positive. Therefore, the discriminant
    function seems to represent a contrast between the concentrations of V8, V13 and
    V14, and the concentrations of V11 and V5.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw above that the individual variables which gave the greatest separations
    between the groups were V8 (separation 233.93), V14 (207.92), V13 (189.97), V2
    (135.08) and V11 (120.66). These were mostly the same variables that had the largest
    loadings in the linear discriminant function (loading for V8: -0.871, for V14:
    -0.464, for V13: -0.464, for V11: 0.537).'
  prefs: []
  type: TYPE_NORMAL
- en: We found above that variables V8 and V11 have a negative between-groups covariance
    (-60.41) and a positive within-groups covariance (0.29). When the between-groups
    covariance and within-groups covariance for two variables have opposite signs,
    it indicates that a better separation between groups can be obtained by using
    a linear combination of those two variables than by using either variable on its
    own.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, given that the two variables V8 and V11 have between-groups and within-groups
    covariances of opposite signs, and that these are two of the variables that gave
    the greatest separations between groups when used individually, it is not surprising
    that these are the two variables that have the largest loadings in the first discriminant
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that although the loadings for the group-standardised variables are easier
    to interpret than the loadings for the unstandardised variables, the values of
    the discriminant function are the same regardless of whether we standardise the
    input variables or not. For example, for wine data, we can calculate the value
    of the first discriminant function calculated using the unstandardised and group-standardised
    variables by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that although the loadings are different for the first discriminant
    functions calculated using unstandardised and group-standardised data, the actual
    values of the first discriminant function are the same.  ### Separation Achieved
    by the Discriminant Functions'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the separation achieved by each discriminant function, we first
    need to calculate the value of each discriminant function, by substituting the
    values of the variables into the linear combination for the discriminant function
    (eg. `-0.403*V2 - 0.165*V3 - 0.369*V4 + 0.155*V5 - 0.002*V6 + 0.618*V7 - 1.661*V8
    - 1.496*V9 + 0.134*V10 + 0.355*V11 - 0.818*V12 - 1.158*V13 - 0.003*V14` for the
    first discriminant function), and then scaling the values of the discriminant
    function so that their mean is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned above, we can do this using the `rpredict()` function which simulates
    the output of the `predict()` function in R. For example, to calculate the value
    of the discriminant functions for the wine data, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned variable has a named element `x` which is a matrix containing
    the linear discriminant functions: the first column of `x` contains the first
    discriminant function, the second column of `x` contains the second discriminant
    function, and so on (if there are more discriminant functions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can therefore calculate the separations achieved by the two linear discriminant
    functions for the wine data by using the `calcSeparations()` function (see above),
    which calculates the separation as the ratio of the between-groups variance to
    the within-groups variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned above, the loadings for each discriminant function are calculated
    in such a way that the within-group variance (`Vw`) for each group (wine cultivar
    here) is equal to 1, as we see in the output from `calcSeparations()` above.
  prefs: []
  type: TYPE_NORMAL
- en: The output from `calcSeparations()` tells us that the separation achieved by
    the first (best) discriminant function is 794.7, and the separation achieved by
    the second (second best) discriminant function is 361.2.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the total separation is the sum of these, which is (`794.652200566216+361.241041493455=1155.893`)
    1155.89, rounded to two decimal places. Therefore, the *percentage separation*
    achieved by the first discriminant function is (`794.652200566216*100/1155.893=`)
    68.75%, and the percentage separation achieved by the second discriminant function
    is (`361.241041493455*100/1155.893=`) 31.25%.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *proportion of trace* (as reported in R by the `lda()` model) is the percentage
    separation achieved by each discriminant function. For example, for the wine data
    we get the same values as just calculated (68.75% and 31.25%). Note that in `sklearn`
    the proportion of trace is reported as `explained_variance_ratio_` in a `LinearDiscriminantAnalysis`
    model and is computed only for an “eigen” solver, while so far we have been using
    the default one, which is “svd” (Singular Value Decomposition):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the first discriminant function does achieve a good separation between
    the three groups (three cultivars), but the second discriminant function does
    improve the separation of the groups by quite a large amount, so is it worth using
    the second discriminant function as well. Therefore, to achieve a good separation
    of the groups (cultivars), it is necessary to use both of the first two discriminant
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We found above that the largest separation achieved for any of the individual
    variables (individual chemical concentrations) was 233.9 for V8, which is quite
    a lot less than 794.7, the separation achieved by the first discriminant function.
    Therefore, the effect of using more than one variable to calculate the discriminant
    function is that we can find a discriminant function that achieves a far greater
    separation between groups than achieved by any one variable alone.  ### A Stacked
    Histogram of the LDA Values'
  prefs: []
  type: TYPE_NORMAL
- en: A nice way of displaying the results of a linear discriminant analysis (LDA)
    is to make a stacked histogram of the values of the discriminant function for
    the samples from different groups (different wine cultivars in our example).
  prefs: []
  type: TYPE_NORMAL
- en: We can do this using the `ldahist()` function defined below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to make a stacked histogram of the first discriminant function’s
    values for wine samples of the three different wine cultivars, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_150_0.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the histogram that cultivars 1 and 3 are well separated by the
    first discriminant function, since the values for the first cultivar are between
    -6 and -1, while the values for cultivar 3 are between 2 and 6, and so there is
    no overlap in values.
  prefs: []
  type: TYPE_NORMAL
- en: However, the separation achieved by the linear discriminant function on the
    training set may be an overestimate. To get a more accurate idea of how well the
    first discriminant function separates the groups, we would need to see a stacked
    histogram of the values for the three cultivars using some unseen “test set”,
    that is, using a set of data that was not used to calculate the linear discriminant
    function.
  prefs: []
  type: TYPE_NORMAL
- en: We see that the first discriminant function separates cultivars 1 and 3 very
    well, but does not separate cultivars 1 and 2, or cultivars 2 and 3, so well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We therefore investigate whether the second discriminant function separates
    those cultivars, by making a stacked histogram of the second discriminant function’s
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_152_0.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the second discriminant function separates cultivars 1 and 2 quite
    well, although there is a little overlap in their values. Furthermore, the second
    discriminant function also separates cultivars 2 and 3 quite well, although again
    there is a little overlap in their values so it is not perfect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we see that two discriminant functions are necessary to separate the
    cultivars, as was discussed above (see the discussion of percentage separation
    above).  ### Scatterplots of the Discriminant Functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain a scatterplot of the best two discriminant functions, with the
    data points labelled by cultivar, by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_155_0.png)'
  prefs: []
  type: TYPE_IMG
- en: From the scatterplot of the first two discriminant functions, we can see that
    the wines from the three cultivars are well separated in the scatterplot. The
    first discriminant function (x-axis) separates cultivars 1 and 3 very well, but
    doesn’t not perfectly separate cultivars 1 and 3, or cultivars 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: The second discriminant function (y-axis) achieves a fairly good separation
    of cultivars 1 and 3, and cultivars 2 and 3, although it is not totally perfect.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve a very good separation of the three cultivars, it would be best
    to use both the first and second discriminant functions together, since the first
    discriminant function can separate cultivars 1 and 3 very well, and the second
    discriminant function can separate cultivars 1 and 2, and cultivars 2 and 3, reasonably
    well.  ### Allocation Rules and Misclassification Rate'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the mean values of the discriminant functions for each of
    the three cultivars using the `printMeanAndSdByGroup()` function (see above):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '|  | LD1 | LD2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -3.422489 | 1.691674 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -0.079726 | -2.472656 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4.324737 | 1.578120 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '|  | LD1 | LD2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.931467 | 1.008978 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.076271 | 0.990268 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.930571 | 0.971586 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '|  | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 71 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 48 |'
  prefs: []
  type: TYPE_TB
- en: We find that the mean value of the first discriminant function is -3.42248851
    for cultivar 1, -0.07972623 for cultivar 2, and 4.32473717 for cultivar 3\. The
    mid-way point between the mean values for cultivars 1 and 2 is (-3.42248851-0.07972623)/2=-1.751107,
    and the mid-way point between the mean values for cultivars 2 and 3 is (-0.07972623+4.32473717)/2
    = 2.122505.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can use the following allocation rule:'
  prefs: []
  type: TYPE_NORMAL
- en: if the first discriminant function is <= -1.751107, predict the sample to be
    from cultivar 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if the first discriminant function is > -1.751107 and <= 2.122505, predict the
    sample to be from cultivar 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if the first discriminant function is > 2.122505, predict the sample to be from
    cultivar 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can examine the accuracy of this allocation rule by using the `calcAllocationRuleAccuracy()`
    function below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to calculate the accuracy for the wine data based on the allocation
    rule for the first discriminant function, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be displayed in a *confusion matrix*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Allocated to group 1 | Allocated to group 2 | Allocated to group 3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Is group 1 | 56 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Is group 2 | 5 | 65 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Is group 3 | 0 | 0 | 48 |'
  prefs: []
  type: TYPE_TB
- en: 'There are 3+5+1=9 wine samples that are misclassified, out of (56+3+5+65+1+48=)
    178 wine samples: 3 samples from cultivar 1 are predicted to be from cultivar
    2, 5 samples from cultivar 2 are predicted to be from cultivar 1, and 1 sample
    from cultivar 2 is predicted to be from cultivar 3\. Therefore, the misclassification
    rate is 9/178, or 5.1%. The misclassification rate is quite low, and therefore
    the accuracy of the allocation rule appears to be relatively high.'
  prefs: []
  type: TYPE_NORMAL
- en: However, this is probably an underestimate of the misclassification rate, as
    the allocation rule was based on this data (this is the *training set*). If we
    calculated the misclassification rate for a separate *test set* consisting of
    data other than that used to make the allocation rule, we would probably get a
    higher estimate of the misclassification rate.
  prefs: []
  type: TYPE_NORMAL
- en: '#### The Python way'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python allows to do all the above in a much faster way and providing extended
    automatic report capabilities by using the `sklearn.metrics` module. The above
    confusion matrix and reporting typical performance metrics, such as *precision*,
    *recall*, *f1-score* can be done in python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Allocated to group 1 | Allocated to group 2 | Allocated to group 3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Is group 1 | 56 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Is group 2 | 5 | 65 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Is group 3 | 0 | 0 | 48 |'
  prefs: []
  type: TYPE_TB
- en: '![png](_images/a_little_book_of_python_for_multivariate_analysis_168_2.png)  ##
    Links and Further Reading'
  prefs: []
  type: TYPE_NORMAL
- en: Here are some info and links for further reading.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn about multivariate analysis I would recommend the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Multivariate Data Analysis](http://www.bookbutler.co.uk/compare?isbn=9781292021904)
    [http://www.bookbutler.co.uk/compare?isbn=9781292021904] by Hair et. al.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Applied Multivariate Data Analysis](http://www.bookbutler.co.uk/compare?isbn=9780340741221)
    [http://www.bookbutler.co.uk/compare?isbn=9780340741221] by Everitt and Dunn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are new to python then you can read one of the overwhelming number of
    tutorials that exist on the web. Here are a couple of links:'
  prefs: []
  type: TYPE_NORMAL
- en: The [official tutorial](https://docs.python.org/2/tutorial/) [https://docs.python.org/2/tutorial/]
    by the Python organization. Extensive covering pretty much everything in core
    python and with lots of detailed non-interactive examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For those that prefer learning by doing through interactive tutorials here
    are some good ones I would recommend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Codeacademy Python tutorial](https://www.codecademy.com/learn/python) [https://www.codecademy.com/learn/python]'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[learnpython.org](http://www.learnpython.org) [http://www.learnpython.org]'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn about data analysis and data science using the Python ecosystem I
    would recommend the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do)
    [http://shop.oreilly.com/product/0636920023784.do] by Wes McKinney'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science from Scratch](http://shop.oreilly.com/product/0636920033400.do)
    [http://shop.oreilly.com/product/0636920033400.do] by Joel Grus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn about doing machine learning in Python using scikit-learn I would
    recommend:'
  prefs: []
  type: TYPE_NORMAL
- en: The [scikit-learn homepage](http://scikit-learn.org) [http://scikit-learn.org]
    not only has excellent documentation and examples, but also provides useful and
    clear resources about machine learning methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)
    [https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning]
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The visualizations here were produced using [matplotlib](http://matplotlib.org)
    [http://matplotlib.org] and [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)
    [http://stanford.edu/~mwaskom/software/seaborn/]. Their homepages have extensive
    documentation about their APIs and tons of examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although Python is self-sufficient, fairly extensive and probably growing more
    rapidly than any other scientific language, if the need arises that you need to
    use R for whatever reason, then you are able to do so within Python itself via
    the [`rpy2` library](http://rpy2.readthedocs.org) [http://rpy2.readthedocs.org].  ##
    Acknowledgements'
  prefs: []
  type: TYPE_NORMAL
- en: I would like to thank [Avril Coghlan](http://www.sanger.ac.uk/research/projects/parasitegenomics/)
    [http://www.sanger.ac.uk/research/projects/parasitegenomics/], Wellcome Trust
    Sanger Institute, Cambridge, U.K. for her excellent resource [A Little Book of
    R for Multivariate Analysis](https://little-book-of-r-for-multivariate-analysis.readthedocs.org)
    [https://little-book-of-r-for-multivariate-analysis.readthedocs.org] and releasing
    it under a [CC-BY-3.0 License](https://creativecommons.org) [https://creativecommons.org],
    hence, allowing this translation from R to Python. All kudos to her.
  prefs: []
  type: TYPE_NORMAL
- en: As the original, many of the examples in this booklet are inspired by examples
    in the Open University book, “Multivariate Analysis” (product code M249/03).
  prefs: []
  type: TYPE_NORMAL
- en: 'I am also grateful to the UCI Machine Learning Repository, http://archive.ics.uci.edu/ml,
    for making data sets available which were used in the examples in this booklet.  ##
    Contact'
  prefs: []
  type: TYPE_NORMAL
- en: 'I will be grateful if you will send me ([Yiannis Gatsoulis](http://gats.me)
    [http://gats.me]) corrections or suggestions for improvements to my email address
    [gatsoulis AT gmail DOT com](mailto:gatsoulis%40gmail.com).  ## License'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Creative Commons License](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: A Little Book of Python for Multivariate Analysis by Yiannis Gatsoulis is licensed
    under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).
  prefs: []
  type: TYPE_NORMAL
- en: Based on a work at [A Little Book of R for Multivariate Analysis](https://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html)
    by Avril Coghlan licensed under [CC-BY-3.0](http://creativecommons.org/licenses/by/3.0/)
    [http://creativecommons.org/licenses/by/3.0/]. © Copyright 2016, Yiannis Gatsoulis.
    Created using [Sphinx](http://sphinx-doc.org/) 1.3.4.
  prefs: []
  type: TYPE_NORMAL
- en: Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[index](# "General Index")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Little Book of Python for Multivariate Analysis 0.1 documentation](index.html)
    »'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: © Copyright 2016, Yiannis Gatsoulis. Created using [Sphinx](http://sphinx-doc.org/)
    1.3.4.
  prefs: []
  type: TYPE_NORMAL
