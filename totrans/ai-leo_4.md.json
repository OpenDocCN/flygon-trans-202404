["```\nprint(\"Hello World\") -- First thing, note that there is no main...\n--[[\nThis is how we do a multi-line comment\non lua, to execute a lua program just use...\nlua someFile.lua\n]] \n```", "```\nsomeVar = \"Leonardo\"\nio.write(\"Size of variable is \", #someVar, \" and it's value is: \\\"\", someVar, \"\\\"\\n\")\n-- Variables on lua are dynamically typed...\nsomeVar = 10; -- You can use \";\" to end a statement\nio.write(\"Now it's value is:\\\"\", someVar, \"\\\"\") \n```", "```\nprint(type(someVar))\nsomeVar = 'Leonardo' -- Strings can use use simple quotes\nprint(type(someVar))\nsomeVar = true\nprint(type(someVar))\nsomeVar = {1,2,\"leo\",true}\nprint(type(someVar)) \n```", "```\nio.write(\"5 + 3 = \", 5+3, \"\\n\")\nio.write(\"5 - 3 = \", 5-3, \"\\n\")\nio.write(\"5 * 3 = \", 5*3, \"\\n\")\nio.write(\"5 / 3 = \", 5/3, \"\\n\")\nio.write(\"5.2 % 3 = \", 5%3, \"\\n\")\n-- Generate random number between 0 and 1\nio.write(\"math.random() : \", math.random(0,3), \"\\n\")\n-- Print float to 10 decimals\nprint(string.format(\"Pi = %.10f\", math.pi)) \n```", "```\n5 + 3 = 8\n5 - 3 = 2\n5 * 3 = 15\n5 / 3 = 1.6666666666667\n5.2 % 3 = 2\nmath.random() : 2\nPi = 3.1415926536 \n```", "```\nrequire 'image'\npedestrian = image.load('./pedestrianSign.png')\nitorch.image(pedestrian) \n```", "```\nage = 17\nif age < 16 then \n    print(string.format(\"You are still a kid with %d years old\\n\", age))\nelseif (age == 34) or (age==35) then\n    print(\"Getting old leo...\")\nelse\n    print(\"Hi young man\")\nend\n\n-- Lua does not have ternary operators\ncanVote = age > 18 and true or false -- canVote=true if age>18 else canVote=false\nio.write(\"Can I vote: \", tostring(canVote)) \n```", "```\ni = 1\nwhile (i <= 10) do\n    io.write(i,\"\\n\")\n    i = i+1\n    if i==4 then break end\nend \n```", "```\n-- Initial value, end value, increment at each loop...\nfor i=1,3,1 do\n    io.write(i,\"\\n\")\nend \n```", "```\n-- Create a table which is a list of items like an array\nsomeTable = {\"January\", \"February\", \"March\", \"April\",10}\n\n-- Iterate on table months\nfor keyVar, valueVar in pairs(someTable) do\n  io.write(valueVar, \"(key=\", keyVar, \"), \")\nend \n```", "```\nJanuary(key=1), February(key=2), March(key=3), April(key=4), 10(key=5), \n```", "```\n-- Function definition\nfunction getSum(a,b)\n    return a+b\nend\n\n-- Call function\nprint(string.format(\"5 + 2 = %d\", getSum(5,2))) \n```", "```\n-- tables\ndict = {a = 1, b = 2, c = 3} \nlist = {10,20,30} \n\n-- two prints that display the same value\nprint(dict.a)\nprint(dict[\"a\"])\n-- Tables start with 1 (Like matlab)\nprint(list[1]) \n\n-- You can also add functions on tables\ntab = {1,3,4}\n-- Add function sum to table tab\nfunction tab.sum ()\n  c = 0\n  for i=1,#tab do\n    c = c + tab[i]\n  end\n  return c\nend\n\nprint(tab:sum()) -- displays 8 (the colon is used for calling methods) \n-- tab:sum() means tab.sum(tab)\nprint(tab.sum()) -- On this case it will also work\nprint(tab) \n```", "```\n1    \n1    \n10    \n8    \n8    \n{\n  1 : 1\n  2 : 3\n  3 : 4\n  sum : function: 0x4035ede8\n} \n```", "```\n--[[\n\nCreate a class \"Animal\" with properties:height,weight,name,sound\nand methods: new,getInfo,saySomething\n\n]]\n\n-- Define the defaults for our table\nAnimal = {height = 0, weight = 0, name = \"No Name\", sound = \"No Sound\"}\n\n-- Constructor\nfunction Animal:new (height, weight, name, sound) \n  -- Set a empty metatable to the table Animal (Crazy whay to create an instance) \n  setmetatable({}, Animal)\n  -- Self is a reference to this table instance\n  self.height = height\n  self.weight = weight\n  self.name = name\n  self.sound = sound \n  return self\nend\n\n-- Some method\nfunction Animal:getInfo() \n  animalStr = string.format(\"%s weighs %.1f kg, is %.1fm in tall\", self.name, self.weight, self.height) \n  return animalStr\nend\n\nfunction Animal:saySomething()\n    print(self.sound)\nend \n```", "```\n-- Create an Animal\nflop = Animal:new(1,10.5,\"Flop\",\"Auau\")\nprint(flop.name) -- same as flop[\"name\"]\nprint(flop:getInfo()) -- same as flop.getInfo(flop)\nprint(flop:saySomething())\n\n-- Other way to say the samething\nprint(flop[\"name\"]) \nprint(flop.getInfo(flop))\n\n-- Type of our object\nprint(type(flop)) \n```", "```\nFlop    \nFlop weighs 10.5 kg, is 1.0m in tall    \nAuau    \n\nFlop    \nFlop weighs 10.5 kg, is 1.0m in tall    \ntable \n```", "```\n-- Open a file to write\nfile = io.open(\"./output.txt\", \"w\")\n\n-- Copy the content of the file input.txt to test.txt\nfor line in io.lines(\"./input.txt\") do\n  print(line)\n  file:write(string.format(\"%s from input (At output)\\n\", line)) -- write on file\nend\n\nfile:close() \n```", "```\nLine 1 at input    \nLine 2 at input \n```", "```\nlocal t = os.execute(\"ls\")\nprint(t)\nlocal catResult = os.execute(\"cat output.txt\")\nprint(catResult) \n```", "```\nFirstContactTorch.ipynb\ninput.txt\nLuaLanguage.ipynb\noutput.txt\npedestrianSign.png\nplot.png\ntrue    \n\nLine 1 at input from input (At output)\nLine 2 at input from input (At output)\ntrue \n```", "```\n-- Include torch library\nrequire 'torch'; -- Like matlab the \";\" also avoid echo the output\n\n-- Create a 2x4 matrix\nm = torch.Tensor({{1, 2, 3, 4}, {5, 6, 7, 8}})\nprint(m)\n\n-- Get element at second row and third collumn\nprint(m[{2,3}]) \n```", "```\n 1  2  3  4\n 5  6  7  8\n[torch.DoubleTensor of size 2x4]\n\n7 \n```", "```\n-- Define some Matrix/Tensors\na = torch.Tensor(5,3) -- construct a 5x3 matrix/tensor, uninitialized\na = torch.rand(5,3) -- Create a 5x3 matrix/tensor with random values\nb=torch.rand(3,4) -- Create a 3x4 matrix/tensor with random values\n\n-- You can also fill a matrix with values (On this case with zeros)\nallZeros = torch.Tensor(2,2):fill(0)\nprint(allZeros)\n\n-- Matrix multiplcation and it's syntax variants\nc = a*b \nc = torch.mm(a,b)\nprint(c)\nd=torch.Tensor(5,4)\nd:mm(a,b) -- store the result of a*b in c\n\n-- Transpose a matrix\nm_trans = m:t()\nprint(m_trans) \n```", "```\n 0  0\n 0  0\n[torch.DoubleTensor of size 2x2]\n\n 0.8259  0.6816  0.3766  0.7048\n 1.3681  0.9438  0.7739  1.1653\n 1.2885  0.9653  0.5573  0.9808\n 1.2556  0.8850  0.5501  0.9142\n 1.8468  1.3579  0.7680  1.3500\n[torch.DoubleTensor of size 5x4]\n\n 1  5\n 2  6\n 3  7\n 4  8\n[torch.DoubleTensor of size 4x2] \n```", "```\n-- Include torch (cuda) library\nrequire 'cutorch'\n\n-- Move arrays to GPU (and convert it's types to cuda types)\na = a:cuda()\nb = b:cuda()\nd = d:cuda()\n\n-- Same multiplication just different syntax\nc = a*b\nd:mm(a,b)\n\nprint(c) \n```", "```\n 1.1058  0.6183  1.0518  0.7451\n 0.5932  0.8015  0.9441  0.5477\n 0.4915  0.8143  0.9613  0.4345\n 0.1699  0.6697  0.6656  0.2500\n 0.6525  0.6174  0.8894  0.4446\n[torch.CudaTensor of size 5x4] \n```", "```\nPlot = require 'itorch.Plot'\n\n-- Give me 10 random numbers\nlocal y = torch.randn(10) \n\n-- Get 1d tensor from 0 to 9 (10 elements)\nlocal x = torch.range(0, 9)\nPlot():line(x, y,'red' ,'Sinus Wave'):title('Simple Plot'):draw() \n```", "```\nrequire \"nn\"\n\n-- make a multi-layer perceptron\nmlp = nn.Sequential();  \n-- 2 inputs, one output 1 hidden layer with 20 neurons\ninputs = 2; outputs = 1; hiddenUnits = 20; \n\n-- Mount the model\nmlp:add(nn.Linear(inputs, hiddenUnits))\nmlp:add(nn.Tanh())\nmlp:add(nn.Linear(hiddenUnits, outputs)) \n```", "```\ncriterion_MSE = nn.MSECriterion() \n```", "```\nfor i = 1,2500 do\n  -- random sample\n  local input= torch.randn(2);     -- normally distributed example in 2d\n  local output= torch.Tensor(1);\n  -- Create XOR lables on the fly....\n  if input[1] * input[2] > 0 then  \n    output[1] = -1\n  else\n    output[1] = 1\n  end\n\n  -- Feed to the model (with current set of weights), then calculate a loss\n  criterion_MSE:forward(mlp:forward(input), output)\n\n  -- Reset the current gradients before backpropagate (Always do)\n  mlp:zeroGradParameters()\n  -- Backpropagate the loss to the hidden layer\n  mlp:backward(input, criterion_MSE:backward(mlp.output, output))\n  -- Update parameters(Gradient descent) with alpha=0.01\n  mlp:updateParameters(0.01)\nend \n```", "```\nx = torch.Tensor(2)\nx[1] =  0.5; x[2] =  0.5; print(mlp:forward(x)) -- 0 XOR 0 = 0 (negative)\nx[1] =  0.5; x[2] = -0.5; print(mlp:forward(x)) -- 0 XOR 1 = 1 (positive)\nx[1] = -0.5; x[2] =  0.5; print(mlp:forward(x)) -- 1 XOR 0 = 1 (positive)\nx[1] = -0.5; x[2] = -0.5; print(mlp:forward(x)) -- 1 XOR 1 = 0 (negative) \n```", "```\n-0.8257\n[torch.DoubleTensor of size 1]\n\n 0.6519\n[torch.DoubleTensor of size 1]\n\n 0.4468\n[torch.DoubleTensor of size 1]\n\n-0.7814\n[torch.DoubleTensor of size 1] \n```", "```\n-- Create a dataset (128 elements)\nbatchSize = 128\nbatchInputs = torch.Tensor(batchSize, inputs)\nbatchLabels = torch.DoubleTensor(batchSize)\n\nfor i=1,batchSize do\n  local input = torch.randn(2)     -- normally distributed example in 2d\n  local label = 1\n  if input[1]*input[2]>0 then     -- calculate label for XOR function\n    label = -1;\n  end\n  batchInputs[i]:copy(input)\n  batchLabels[i] = label\nend \n```", "```\n-- Get flatten parameters (Needed to use optim)\nparams, gradParams = mlp:getParameters()\n-- Learning parameters\noptimState = {learningRate=0.01} \n```", "```\nrequire 'optim'\n\nfor epoch=1,200 do\n  -- local function we give to optim\n  -- it takes current weights as input, and outputs the loss\n  -- and the gradient of the loss with respect to the weights\n  -- gradParams is calculated implicitly by calling 'backward',\n  -- because the model's weight and bias gradient tensors\n  -- are simply views onto gradParams\n  local function feval(params)\n    gradParams:zero()\n\n    local outputs = mlp:forward(batchInputs)\n    local loss = criterion_MSE:forward(outputs, batchLabels)\n    local dloss_doutput = criterion_MSE:backward(outputs, batchLabels)\n    mlp:backward(batchInputs, dloss_doutput)    \n    return loss,gradParams\n  end\n  optim.sgd(feval, params, optimState)\nend \n```", "```\nx = torch.Tensor(2)\nx[1] =  0.5; x[2] =  0.5; print(mlp:forward(x)) -- 0 XOR 0 = 0 (negative)\nx[1] =  0.5; x[2] = -0.5; print(mlp:forward(x)) -- 0 XOR 1 = 1 (positive)\nx[1] = -0.5; x[2] =  0.5; print(mlp:forward(x)) -- 1 XOR 0 = 1 (positive)\nx[1] = -0.5; x[2] = -0.5; print(mlp:forward(x)) -- 1 XOR 1 = 0 (negative) \n```", "```\n-0.6607\n[torch.DoubleTensor of size 1]\n\n 0.5321\n[torch.DoubleTensor of size 1]\n\n 0.8285\n[torch.DoubleTensor of size 1]\n\n-0.7458\n[torch.DoubleTensor of size 1] \n```", "```\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl\nsudo pip3 install --ignore-installed --upgrade $TF_BINARY_URL \n```", "```\n# Import tensorflow\nimport tensorflow as tf\n\n# Build graph\na = tf.placeholder('float')\nb = tf.placeholder('float')\n\n# Graph\ny = tf.mul(a,b)\n\n# Create session passing the graph\nsession = tf.Session()\n# Put the values 3,4 on the placeholders a,b\nprint session.run(y,feed_dict={a: 3, b:4}) \n```", "```\n # Import libraries (Numpy, Tensorflow, matplotlib)\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nget_ipython().magic(u'matplotlib inline')\n\n# Create 100 points following a function y=0.1 * x + 0.3 with some normal random distribution\nnum_points = 100\nvectors_set = []\nfor i in xrange(num_points):\n    x1 = np.random.normal(0.0, 0.55)\n    y1 = x1 * 0.1 + 0.3 + np.random.normal(0.0, 0.03)\n    vectors_set.append([x1, y1])\n\nx_data = [v[0] for v in vectors_set]\ny_data = [v[1] for v in vectors_set]\n\n# Plot data\nplt.plot(x_data, y_data, 'r*', label='Original data')\nplt.legend()\nplt.show() \n```", "```\n # Create our linear regression model\n# Variables resides internally inside the graph memory\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.zeros([1.0]))\ny = W * x_data + b\n\n# Define a loss function that take into account the distance between\n# the prediction and our dataset\nloss = tf.reduce_mean(tf.square(y-y_data))\n\n# Create an optimizer for our loss function (With gradient descent)\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss) \n```", "```\n # Run session\n# Initialize all graph variables\ninit = tf.initialize_all_variables()\n# Create a session and initialize the graph variables (Will acutally run now...)\nsession = tf.Session()\nsession.run(init)\n\n# Train on 8 steps\nfor step in xrange(8):\n    # Optimize one step\n    session.run(train)\n    # Get access to graph variables(just read) with session.run(varName) \n    print(\"Step=%d, loss=%f, [W=%f b=%f]\") % (step,session.run(loss),session.run(W),session.run(b))\n\n# Just plot the set of weights and bias with less loss (last)\nplt.plot(x_data, y_data, 'ro')\nplt.plot(x_data, session.run(W) * x_data + session.run(b))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n# Close the Session when we're done.\nsession.close() \n```", "```\nimage1.png 0\nimage2.png 0\nimage3.png 1\nimage4.png 1\nimage5.png 2\nimage6.png 2 \n```", "```\n train_data, train_label = getDataFromFile('trainingFile.txt')\nval_data, val_label = getDataFromFile('validationFile.txt')\n\n## Give to your graph through placeholders... \n```", "```\ntensorboard --logdir=/home/leo/test \n```", "```\n # Create our linear regression model\n# Variables resides internally inside the graph memory\n\n#tf.name_scope organize things on the tensorboard graphview\nwith tf.name_scope(\"LinearReg\") as scope:\n    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\"Weights\")\n    b = tf.Variable(tf.zeros([1.0]), name=\"Bias\")\n    y = W * x_data + b\n\n# Define a loss function that take into account the distance between\n# the prediction and our dataset\nwith tf.name_scope(\"LossFunc\") as scope:\n    loss = tf.reduce_mean(tf.square(y-y_data))\n\n# Create an optimizer for our loss function\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\n#### Tensorboard stuff\n# Annotate loss, weights and bias (Needed for tensorboard)\nloss_summary = tf.scalar_summary(\"loss\", loss)\nw_h = tf.histogram_summary(\"W\", W)\nb_h = tf.histogram_summary(\"b\", b)\n\n# Merge all the summaries\nmerged_op = tf.merge_all_summaries() \n```", "```\n # Initialize all graph variables\ninit = tf.initialize_all_variables()\n\n# Create a session and initialize the graph variables (Will acutally run now...)\nsession = tf.Session()\nsession.run(init)\n\n# Writer for tensorboard (Directory)\nwriter_tensorboard = tf.train.SummaryWriter('/home/leo/test', session.graph_def) \n```", "```\n for step in xrange(1000):\n    # Optimize one step\n    session.run(train)\n\n    # Add summary (Everytime could be to much....)\n    result_summary = session.run(merged_op)    \n    writer_tensorboard.add_summary(result_summary, step) \n```", "```\nfrom tensorflow.python.client import device_lib\ndef get_devices_available():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos] \n```", "```\nprint(get_devices_available()) \n```", "```\n['/cpu:0', '/gpu:0', '/gpu:1'] \n```", "```\nimport tensorflow as tf\n\n# Creates a graph.\nwith tf.device('/gpu:0'):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\n\n# Creates a session with log_device_placement set to True, this will dump \n# on the log how tensorflow is mapprint the operations on devices\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n# Runs the op.\nprint(sess.run(c))\nsess.close() \n```", "```\n[[ 22\\.  28.]\n [ 49\\.  64.]] \n```", "```\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nprint('Test shape:',mnist.test.images.shape)\nprint('Train shape:',mnist.train.images.shape) \n```", "```\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nTest shape: (10000, 784)\nTrain shape: (55000, 784) \n```", "```\n# Parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits) \n```", "```\nx = tf.placeholder(\"float\", [None, n_input])\ny = tf.placeholder(\"float\", [None, n_classes]) \n```", "```\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())        \n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.relu(layer_1)\n\n    # Hidden layer with RELU activation\n    print( 'layer_1:', layer_1.get_shape(), 'W2:', weights['h2'].get_shape(), 'b2:', biases['b2'].get_shape())        \n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.relu(layer_2)\n\n    # Output layer with linear activation\n    print( 'layer_2:', layer_2.get_shape(), 'W3:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out'] \n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer \n```", "```\n# Store layers weight & bias\nweights = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),    #784x256\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #256x256\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))  #256x10\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #256x1\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #256x1\n    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n}\n\n# Construct model\npred = multilayer_perceptron(x, weights, biases) \n```", "```\nx: (?, 784) W1: (784, 256) b1: (256,)\nlayer_1: (?, 256) W2: (256, 256) b2: (256,)\nlayer_2: (?, 256) W3: (256, 10) b3: (10,)\nout_layer: (?, 10) \n```", "```\n# Cross entropy loss function\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n\n# On this case we choose the AdamOptimizer\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) \n```", "```\n# Initializing the variables\ninit = tf.initialize_all_variables()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                                          y: batch_y})\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n                \"{:.9f}\".format(avg_cost))\n    print(\"Optimization Finished!\")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    # To keep sizes compatible with model\n    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})) \n```", "```\nEpoch: 0001 cost= 152.289635962\nEpoch: 0002 cost= 39.134648348    \n...\nEpoch: 0015 cost= 0.850344581\nOptimization Finished!\nAccuracy: 0.9464 \n```", "```\nimport tensorflow.contrib.learn as skflow\nfrom sklearn import datasets, metrics\nfrom sklearn import cross_validation \n```", "```\niris = datasets.load_iris()\nx_train, x_test, y_train, y_test = cross_validation.train_test_split(\n    iris.data, iris.target, test_size=0.2, random_state=42)\n\n# Feature columns is required for new versions\nfeature_columns = skflow.infer_real_valued_columns_from_input(x_train) \n```", "```\nclassifier = skflow.LinearClassifier(feature_columns=feature_columns, n_classes=3,model_dir='/tmp/tf/linear/')\nclassifier.fit(x_train, y_train, steps=200, batch_size=32)\nscore = metrics.accuracy_score(y_test, classifier.predict(x_test))\nprint(\"Accuracy: %f\" % score) \n```", "```\nAccuracy: 0.966667 \n```", "```\nclassifier = skflow.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 20, 10], \n                                  n_classes=3,model_dir='/tmp/tf/mlp/')\nclassifier.fit(x_train, y_train, steps=200)\n\nscore = metrics.accuracy_score(y_test, classifier.predict(x_test))\nprint(\"Accuracy: %f\" % score) \n```", "```\nAccuracy: 1.000000 \n```", "```\ntensorboard --logdir=/tmp/tf_examples/test/ \n```", "```\nclassifier = skflow.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3,model_dir='/tmp/tf_examples/test/')\nclassifier.fit(x_train, y_train, steps=200)\nscore = metrics.accuracy_score(y_test, classifier.predict(x_test))\nprint(\"Accuracy: %f\" % score) \n```", "```\nAccuracy: 1.000000 \n```"]