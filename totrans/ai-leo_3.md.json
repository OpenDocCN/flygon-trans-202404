["```\nfunction [ reward, state ] = simple_RL_enviroment( action, restart )\n% Simple enviroment of reinforcement learning example\n%   http://mnemstudio.org/path-finding-q-learning-tutorial.htm\npersistent current_state\nif isempty(current_state)\n    % Initial random state (excluding goal state)\n    current_state = randi([1,5]);     \nend\n\n% The rows of R encode the states, while the columns encode the action\nR = [ -1 -1 -1 -1  0  -1; ...\n    -1 -1 -1  0 -1 100; ...\n    -1 -1 -1  0 -1  -1; ...\n    -1  0  0 -1  0  -1; ...\n    0 -1 -1  0 -1 100; ...\n    -1  0 -1 -1  0 100 ];\n\n% Sample our R matrix (model)\nreward = R(current_state,action);\n\n% Good action taken\nif reward ~=-1\n    % Returns next state (st+1)\n    current_state = action;        \nend\n\n% Game should be reseted\nif restart == true\n    % Choose another initial state\n    current_state = randi([1,5]); \n    reward = -1;\n    % We decrement 1 because matlab start the arrays at 1, so just to have\n    % the messages with the same value as the figures on the tutorial we\n    % take 1....\n    fprintf('Enviroment initial state is %d\\n',current_state-1);\nend\n\nstate = current_state;\nend \n```", "```\nfunction [ Q ] = simple_RL_agent( )\n% Simple agent of reinforcement learning example\n%   http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n% Train, then normalize Q (divide Q by it's biggest value)\nQ = train(); Q = Q / max(Q(:));\n% Get the best actions for each possible initial state (1,2,3,4,5)\ntest(Q);\nend\n\nfunction Q = train()\n% Initial training parameters\ngamma = 0.8;\ngoalState=6;\nnumTrainingEpisodes = 20;\n% Set Q initial value\nQ = zeros(6,6);\n\n% Learn from enviroment iteraction\nfor idxEpisode=1:numTrainingEpisodes\n    validActionOnState = -1;\n    % Reset environment\n    [~,currentState] = simple_RL_enviroment(1, true);\n\n    % Episode (initial state to goal state)\n    % Break only when we reach the goal state\n    while true\n        % Choose a random action possible for the current state\n        while validActionOnState == -1\n            % Select a random possible action\n            possibleAction = randi([1,6]);\n\n            % Interact with enviroment and get the immediate reward\n            [ reward, ~ ] = simple_RL_enviroment(possibleAction, false);\n            validActionOnState = reward;\n        end\n        validActionOnState = -1;\n\n        % Update Q\n        % Get the biggest value from each row of Q, this will create the\n        % qMax for each state\n        next_state = possibleAction;\n        qMax = max(Q,[],2);\n        Q(currentState,possibleAction) = reward + ...\n            (gamma*(qMax(next_state)));\n\n        if currentState == goalState\n            break;\n        end\n\n        % Non this simple example the next state will be the action\n        currentState = possibleAction;\n    end\n    fprintf('Finished episode %d restart enviroment\\n',idxEpisode);\nend\nend\n\nfunction test(Q)\n    % Possible permuted initial states, observe that you don't include the\n    % goal state 6 (room5)\n    possible_initial_states = randperm(5);\n    goalState=6;\n\n    % Get the biggest action for every state\n    [~, action_max] = max(Q,[],2);\n\n    for idxStates=1:numel(possible_initial_states)\n        curr_state = possible_initial_states(idxStates);\n        fprintf('initial state room_%d actions=[ ', curr_state-1);\n        % Follow optimal policy from intial state to goal state\n        while true\n            next_state = action_max(curr_state);\n            fprintf(' %d,', next_state-1);\n            curr_state = next_state;\n            if curr_state == goalState\n                fprintf(']');\n               break \n            end\n        end\n        fprintf('\\n');\n    end\nend \n```"]