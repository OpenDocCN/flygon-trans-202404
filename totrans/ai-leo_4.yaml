- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'On this chapter we will learn the basics topics to better understand the book.
    The following topics will be presented:'
  prefs: []
  type: TYPE_NORMAL
- en: Lua, Torch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python, numpy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matlab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lua and Torch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this book I stressed out the importance of knowing how to write your own
    deep learning/artificial intelligence library. But is also very important specially
    while researching some topic, to understand the most common libraries. This chapter
    will teach the basics on Torch, but before that we're going also to learn Lua.
  prefs: []
  type: TYPE_NORMAL
- en: Lua language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Lua was first created to be used on embedded systems, the idea was to have a
    simple cross-platform and fast language. One the main features of Lua is it's
    easy integration with C/C++.
  prefs: []
  type: TYPE_NORMAL
- en: Lua was originally designed in 1993 as a language for extending software applications
    to meet the increasing demand for customization at the time.
  prefs: []
  type: TYPE_NORMAL
- en: This extension means that you could have a large C/C++ program and, some parts
    in Lua where you could easily change without the need to recompile everything.
  prefs: []
  type: TYPE_NORMAL
- en: Torch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Torch is a scientific computing framework based on Lua with CPU and GPU backends.
    You can imagine like a Numpy but with CPU and GPU implementation. Some nice features:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient linear algebra functions with GPU support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural Network package, with automatic differentiation (No need to backpropagate
    manually)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-GPU support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First contact with Lua
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we have some simple examples on Lua just to have some contact with the
    language.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Lua datatypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The language offer those basic types:'
  prefs: []
  type: TYPE_NORMAL
- en: Numbers(Float)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: string
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: boolean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Doing some math
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Normally we will rely on Torch, but Lua has some math support as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Lua include (require)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The lua statement to include other lua files is the "require", as usual it is
    used to add some library
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](pedestrianSign.png)'
  prefs: []
  type: TYPE_IMG
- en: Conditionals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Just the simple if-then-else. Lua does not have switch statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Lua have while, repeat and for loops. For loops has also a "for-each" extension
    to iterate on tables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Defining functions in Lua is quite easy, it's syntax reminds matlab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Lua we use tables for everything else (ie: Lists, Dictionaries, Classes,
    etc...)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Object oriented programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Lua does not support directly OOP, but you can emulate all it's main functionalities
    (Inheritance, Encapsulation) with tables and metatables
  prefs: []
  type: TYPE_NORMAL
- en: 'Metatable tutorial: Used to override operations (metamethods) on tables.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: File I/O
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Run console commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: First contact with Torch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this section we're going to see how to do simple operations with Torch, more
    complex stuff will be dealt latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the torch objectives is to give some matlab functionality, an usefull
    cheetsheat can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Some Matrix operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Doing operations on GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Plotting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](SimplePlotItorch.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting with nn (XOR problem)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Define the loss function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On torch the loss function is called criterion, as on this case we're dealling
    with a binary classification, we will choose the Mean Squared Error criterion
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Training Manually
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we're going to back-propagate our model to get the output related to the
    loss gradient ![](9a616dbf.png) then use gradient descent to update the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Test the network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Trainning with optimim
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Torch provides a standard way to optimize any function with respect to some
    parameters. In our case, our function will be the loss of our network, given an
    input, and a set of weights. The goal of training a neural net is to optimize
    the weights to give the lowest loss over our training set of input data. So, we
    are going to use optim to minimize the loss with respect to the weights, over
    our training set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Test the network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Tensorflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensorflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: On this chapter we're going to learn about tensorflow, which is the goolge library
    for machine learning. In simple words it's a library for numerical computation
    that uses graphs, on this graph the nodes are the operations, while the edges
    of this graph are tensors. Just to remember tensors, are multidimensional matrices,
    that will flow on the tensorflow graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](tensors_flowing.gif)'
  prefs: []
  type: TYPE_IMG
- en: '![](Tensorflow_Graph_0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After this computational graph is created it will create a session that can
    be executed by multiple CPUs, GPUs distributed or not. Here are the main components
    of tensorflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Variables: Retain values between sessions, use for weights/bias'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Nodes: The operations'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tensors: Signals that pass from/to nodes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Placeholders: Used to send data between your program and the tensorflow graph'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Session: Place when graph is executed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TensorFlow implementation translates the graph definition into executable
    operations distributed across available compute resources, such as the CPU or
    one of your computer's GPU cards. In general you do not have to specify CPUs or
    GPUs explicitly. TensorFlow uses your first GPU, if you have one, for as many
    operations as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Your job as the "client" is to create symbolically this graph using code (C/C++
    or python), and ask tensorflow to execute this graph. As you may imagine the tensorflow
    code for those "execution nodes" is some C/C++, CUDA high performance code. (Also
    difficult to understand).
  prefs: []
  type: TYPE_NORMAL
- en: For example, it is common to create a graph to represent and train a neural
    network in the construction phase, and then repeatedly execute a set of training
    ops in the graph in the execution phase.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Tensorflow_Graph_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you have already a machine with python (anaconda 3.5) and the nvidia cuda
    drivers installed (7.5) install tensorflow is simple
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If you still need to install some cuda drivers refer [here](https://www.youtube.com/watch?v=cVWVRA8XXxs)
    for instructions
  prefs: []
  type: TYPE_NORMAL
- en: Simple example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Just as a hello world let's build a graph that just multiply 2 numbers. Here
    notice some sections of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Import tensorflow library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also notice that on this example we're passing to our model some constant values
    so it's not so useful in real life.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Tensorflow_Graph_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Exchanging data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow allow exchanging data with your graph variables through "placeholders".
    Those placeholders can be assigned when we ask the session to run. Imagine placeholders
    as a way to send data to your graph when you run a session "session.run"
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Linear Regression on tensorflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to see how to create a linear regression system on tensorflow
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![](Dataset_Linear_Regression.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we're going to implement a graph with a function ![](6e6dbcfc.png), a loss
    function ![](b403dde9.png). The loss function will return a scalar value with
    the mean of all distances between our data, and the model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: With the graph built, our job is create a session to initialize all our graph
    variables, which in this case is our model parameters. Then we also need to call
    a session x times to train our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](Dataset_Linear_Regression_Result.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Is almost entirely up to you to load data on tensorflow, which means you need
    to parse the data yourself. For example one option for image classification could
    be to have text files with all the images filenames, followed by it''s class.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: trainingFile.txt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: A common API to load the data would be something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Tensorboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow offers a solution to help visualize what is happening on your graph.
    This tool is called Tensorboard, basically is a webpage where you can debug your
    graph, by inspecting it's variables, node connections etc...
  prefs: []
  type: TYPE_NORMAL
- en: '![](TensorBoardScreenhsot.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to use tensorboard you need to annotate on your graph, with the variables
    that you want to inspect, ie: the loss value. Then you need to generate all the
    summaries, using the function tf.merge_all_summaries().'
  prefs: []
  type: TYPE_NORMAL
- en: Optionally you can also use the function "tf.name_scope" to group nodes on the
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all variables are annotated and you configure your summary, you can go
    to the console and call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Considering the previous example here are the changes needed to add information
    to tensorboard.
  prefs: []
  type: TYPE_NORMAL
- en: 1) First we annotate the information on the graph that you are interested to
    inspect building phase. Then call merge_all_summaries(). On our case we annotated
    loss (scalar) and W,b(histogram)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 2) During our session creation we need to add a call to "tf.train.SummaryWriter"
    to create a writer. You need to pass a directory where tensorflow will save the
    summaries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 3) Then when we execute our graph, for example during training we can ask tensorflow
    to generate a summary. Of course calling this every time will impact performance.
    To manage this you could call this at the end of every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Results on tensorboard
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see our linear regression model as a computing graph. ![](GraphLinearRegTensorflow.png)
  prefs: []
  type: TYPE_NORMAL
- en: Bellow we can see how the loss evolved on each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](LossLinearRegTensorflow.png)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes ipython hold versions of your graph that create problems when using
    tensorboard, one option is to restart the kernel, if you have problems.
  prefs: []
  type: TYPE_NORMAL
- en: Using GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow also allows you to use GPUs to execute graphs or particular sections
    of your graph.
  prefs: []
  type: TYPE_NORMAL
- en: On common machine learning system you would have one multi-core CPU, with one
    or more GPUs, tensorflow represent them as follows
  prefs: []
  type: TYPE_NORMAL
- en: '"/cpu:0": Multicore CPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"/gpu0": First GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"/gpu1": Second GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately tensorflow does not have an official function to list the devices
    available on your system, but there is an unofficial way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Fix graph to a device
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Use the "with tf.device('/gpu:0')" statement on python to lock all nodes on
    this graph block to a particular gpu.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Multiple Gpus and training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will explain how training is one on a multiple GPU system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](multipleGpu_Train.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Baiscally the steps for multiple gpu training is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Separate your training data in batches as usual
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a copy of the model in each gpu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute different batches for each gpu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each gpu will forward the batch and calculate it's gradients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each gpu will send the gradients to the cpu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cpu will average each gradient, and do a gradient descent. The model parameters
    are updated with the gradients averaged across all model replicas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cpu will distribute the new model to all gpus
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the process loop again until all training is done
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multi Layer Perceptron MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi Layer Perceptron MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load tensorflow library and MNIST data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Neural network parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Build graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Initialize weights and construct the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Define Loss function, and Optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Launch graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Convolution Neural Network MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution Neural Network MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SkFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SkFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to make the use of tensorflow simpler to experiment machine learning,
    google offered a library that stays on top of tensorflow. Skflow make life easier.
  prefs: []
  type: TYPE_NORMAL
- en: Import library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Load dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Linear classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Multi layer perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Using Tensorboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's much easier to monitor your model with tensorboard through skflow. Just
    add the parameter "model_dir" to the classifier constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this code, type on your server console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
