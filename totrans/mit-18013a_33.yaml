- en: 'Chapter 32: Some Linear Algebra'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter contains a review of the basics of linear algebra: the solution
    of linear equations, matrix inversion, determinants, transformations, invariants,
    eigenvalues, and diagonalizability.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Topics
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 32.1   [Linear Equations](section01.html)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 32.2   [Matrices](section02.html)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 32.3   [The Inverse of a Matrix](section03.html)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 32.4   [More on Determinants](section04.html)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 32.5   [Matrices and Transformations](section05.html)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 32.6   [Invariants of Transformations](section06.html)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 32.7   [Other Notions of Diagonalizability](section07.html)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 32.8   [Computing Eigenvalues and Eigenvectors](section08.html)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 32.9   [Application to Quadratic Forms and Spring Systems](section09.html)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 32.10  [Computing Eigenvalues and Eigenvectors on a Spreadsheet](section10.html)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 32.11  [Guessing Eigenvectors](section11.html)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 32.1 Linear Equations
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a set of linear equations, **for example**
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/154e9b105ff24f2f645d842e2d91ff0b.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: and we wish to **find a solution,** by which we mean to find the **explicit
    values of x, y and z which make these equations all true.**
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental facts that allows us to find solutions are these:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Given any equation, you can multiply it (that is, multiply every term
    in it, both on the right and on the left) by any non-zero number without changing
    its implications.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Given any two equations we define their sum to be the equation whose
    left hand side is the sum of the two left hand sides, and whose right hand side
    is the sum of the two right hand sides.**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Then you can replace either of the two equations by its sum with any multiple
    of the other without changing their implications.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: you can replace the top equation above by 3x + 4y = 6 by subtracting
    the third equation from it; (subtracting the equation is the same as adding -1
    multiplied by it)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.1 Prove claim 2 here.** [Solution](exercise01.html)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: You can solve the equations by using a sequence of manipulations of the kind
    just mentioned that put the equations into the form x = a, y = b, z = c, which
    is the solution to them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**What sequence of manipulations should you use to solve equations?**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the subtraction made in the example above was chosen so that z does
    not appear in the subtracted equation, which was 3x + 4y = 6.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: If we make an addition of an appropriate multiple of the third equation to the
    second one, again the z term in the resulting sum equation can be eliminated similarly;
    with result x - 3y = 6.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: We started with three equations in three variables.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: After these operations we have eliminated z from two of them and have two equations
    in two variables.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: By a similar manipulation we can eliminate x, for example, by subtracting three
    times the second one from the first. The resulting equation is then 13y = -12.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Dividing this equation by 13 then gives us an expression for y.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: We can substitute it for y into either of the previous two equations and solve
    the resulting equation for x.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: We get ![](../Images/62f374eaec1326302efb8b511a20e170.jpg) Substituting both
    for x and y in any of the original equations then gives ![](../Images/363c1bfab175102aef42b86dc5d900af.jpg)
    and we have a complete solution to our equations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**In general you can systematically eliminate one variable at a time from all
    equations, reducing n equations in n unknowns to (n - 1) equations in (n-1) unknowns,
    and repeat the process until you can solve one equation for one unknown, and then
    substitute back to find the others, one at a time.**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: This procedure is called **"Gaussian elimination".**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.2 Perform Gaussian elimination on the following set of equations
    to find a solution**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d2893842deb50cbaeaaddcde36ce38b.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: '[Solution](exercise02.html)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Please notice that in doing these operations it is very easy to make a mistake,
    and it is wise to check your answer, once you have it, in **all** the original
    equations to see if they are satisfied.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Can this procedure fail?**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: If the equations that you start with are consistent, they will produce a unique
    solution **unless** when you try to eliminate one variable by subtracting a multiple
    of an equation from another, you eliminate the entire equation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: That is, at some stage one of your equations is a multiple of another and subtracting
    that multiple from it eliminates the entire equation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: This will happen if in the beginning one of your equations can be expressed
    as a sum of multiples of one or more of the others. (The simplest way this happens
    is when two of the equations are identical)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In this case the left hand sides of your equations are said to be **linearly
    dependent.**
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, when the equations have a unique solution, the left hand sides are
    said to be **linearly independent.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: When your equations are linearly dependent, (and you started with the same number
    of equations as you had unknowns,) you will find that you do not have enough equations
    to determine a unique solution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**This is not a disaster,** but it means that there are lots of solutions,
    at least a whole line of them.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: You can continue the Gaussian elimination process until you are down to one
    non-vanishing equation in now two or more variables. Then **any** solution to
    that equation is a solution to the original set of equations, which is said to
    be **an underdetermined set of equations.**
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Suppose for example, your last equation with all other unknowns eliminated is
    x = 2y + 3.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Then you can choose any value you please for y, compute x and then go on to
    use your other equations to compute your other unknowns, and that will be a solution,
    though of course not the only possible one. **Solutions to an equation like this
    one form a line in the xy plane.**
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 32.2 Matrices
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrices provide a convenient way to describe linear equations. Thus if you
    take the coefficients of your unknowns, in some standard order, as the row elements
    of your matrix, you define **a matrix of coefficients for any set of equations.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: For the example equations above, the coefficient matrix, call it M, is, with
    the standard ordering of x, y and z
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7149e6da35a1ff32dea21d39e453e226.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: We can then write the original equations as the single matrix equation
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/569ce160ba5a560d93f59db9050e035e.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Using the definition of matrix multiplication, which is: **taking the dot products
    of the rows of the first matrix with the columns (here single column) of the second
    to produce the corresponding elements of the product,** you should verify that
    this matrix equation is exactly the same as our original three equations.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of **Gaussian elimination** can be applied in this matrix form
    here. The rules are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. You can multiply an entire row (on both sides of the equation) by any
    non-zero number without changing the content of the equations.**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. You can add a multiple of any row to another without changing the content
    of the equations. You must add entirely across the row, including the other side
    of the matrix, however.**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: In this form such operations are called **"elementary row operations"** and
    Gaussian elimination is called **row reduction.**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: What you do here is perform enough of operation 2 to **form 0's in the matrix
    on one side of the main diagonal.** When this is done you can determine one unknown
    and then substitute successively to find the others.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: You can also attempt to perform these operations **until all elements of your
    matrix off the main diagonal are 0's,** and the diagonal elements are 1\. In that
    case the right hand side vectors are the solutions for the corresponding variables
    and you need not substitute back to find all the unknowns.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '**The n dimensional matrix whose diagonal elements are 1 and off diagonal elements
    are 0** is called the **n dimensional identity matrix,** and is written as **I**
    usually without any indication of what its size is, unless that can cause confusion,
    in which case it is written as **I[n].**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: It has the property that its matrix product with any matrix M of the same dimension
    is M itself, and its operation on any n dimensional vector **v** is **v** itself.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Thus if you start with the matrix equation **Mv = r,** and row reduce to find
    another representation of the same set of equations for which M has been reduced
    to the identity matrix I, you have **Iv = r'** where **r' is the result of the
    same row operations on the right side of the equation as those that reduced M
    to I.**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: You thereby obtain **the solution, v = r'.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 32.3 The Inverse of a Matrix
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If two square matrices M and A have the property that **MA = I,** (in infinite
    dimensions you also need the condition that AM = I) then **A and M are said to
    be inverses of one another and we write A = M^(-1) and M= A^(-1).**
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: A wonderful feature of row reduction as we have described it is that when you
    have a matrix equation AB = C, **you can apply your reduction operations for the
    matrix A to the rows of A and C simultaneously and ignore B, and what you get
    will be as true as what** you started with.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所描述的，行简化的一个很棒的特性是，当你有一个矩阵方程AB = C时，**你可以将A的简化操作同时应用于A和C的行，而忽略B，你得到的结果与**你开始的一样正确。
- en: This is exactly what we did when B was the column vector with components equal
    to our unknowns, x, y and z, but it is equally true for any matrix B.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是当B是列向量，其分量等于我们的未知数x，y和z时所做的事情，但对于任何矩阵B来说同样成立。
- en: Thus, suppose you start with the matrix equation **AA^(-1) = I.**
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，假设你从矩阵方程**AA^(-1) = I**开始。
- en: If we row reduce A so it becomes the identity matrix I, then the left hand side
    here becomes IA^(-1) which is A^(-1), the matrix inverse to A. The right hand
    side however is **what you obtain if you apply the row operations necessary to
    reduce A to the identity, starting with the identity matrix I.**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对A进行行简化，使其成为单位矩阵I，那么这里的左侧变为IA^(-1)，即A^(-1)，A的逆矩阵。然而右侧是**如果你应用必要的行操作将A简化为单位矩阵，从单位矩阵I开始，你会得到的结果。**
- en: We can conclude that the inverse matrix, **A^(-1) can be obtained by applying
    the row reduction operations that make A into I starting with I.**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，逆矩阵**A^(-1)可以通过将使A成为I的行简化操作应用于I开始的矩阵A来获得。**
- en: '**Example:** we give a two dimensional example, but the method and idea hold
    in any dimension, and computers are capable of doing this for n by n matrices
    when n is in the hundreds even thousands.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**例子：** 我们给出一个二维的例子，但这种方法和思想在任何维度都适用，当n为数百甚至数千时，计算机可以对n乘n矩阵进行这样的操作。'
- en: Suppose we want the inverse of the following matrix
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要以下矩阵的逆矩阵。
- en: '![](../Images/8faa047e5297fdab11c561949736712d.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8faa047e5297fdab11c561949736712d.jpg)'
- en: We can place an identity matrix next to it, and perform row operations simultaneously
    on both. Here we will first subtract 5 times the first row from the second row,
    then divide the second row by -9 then subtract three times the second from the
    first
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在其旁边放置一个单位矩阵，并同时对两者进行行操作。这里我们首先从第二行减去第一行的5倍，然后将第二行除以-9，然后从第一行减去第二行的三倍。
- en: '![](../Images/bdef25fe56047e22f4bf1f4d71213e45.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdef25fe56047e22f4bf1f4d71213e45.jpg)'
- en: '![](../Images/458e404765f996bbacb1fa9a265145e1.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/458e404765f996bbacb1fa9a265145e1.jpg)'
- en: and the last matrix here is the inverse, A^(-1) of our original matrix A.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个矩阵是我们原始矩阵A的逆矩阵。
- en: Notice that the **rows of A and the columns of A^(-1) have dot products either
    1 or 0 with one another, and the same statement holds with rows of A^(-1) and
    columns of A.** This is of course the defining property of being inverses.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意**A的行和A^(-1)的列彼此之间的点积要么为1，要么为0，A^(-1)的行和A的列也是如此。** 这当然是逆矩阵的定义特性。
- en: '**Exercise 32.3 Find the inverse to the matrix B whose rows are first (2 4);
    second (1 3).** [Solution](exercise03.html)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 32.3 找到矩阵B的逆矩阵，其行为(2 4)；第二行为(1 3)。** [解答](exercise03.html)'
- en: The inverse of a matrix can be useful for solving equations, when you need to
    solve the same equations with different right hand sides. It is overkill if you
    only want to solve the equations once.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的逆矩阵在解方程时非常有用，当你需要用不同的右侧解相同的方程时。如果你只想解方程一次，那么这就有点大材小用了。
- en: If your original equations had the form M**v** = **r**, by multiplying both
    sides by M^(-1) you obtain **v** = I**v** = M^(-1)M**v** = M^(-1)**r**, so you
    need only multiply the inverse, M^(-1) of M by your right hand side, **r**, to
    obtain a solution of your equations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的原始方程形式为M**v** = **r**，通过将两边乘以M^(-1)，你会得到**v** = I**v** = M^(-1)M**v** =
    M^(-1)**r**，因此你只需将逆矩阵M^(-1)乘以右侧的**r**，就可以得到方程的解。
- en: If you think of what you do here to compute the inverse matrix, and realize
    that in the process the different columns of M^(-1) do not interact with one another
    at all, you are **essentially solving the inhomogeneous equation Mv = r for r
    given by each of the three columns of the identity matrix,** and **arranging the
    results next to each other.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑在这里计算逆矩阵时所做的事情，并意识到在这个过程中M^(-1)的不同列根本不相互作用，你**本质上是在解非齐次方程Mv = r，其中r由单位矩阵的三列给出**，**并将结果排列在一起。**
- en: What we are saying here then is that to solve the equations for general **r**
    it is sufficient to **solve it for each of the columns of I, and then the solution
    for a general linear combination r of these columns is the same linear combination
    of the corresponding solutions.**
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '**What matrices have inverses?**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Not every matrix has an inverse.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, when the rows of M are **linearly dependent,** the equations
    that M defines do not have unique solutions, which means that for some right hand
    sides there are lots of solutions and for some there are none. If so the **matrix
    M does not have an inverse.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: One way to characterize the **linear dependence** of the rows (or columns, if
    the rows are linearly dependent and the matrix is square, then the columns are
    linearly dependent as well) in three dimensions is that the volume of the parallelepiped
    formed by the rows (or columns) of M is zero.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The volume of the parallelepiped formed by the rows of M is not changed under
    the second kind of row operation, adding a multiple of a row to another, though
    it is changes by a factor |c| if you multiply each element of a row by c.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that **volume is always positive** so that the absolute value |c|
    appears here is a bit awkward, and so it is customary to define a quantity that
    **when positive is this volume but has the property of linearity: if you multiply
    a column by c it changes by a factor of c rather than by |c|.** This quantity
    (and an analogue holds in any dimension) is called **the determinant of M.**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus the absolute value of the determinant of M is:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: In **one dimension the absolute value of the single matrix element of M.**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In **two dimensions the area of the parallelogram with sides given by the rows
    (or if you prefer, columns) of M.**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In **three dimensions the volume of the parallelepiped with sides given by the
    rows (or alternately the columns) of M.**
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: In **higher dimensions the "hypervolume" or higher dimensional analogue of volume
    of the region with sides given by the rows (or columns) of M.**
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: In **0 dimensions we give whatever it is the value 1.**
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: And the determinant of M in any dimension is **linear in each of its rows or
    columns and is unchanged upon replacing one row , say q, by the sum of q and any
    multiple of any other row.**
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: These statements specify the determinant up to a sign. **The sign is determined
    by convention to be positive for the identity matrix I whose determinant is always
    1.**
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition that M has an inverse is: **the determinant of M is not zero.**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: We will soon see how to calculate determinants, and how to express the inverse
    of a matrix in terms of its determinant.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 32.4 More on Determinants
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have defined **the determinant of a matrix to be a linear function of its
    rows or columns whose magnitude is the hypervolume of the region with edges given
    by its columns, or by its rows.**
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The determinant has a number of important properties as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: We will list them then offer proofs of them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 1\. **Linearity in columns:** if we have column n-vectors c(k) and d(k), for
    k = 1 to n, and pick any j in this range then the n dimensional determinant obeys
    the condition
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: det (c(1), …c(j - 1), **a*c(j) + b*d(j)**, c(j + 1),…,c(n)) = **a***det (c(1),
    …c(j - 1), **c(j)**, c(j + 1),...,c(n)) + **b***det (c(1), …c(j-1), **d(j)**,
    c(j + 1),...,c(n)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Linearity in rows:** write this one out yourself.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **The determinant is 0 if two columns are the same.** (Likewise for rows.)
    Equivalently, it changes sign if you interchange two rows (or columns).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '**4.** The determinant can be evaluated by a process **like row reduction.**
    You can add multiples of rows to one another until all elements on one side of
    the main diagonal are 0.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Then the product of the diagonal elements is the determinant.**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 5\. **The determinant of the matrix product of two matrices is the product of
    their determinants.**
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 6\. In terms of the elements of a matrix M in any one column say M[1j], M[2j],
    ...
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The determinant can be expressed as
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '**det M = M[1j]*C(1, j) + M[2j]*C(2, j) + ...**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The quantities **C(i, j)** that occur here are called **co-factors** of the
    **matrix M.**
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**C(i, j)** must be **linear in all the rows of M except the i-th and in all
    the columns of M except the j-th, and it must be 0 if two of those rows or columns
    are the same;** so it is **proportional to the determinant of the matrix obtained
    by removing the i-th row and j-th column from M. The proportionality constant
    turns out to be (-1)^(i+j).**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. The inverse of the matrix M is the matrix whose (i, j)-th element is
    ![](../Images/d3936880283c27a9cd884203d9ca53fe.jpg).**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 8\. If you have a set of equations of the form M**v** = **c**, then the i-th
    component of **v** is given by **the ratio of the determinant of the matrix obtained
    by taking M and substituting c for the i-th column of M, divided by the determinant
    of M itself.** (This statement is called Cramer's Rule.)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 9\. The condition that the determinant of a matrix is 0 means that the **hyper-volume
    of the region determined by the columns is 0** which means that **they are linearly
    dependent,** and it means that **there is a non-zero linear combination of the
    columns that is the zero vector.** Which means that for this vector **v**, we
    have M**v** = **0**.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 10\. The determinant is unchanged by rotations of coordinates.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 11\. The polynomial of degree n in x defined by **det(M - xI)** is called **the
    characteristic polynomial of M.** Its roots (solutions to it = 0) are called **the
    eigenvalues** of M.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: We now comment on these claims.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: The first three follow immediately from the definition of the determinant as
    a linear version of hyper-volume.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'It follows from these that you can add a multiple of one row to another without
    changing the determinant: because by linearity the change would have to be a multiple
    of the determinant of a matrix with two identical rows.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: But then you can do this until the matrix is diagonal, at which point the determinant,
    again by linearity, is the product of the diagonal elements times the determinant
    of the identity matrix (which is 1).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可以一直这样做，直到矩阵是对角线矩阵，此时行列式，再次通过线性性，是对角线元素的乘积乘以单位矩阵的行列式（为1）。
- en: 'The statement **that the determinant of a product of two matrices is the product
    of the determinants** is important and useful. It follows by these two observations:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 两个矩阵乘积的行列式是行列式乘积的陈述**是重要且有用的。这可以通过以下两点观察得出：
- en: 1\. If the **matrix A is diagonal,** then det A is the product of the diagonal
    elements of A.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如果**矩阵 A 是对角线矩阵**，那么 det A 是 A 的对角线元素的乘积。
- en: On the other hand, the rows **of AB are just the rows of B each multiplied by
    the corresponding diagonal element of A.**
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，AB 的行**只是 B 的行，每个都乘以 A 的对角线元素。**
- en: By linearity then, the **determinant of AB is the product of the diagonal elements
    of A times the determinant of B,** that is, it is **the product of the determinant
    of A and that of B,** as we have claimed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过线性性，**AB 的行列式是 A 的对角线元素的乘积乘以 B 的行列式，也就是 A 的行列式和 B 的行列式的乘积，**正如我们所声称的。
- en: 2\. If we **apply a row operation (no multiplying rows by constants allowed)
    as discussed in property 4 above, on A to obtain a new matrix A' and apply the
    same row operation to (AB) to obtain (AB)' we will have**
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 如果我们**对 A 应用一个行操作（不允许将行乘以常数）如上述属性 4 中讨论的，得到一个新矩阵 A'，并对 (AB) 应用相同的行操作得到 (AB)'，我们将有**
- en: (A'B) = (AB)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (A'B) = (AB)'
- en: and we will have det A = det A' , and det AB = det A'B.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将有 det A = det A'，以及 det AB = det A'B。
- en: 'We can do this until A is diagonal, at which point we can use the first statement
    here to tell us: (det A'') * (det B) = det A''B, from which our conclusion follows.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以一直这样做，直到 A 是对角线矩阵，此时我们可以使用这里的第一个陈述告诉我们：(det A') * (det B) = det A'B，从而得出我们的结论。
- en: The statements about cofactors merely make explicit what it means to be linear
    in each row and column.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于余子式的陈述只是明确了在每行和每列中线性的含义。
- en: The sign factor can be deduced from the fact that it is 1 if you consider the
    first row and column, (think of the identity matrix) and you can switch rows and
    columns with their neighbors i - 1 and j - 1 times to rearrange things so that
    the i-th row and j-th column become the first and everything else is in their
    original orders.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 符号因子可以从这样一个事实中推导出来，即如果你考虑第一行和第一列，（想想单位矩阵）你可以交换行和列与它们的邻居 i - 1 和 j - 1 次，重新排列事物，使得第
    i 行和第 j 列成为第一行，其他所有行列保持原始顺序。
- en: This will cause i + j - 2 sign changes, which gives the sign factor noted.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致 i + j - 2 个符号变化，这给出了所述的符号因子。
- en: As already noted somewhere the **cofactor formula for the inverse** is a statement
    about the dot products of the rows of the inverse with the columns of the original
    matrix. The diagonal products must be 1 which follows for **![](../Images/d3936880283c27a9cd884203d9ca53fe.jpg)**
    from the cofactor formula for the determinant, and the off diagonal ones must
    be zero because by that same formula they represent the determinants of matrices
    with two identical columns or rows.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**逆的余子式公式**是关于逆的行与原始矩阵的列的点积的陈述。对角线乘积必须为 1，这可以从行列式的余子式公式中得出，而非对角线乘积必须为零，因为根据相同的公式，它们代表具有两个相同列或行的矩阵的行列式。
- en: '**Cramer''s rule** is the observation that by the **definition of the inverse,**
    the desired coefficient is **the dot product of the i-th row of the inverse of
    M with the vector c.** But by the cofactor formula this is **the dot product of
    the i-th column of the cofactor matrix with the vector c, divided by the determinant
    of M,** and that is the ratio of the two determinants of Cramer''s rule.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**克莱姆法则**是观察到，根据**逆的定义，**所需系数是**矩阵 M 的逆的第 i 行与向量 c 的点积。**但根据余子式公式，这是**余子式矩阵的第
    i 列与向量 c 的点积，除以 M 的行列式，**这就是克莱姆法则的两个行列式的比率。'
- en: 32.5 Matrices and Transformations
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 32.5 矩阵和变换
- en: '**The most important use of matrices lies in representing linear transformations
    on a vector space.**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵最重要的用途在于表示向量空间上的线性变换。**'
- en: '**How?**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何？**'
- en: A matrix represents the tranformation which takes **the first basis vector into
    first column of the matrix, second basis vector into the second column of the
    matrix, j-th basis vector into j-th column.**
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩阵表示了将**第一个基向量转换为矩阵的第一列，第二个基向量转换为矩阵的第二列，第 j 个基向量转换为矩阵的第 j 列。**
- en: '**What does it do to other vectors?**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that any other vector, say **v** can be expressed as **a linear combination
    of the basis vectors: v** gets transformed by the transformation to **that same
    linear combination of the column vectors of the matrix.**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: For example, the sum of the first two basis vectors gets mapped into the sum
    of the first two columns of the matrix; the average of the two basis vectors into
    the average of the columns, and so on.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '**Notice that the same transformation acting on vectors will usually be described
    by a different matrix if you use a different basis.**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[Example](example01.html)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 32.6 Invariants of Transformations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the same transformation can often be represented by many different matrices,
    depending upon the basis chosen to describe them, the following questions can
    be raised:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '**What properties of a matrix are the same independent of the basis, being
    intrinsic properties of the transformation the matrices represent?**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '**When do two matrices represent the same transformation with different bases?**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: There are actually several questions that can be raised for each of these, because
    we can be describing matrices whose elements are **all real, or we can permit
    complex elements,** and we can insist that we stick to **bases that are orthonormal
    (the dot product of any basis vector with itself is 1 and its dot product with
    any other basis vector is 0) or allow more general bases including those with
    complex components.**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The answers are a bit different depending on which context we consider, but
    they are fundamentally similar.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: We will here consider **real matrices and real orthonormal bases** only.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: A matrix which takes our original basis vectors into another orthonormal set
    of basis vectors is called an **orthogonal matrix;** its columns must be **mutually
    orthogonal and have dot products 1 with themselves, since these columns must form
    an orthonormal basis.**
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: These conditions mean that an orthogonal matrix has its [transpose](#Transpose)
    as its inverse! **(The condition for two matrices to be inverses of one another
    is that the rows of one are orthogonal to the columns of the other, except that
    rows and columns with the same index have dot product one with one another.)**
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question we address is: **what happens to a matrix M when an orthogonal
    transformation A is applied to the original basis vectors?**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: A transforms the initial basis to A's columns. We want to know what the matrix
    M does to these column vectors. That is **exactly what the matrix MA** does to
    the original column basis vectors. **A takes them into the new basis vectors and
    M then transforms these into whatever it does to them.**
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: However the product MA expresses what M does to the new basis vectors **in terms
    of the old ones; its columns give the effect of M on the new basis vectors as
    linear combinations of the old basis vectors.**
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: We want to re-express these columns as linear combinations of the new basis
    vectors.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we do this?**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to see is to **look at what happens when M is the identity matrix,
    I. This is the matrix which takes any vector into itself. After the change of
    basis, it must still take any vector into itself, so it must still be the identity
    matrix.**
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: But if M = I then MA is just IA or A itself and that is what I becomes in terms
    of the old basis That is columns of A tell what the new basis vectors look like
    in terms of the old ones.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: To re-express I in terms of the new basis you must do something which takes
    AI back into I. **The way to do this is to multiply on the left by A^(-1) which
    is A^T.**
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We deduce that multiplying on the left by A^(-1) performs the desired re-expression
    **for I and therefore for any matrix M.** We conclude that in the new basis the
    matrix M becomes **A^TMA.**
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**The transpose of a matrix is the matrix obtained by interchanging its rows
    and columns.**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '**A matrix is symmetric if it the same after such a transformation.**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We have just seen that **an orthogonal transformation** changes a matrix M to
    one of the form A^TMA, where A^TA = I, and the **matrix A has columns given by
    the new orthonormal basis expressed in terms of the old basis.**
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A nice feature of such a transformation is that **if M is symmetric, it stays
    symmetric after any such transformation.**
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.4 Prove this claim: that M is symmetric if and only if A^TMA
    is symmetric.**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that the only matrices that can possibly be made diagonal by such
    transformations are **symmetric;** since **when they are diagonal they are trivially
    symmetric.**
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**If a matrix is diagonal, its eigenvectors are the basis vectors. So we have
    shown that only symmetric matrices have real orthonormal bases.**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **any matrix that is symmetric can be made diagonal by an
    orthogonal transformation.** Another way to say this is there is an orthonormal
    basis of real eigenvectors for every symmetric matrix. [Proof of this claim](proof01.html)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'We have answered our first question: **which matrices can be put in diagonal
    form by choosing a new orthonormal basis? The answer being any symmetric matrix.**
    And the way to put a matrix in such form is to find its eigenvectors and choose
    an orthonormal set of these.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second question was: **when will two such matrices be representations of
    the same transformation in a different basis**. And the answer is, when their
    characteristic equations are the same, so that their eigenvalues are the same
    and have the same multiplicities.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 32.7 Other Notions of Diagonalizability
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have noted that our first question has a number of variants, and we will
    note the changes in the answers when the variants are used.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: When we allow complex matrix elements, and complex vectors, we can diagonalize
    a wider class of matrices.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: When a vector has complex valued entries, we still want to interpret its length
    as the square root of its dot product with itself. **We want this to be positive.**
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '**Therefore we redefine the dot product to make this so: the dot product of
    a complex vector with itself is the sum of the absolute value squared of its entries.**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We generalize this to the dot product of a row vector and a column vector by
    making it **the sum of the products of the complex conjugate of the component
    of the row vector with the corresponding component of the column vector.**
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Thus the dot product of the column vector with entries (a + ib, c + id) with
    the same row vector is
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: (a - ib) * (a + ib) + (c - id) * (c + id)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: or
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: a² + b² + c² + d²
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The dot product of the same column vector with (e + if, g + ih) is instead
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: (e - if) * (a + ib) + (g - ih) * (c + id)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Notice that with this definition the dot product is no longer symmetric. However
    it does not change if you interchange row and column and also take the complex
    conjugate, since the asymmetry lies in taking the complex conjugate of the row
    and not the column.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: With complex vectors we define an **orthonormal basis** to be one for which
    **the dot product of each column with the complex conjugate of the entries in
    the other columns are zero.**
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: This means that with this definition, a matrix that takes a given basis into
    another **orthonormal basis in this context has the property that its complex
    conjugate transpose is its inverse.**
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Such a matrix is called a **unitary matrix,** and **the linear transformation
    which takes one orthonormal complex basis to another is called a unitary transformation.**
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The effect of a unitary transformation described by the unitary matrix U on
    a matrix M is now U^t * MU as can be shown by the same argument as before. (Of
    course real unitary matrices are orthogonal.)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '**Again we can ask, what matrices can be diagonalized by a unitary transformation?**
    A preliminary question is: **which matrices can be diagonalized so that its eigenvalues,
    which are what appear on the diagonal when it is diagonalized, are all real?**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer now is that any matrix that **is its own transpose complex conjugate**
    will have this property: which implies if M is n by n, **M has n real eigenvalues
    and an orthonormal basis of eigenvectors.**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Such matrices are called Hermitian matrices.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Again the necessity of this condition follows from the fact that **"Hermitivity"
    is preserved by unitary transformations and real diagonal matrices are Hermitian.**
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '**Hermitian matrices are of particular importance** because they have the possibility
    of representing measurable real observables in physical systems. They do so in
    quantum mechanics.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Answer to the general question, **without reference to real eigenvalues is that
    the matrix must commute with its complex conjugate transpose.**
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: This condition is again preserved under unitary transformations, and it is a
    property of diagonal matrices, since all diagonal matrices commute with one another,
    so it is definitely necessary.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Still another question is, **when can a matrix be diagonalized by any change
    of basis, without any requirement about orthonormality; that is when does there
    exist any kind of a basis of eigenvectors for the matrix M?**
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: There is an easy answer which again can easily be seen to be necessary. Suppose
    a[1], a[2], ..., a[k] are the **distinct eigenvalues of M.**
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Any vector can be written as a sum of basis vectors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: If each basis vector is an eigenvector of M, say corresponding to eigenvalue
    a[j], then M - a[j]I acting on it will be the zero vector.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand M - a[h]I for a[h] different from a[j], acting on it merely
    multiplies it by a[j] - a[h].
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if there is a basis consisting of eigenvectors of M then the **product
    over all j from 1 to k of (M - a[j]I)** must be the zero matrix, **since it must
    give 0 in acting on every basis vector.**
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: This product is called the **minimal polynomial of M** and the equation that
    it is the zero matrix is called the minimal equation for M. Thus **if M obeys
    its own minimal equation then it has a basis of eigenvectors.**
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: By the way an interesting and curious fact is that **every matrix obeys its
    own characteristic equation** (that is if you substitute M for the variable x
    in it, you get the 0 matrix).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 32.8 Computing Eigenvalues and Eigenvectors
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We here address the following questions:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 1\. How can we actually compute eigenvalues and eigenvectors of a given matrix?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 2\. How can we apply what we know about matrices to quadratic functions (also
    called quadratic forms)? And to critical points and saddle points.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '3\. We describe an eigenvector game: learn how to just look at a matrix and
    guess an eigenvector!'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: How to compute eigenvalues and eigenvectors for large matrices is an important
    question in numerical analysis. We will merely scratch the surface for small matrices.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an obvious way to look for real eigenvalues of a real matrix: you
    need only write out its characteristic polynomial, plot it and find its solutions.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: This is quite easy to do in two dimensions, not difficult in three or four dimensions,
    and not really difficult for a computer in many more dimensions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: This is very straightforward and dull.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions the characteristic equation is
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: x² - tr(M)x + det(M) = 0
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: This equation can be solved using the quadratic formula and the eigenvalues
    can be obtained by explicit formulae.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In three dimensions the characteristic equation is
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: x³ - tr(M)x² + Ax - det(M) = 0
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: where A is the sum of pairs of diagonal elements minus the products of each
    opposite pair of off diagonal elements
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: A = M[11] * M[22] + M[11] * M[33] + M[22] * M[33] - M[12] * M[21] - M[13] *
    M[31] - M[23] * M[32]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: There is a cubic formula for solving this equation but it is probably easier
    to find one solution, say z, numerically, whereupon the other two obey the quadratic
    equation
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcdf31ff51d8ba43fca4aae2411d804a.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: Since the characteristic polynomial is cubic, it goes in opposite directions
    for large arguments positive versus negative, and so by starting at same and homing
    in (by the divide and conquer approach) you can find a solution to any desired
    accuracy with relative ease.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: So how do we find an eigenvector given an eigenvalue z? There is a very simple
    answer that usually works. A column eigenvector can be obtained by taking the
    cofactors of any row of M - zI and arranging them as a column vector.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises:**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '**32.5 When will this approach fail?**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**32.6 Prove that if the cofactors don''t all vanish they provide a column
    eigenvector.**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '**32.7 Choose a random 3 by 3 matrix and find an eigenvalue and corresponding
    eigenvector.**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to find eigenvectors and eigenvalues that often work.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: One approach is to raise the matrix to a high power. This is easier to do than
    it sounds.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: You can then notice that the high power of the matrix will tend to have rank
    1, usually, and you can read off a row and a column eigenvector from it.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: You can easily deduce the corresponding eigenvalue by having the matrix act
    on the eigenvector you find.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: If there is an eigenvalue that has greater magnitude than any other and it has
    only one eigenvector, (it is not a multiple root of the characteristic equation
    for M) then this method will usually find it.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: You can apply the same approach to the inverse matrix to M to find an eigenvalue
    smallest in magnitude and its eigenvector.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: These actions are relatively easy on Excel spreadsheets, because these have
    functions that take the product of two matrices (called mmult), which finds the
    inverse of a matrix (minverse) and takes the determinant of a matrix (mdeterm).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Using mmult it is quite easy to square a matrix, copying the procedure to raise
    it to the fourth power, copy both procedures to raise it to the eighth and then
    sixteenth power; copy the whole mess to raise to the 256^(th) power etc.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: For a four by four matrix once you have two eigenvalues, then you can get the
    rest by solving quadratics and you can usually get the largest and smallest in
    magnitude by raising A and A^(-1) to high powers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Of course once you have the eigenvalue that is largest in magnitude, you could
    look for the second largest. This can be accomplished by projecting the columns
    of M to vectors normal to the first row eigenvector, and working with the resulting
    matrix.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Of course you run into trouble with this approach if there are two eigenvectors
    with the same largest magnitude of eigenvalue or nearly the same one.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: How do we find the matrix A which we can use to diagonalize M?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: A's columns are normalized eigenvectors of M.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: <applet code="LinearTransformations3D" codebase="../applets/" archive="linearTransformations3D.jar,go.jar,goText.jar,mk_lib.jar,parser_math.jar,jcbwt363.jar,jama.jar"
    width="760" height="450"></applet>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 32.9 Application to Quadratic Forms and Spring Systems
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another place in which matrices have appeared in previous chapters was in the
    discussion of the behavior of functions of several variables at a critical point
    (at which the gradient of the function is the **0** vector).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: We then noticed that the behavior of the function could be described by the
    matrix of second derivatives of the function at that point. This is the matrix
    whose ij-th element is the second partial derivative of the function with respect
    to the i-th and j-th variable.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric matrices each have a an orthonormal basis of real eigenvectors as
    we proved above, obtainable by an orthogonal transformation.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'If we examine the structure of the matrix for the form using this basis, we
    find that it is diagonal, and so the conditions for an extremum become simple:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '**If all the eigenvalues of the second derivative matrix have the same sign
    the function has a local maximum or minimum, with a minimum when they are all
    positive.**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: There is a saddle if the signs are mixed, and you must sometimes look at higher
    derivatives when some of the eigenvalues are 0's.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: When talking about the matrix of second derivatives we are really talking about
    the quadratic form which describes the quadratic terms in the Taylor series expansion
    of our function about the critical point.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: If we focus on quadratic forms we realize that we can use a wider class of transformations
    in order to change their appearance than we can when dealing with transformations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Thus we can make changes of scale of the individual variables to make any positive
    diagonal quadratic into one that has all (non-zero) eigenvalues the same. (Thus
    we can change ![](../Images/4638051d3cef96abffe3c6cb680c6c6a.jpg) by setting ![](../Images/060ade7be3680b8729c5429cb2474691.jpg))
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to diagonalize two distinct quadratic forms simultaneously. You
    can make the matrix of one into an identity matrix, and then diagonalize the other.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: This is in contrast to what happens with transformations. Two transformations
    must commute to be simultaneously diagonal with the same basis (obviously necessary
    since all diagonal matrices commute with one another).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '(This statement is proven as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Diagonalize the matrix M. You can then observe that the condition that M and
    N commute is the condition that all the off diagonal elements of N say the ij-th
    link indices whose diagonal elements of M are the same.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Thus if the ij-th entry of N is non-zero then the i-th and j-th eigenvalues
    of M must be the same if N and M are to commute.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: If they are the same then as far as diagonalizing N is concerned, the problem
    breaks up into parts for each eigenvalue of M; and for each part M is a multiple
    of the identity matrix and will stay diagonal when the corresponding block of
    N is diagonalized.)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Given a system of springs and masses, there will be one quadratic form that
    represents the kinetic energy of the system in terms of momentum variables, and
    another which represents the potential energy of the system in position variables.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The remarks above tell us that it is always possible to choose a normalization
    and basis of coordinates so that both of these forms are diagonal. This means
    that the entire system can be analyzed as a bunch of independent simple one dimensional
    springs (each of which can represent a complex combination of original coordinates).
    The corresponding eigenvalues determine the "normal modes" of the system.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 32.10 Computing Eigenvalues and Eigenvectors on a Spreadsheet
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can build a spreadsheet that will find same for any 3 by 3 matrix that has
    three real eigenvalues, as follows. It is very worthwhile for you to attempt to
    do this.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: First find the trace determinant and second invariant (A) of the given matrix.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: How?
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The trace is easy, the determinant is a single command in excel.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the second invariant in excel is also easy: extend the matrix by making
    a fourth row and column which are the same as the first ones, and make the 44
    entry the same as the 11 entry.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Then find the two by two diagonal matrices in columns and rows 1, 2 in columns
    and rows 2, 3 and in columns and rows 3, 4\. The sum of these three will be A.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Then solve the characteristic equation. This can be done by starting with very
    large values say +1000 and -1000 and homing in on a solution using the divide
    and conquer approach.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Then find the other two eigenvalues by solving the quadratic equation previously
    described.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: They will not always exist, since the roots of the quadratic could be complex;
    if so change your matrix to make them real.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: It is barely possible that your matrix is not diagonalizable, in which case
    it does not have three eigenvectors, but this can only happen if two of the eigenvalues
    are the same.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Now find eigenvectors.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: How?
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a good try: write down the matrix M - zI where z is one of your eigenvalues.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Extend the matrix to a fourth and fifth column by copying the first column to
    the fourth and second to fifth.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Then take the two by two determinants given by the first two rows and columns
    23, 34 and 45\. Arrange these as a column. This should be your eigenvector.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: It could be that these two by two determinants are all 0\. If so you can try
    again with the second and third rows and you could even copy the first row to
    a fourth row and do the same for the third and fourth rows.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: If you always fail that means that you had a double eigenvalue (at least two
    of your three eigenvalues are the same). Eigenvectors are actually easier to find
    in this case, when they exist.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: If all the eigenvalues are the same then M was a multiple of the identity, and
    every vector is an eigenvector.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise you can find a column eigenvector for that eigenvalue as described,
    and find a row eigenvector by doing the same thing interchanging rows and columns.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Then the column eigenvectors for your double eigenvalue will be any vectors
    normal to your row eigenvector for the other eigenvalue.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Once you have three column eigenvectors, you can form them into a matrix, A
    and examine A^(-1) and A^(-1)MA, which should come out to be diagonal. (These
    are very easy to find in excel using the mmult and minverse functions.)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 32.11 Guessing Eigenvectors
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is a game you can set up on a spreadsheet. Enter an arbitrary matrix M
    somewhere.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Three by three is a good way to start.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Enter a 3 component column vector **v** and use the mmult command (or do it
    out yourself) to compute M**v** and for each component of **v** compute the ratio
    of ![](../Images/22cc4f958539c007ba1cb9778692ae92.jpg)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the variance of these ratios (that is, the sum of their squares minus
    the square of their sum).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: The players can take turns generating the original M and **v**; then they take
    turns modifying **v** by changing **one** of its components.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: If the variance of the ratios decreases the player scores a point, otherwise
    loses one. The game ends when the variance becomes negligible, say less than 10^(-10).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The ratios then will be more or less the same and hence the eigenvalue associated
    with the eigenvector produced.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: If you get too good at this, you can try with a 5 by 5 matrix, though it is
    boring to enter one at the start.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
