- en: 'Chapter 32: Some Linear Algebra'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter contains a review of the basics of linear algebra: the solution
    of linear equations, matrix inversion, determinants, transformations, invariants,
    eigenvalues, and diagonalizability.'
  prefs: []
  type: TYPE_NORMAL
- en: Topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 32.1   [Linear Equations](section01.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.2   [Matrices](section02.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.3   [The Inverse of a Matrix](section03.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.4   [More on Determinants](section04.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.5   [Matrices and Transformations](section05.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.6   [Invariants of Transformations](section06.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.7   [Other Notions of Diagonalizability](section07.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.8   [Computing Eigenvalues and Eigenvectors](section08.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.9   [Application to Quadratic Forms and Spring Systems](section09.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.10  [Computing Eigenvalues and Eigenvectors on a Spreadsheet](section10.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.11  [Guessing Eigenvectors](section11.html)
  prefs: []
  type: TYPE_NORMAL
- en: 32.1 Linear Equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a set of linear equations, **for example**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/154e9b105ff24f2f645d842e2d91ff0b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and we wish to **find a solution,** by which we mean to find the **explicit
    values of x, y and z which make these equations all true.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental facts that allows us to find solutions are these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Given any equation, you can multiply it (that is, multiply every term
    in it, both on the right and on the left) by any non-zero number without changing
    its implications.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Given any two equations we define their sum to be the equation whose
    left hand side is the sum of the two left hand sides, and whose right hand side
    is the sum of the two right hand sides.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Then you can replace either of the two equations by its sum with any multiple
    of the other without changing their implications.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: you can replace the top equation above by 3x + 4y = 6 by subtracting
    the third equation from it; (subtracting the equation is the same as adding -1
    multiplied by it)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.1 Prove claim 2 here.** [Solution](exercise01.html)'
  prefs: []
  type: TYPE_NORMAL
- en: You can solve the equations by using a sequence of manipulations of the kind
    just mentioned that put the equations into the form x = a, y = b, z = c, which
    is the solution to them.
  prefs: []
  type: TYPE_NORMAL
- en: '**What sequence of manipulations should you use to solve equations?**'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the subtraction made in the example above was chosen so that z does
    not appear in the subtracted equation, which was 3x + 4y = 6.
  prefs: []
  type: TYPE_NORMAL
- en: If we make an addition of an appropriate multiple of the third equation to the
    second one, again the z term in the resulting sum equation can be eliminated similarly;
    with result x - 3y = 6.
  prefs: []
  type: TYPE_NORMAL
- en: We started with three equations in three variables.
  prefs: []
  type: TYPE_NORMAL
- en: After these operations we have eliminated z from two of them and have two equations
    in two variables.
  prefs: []
  type: TYPE_NORMAL
- en: By a similar manipulation we can eliminate x, for example, by subtracting three
    times the second one from the first. The resulting equation is then 13y = -12.
  prefs: []
  type: TYPE_NORMAL
- en: Dividing this equation by 13 then gives us an expression for y.
  prefs: []
  type: TYPE_NORMAL
- en: We can substitute it for y into either of the previous two equations and solve
    the resulting equation for x.
  prefs: []
  type: TYPE_NORMAL
- en: We get ![](../Images/62f374eaec1326302efb8b511a20e170.jpg) Substituting both
    for x and y in any of the original equations then gives ![](../Images/363c1bfab175102aef42b86dc5d900af.jpg)
    and we have a complete solution to our equations.
  prefs: []
  type: TYPE_NORMAL
- en: '**In general you can systematically eliminate one variable at a time from all
    equations, reducing n equations in n unknowns to (n - 1) equations in (n-1) unknowns,
    and repeat the process until you can solve one equation for one unknown, and then
    substitute back to find the others, one at a time.**'
  prefs: []
  type: TYPE_NORMAL
- en: This procedure is called **"Gaussian elimination".**
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.2 Perform Gaussian elimination on the following set of equations
    to find a solution**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d2893842deb50cbaeaaddcde36ce38b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Solution](exercise02.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Please notice that in doing these operations it is very easy to make a mistake,
    and it is wise to check your answer, once you have it, in **all** the original
    equations to see if they are satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can this procedure fail?**'
  prefs: []
  type: TYPE_NORMAL
- en: If the equations that you start with are consistent, they will produce a unique
    solution **unless** when you try to eliminate one variable by subtracting a multiple
    of an equation from another, you eliminate the entire equation.
  prefs: []
  type: TYPE_NORMAL
- en: That is, at some stage one of your equations is a multiple of another and subtracting
    that multiple from it eliminates the entire equation.
  prefs: []
  type: TYPE_NORMAL
- en: This will happen if in the beginning one of your equations can be expressed
    as a sum of multiples of one or more of the others. (The simplest way this happens
    is when two of the equations are identical)
  prefs: []
  type: TYPE_NORMAL
- en: In this case the left hand sides of your equations are said to be **linearly
    dependent.**
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, when the equations have a unique solution, the left hand sides are
    said to be **linearly independent.**
  prefs: []
  type: TYPE_NORMAL
- en: When your equations are linearly dependent, (and you started with the same number
    of equations as you had unknowns,) you will find that you do not have enough equations
    to determine a unique solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**This is not a disaster,** but it means that there are lots of solutions,
    at least a whole line of them.'
  prefs: []
  type: TYPE_NORMAL
- en: You can continue the Gaussian elimination process until you are down to one
    non-vanishing equation in now two or more variables. Then **any** solution to
    that equation is a solution to the original set of equations, which is said to
    be **an underdetermined set of equations.**
  prefs: []
  type: TYPE_NORMAL
- en: Suppose for example, your last equation with all other unknowns eliminated is
    x = 2y + 3.
  prefs: []
  type: TYPE_NORMAL
- en: Then you can choose any value you please for y, compute x and then go on to
    use your other equations to compute your other unknowns, and that will be a solution,
    though of course not the only possible one. **Solutions to an equation like this
    one form a line in the xy plane.**
  prefs: []
  type: TYPE_NORMAL
- en: 32.2 Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrices provide a convenient way to describe linear equations. Thus if you
    take the coefficients of your unknowns, in some standard order, as the row elements
    of your matrix, you define **a matrix of coefficients for any set of equations.**
  prefs: []
  type: TYPE_NORMAL
- en: For the example equations above, the coefficient matrix, call it M, is, with
    the standard ordering of x, y and z
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7149e6da35a1ff32dea21d39e453e226.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can then write the original equations as the single matrix equation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/569ce160ba5a560d93f59db9050e035e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the definition of matrix multiplication, which is: **taking the dot products
    of the rows of the first matrix with the columns (here single column) of the second
    to produce the corresponding elements of the product,** you should verify that
    this matrix equation is exactly the same as our original three equations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of **Gaussian elimination** can be applied in this matrix form
    here. The rules are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. You can multiply an entire row (on both sides of the equation) by any
    non-zero number without changing the content of the equations.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. You can add a multiple of any row to another without changing the content
    of the equations. You must add entirely across the row, including the other side
    of the matrix, however.**'
  prefs: []
  type: TYPE_NORMAL
- en: In this form such operations are called **"elementary row operations"** and
    Gaussian elimination is called **row reduction.**
  prefs: []
  type: TYPE_NORMAL
- en: What you do here is perform enough of operation 2 to **form 0's in the matrix
    on one side of the main diagonal.** When this is done you can determine one unknown
    and then substitute successively to find the others.
  prefs: []
  type: TYPE_NORMAL
- en: You can also attempt to perform these operations **until all elements of your
    matrix off the main diagonal are 0's,** and the diagonal elements are 1\. In that
    case the right hand side vectors are the solutions for the corresponding variables
    and you need not substitute back to find all the unknowns.
  prefs: []
  type: TYPE_NORMAL
- en: '**The n dimensional matrix whose diagonal elements are 1 and off diagonal elements
    are 0** is called the **n dimensional identity matrix,** and is written as **I**
    usually without any indication of what its size is, unless that can cause confusion,
    in which case it is written as **I[n].**'
  prefs: []
  type: TYPE_NORMAL
- en: It has the property that its matrix product with any matrix M of the same dimension
    is M itself, and its operation on any n dimensional vector **v** is **v** itself.
  prefs: []
  type: TYPE_NORMAL
- en: Thus if you start with the matrix equation **Mv = r,** and row reduce to find
    another representation of the same set of equations for which M has been reduced
    to the identity matrix I, you have **Iv = r'** where **r' is the result of the
    same row operations on the right side of the equation as those that reduced M
    to I.**
  prefs: []
  type: TYPE_NORMAL
- en: You thereby obtain **the solution, v = r'.**
  prefs: []
  type: TYPE_NORMAL
- en: 32.3 The Inverse of a Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If two square matrices M and A have the property that **MA = I,** (in infinite
    dimensions you also need the condition that AM = I) then **A and M are said to
    be inverses of one another and we write A = M^(-1) and M= A^(-1).**
  prefs: []
  type: TYPE_NORMAL
- en: A wonderful feature of row reduction as we have described it is that when you
    have a matrix equation AB = C, **you can apply your reduction operations for the
    matrix A to the rows of A and C simultaneously and ignore B, and what you get
    will be as true as what** you started with.
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly what we did when B was the column vector with components equal
    to our unknowns, x, y and z, but it is equally true for any matrix B.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, suppose you start with the matrix equation **AA^(-1) = I.**
  prefs: []
  type: TYPE_NORMAL
- en: If we row reduce A so it becomes the identity matrix I, then the left hand side
    here becomes IA^(-1) which is A^(-1), the matrix inverse to A. The right hand
    side however is **what you obtain if you apply the row operations necessary to
    reduce A to the identity, starting with the identity matrix I.**
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that the inverse matrix, **A^(-1) can be obtained by applying
    the row reduction operations that make A into I starting with I.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** we give a two dimensional example, but the method and idea hold
    in any dimension, and computers are capable of doing this for n by n matrices
    when n is in the hundreds even thousands.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want the inverse of the following matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8faa047e5297fdab11c561949736712d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can place an identity matrix next to it, and perform row operations simultaneously
    on both. Here we will first subtract 5 times the first row from the second row,
    then divide the second row by -9 then subtract three times the second from the
    first
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdef25fe56047e22f4bf1f4d71213e45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/458e404765f996bbacb1fa9a265145e1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and the last matrix here is the inverse, A^(-1) of our original matrix A.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the **rows of A and the columns of A^(-1) have dot products either
    1 or 0 with one another, and the same statement holds with rows of A^(-1) and
    columns of A.** This is of course the defining property of being inverses.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.3 Find the inverse to the matrix B whose rows are first (2 4);
    second (1 3).** [Solution](exercise03.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The inverse of a matrix can be useful for solving equations, when you need to
    solve the same equations with different right hand sides. It is overkill if you
    only want to solve the equations once.
  prefs: []
  type: TYPE_NORMAL
- en: If your original equations had the form M**v** = **r**, by multiplying both
    sides by M^(-1) you obtain **v** = I**v** = M^(-1)M**v** = M^(-1)**r**, so you
    need only multiply the inverse, M^(-1) of M by your right hand side, **r**, to
    obtain a solution of your equations.
  prefs: []
  type: TYPE_NORMAL
- en: If you think of what you do here to compute the inverse matrix, and realize
    that in the process the different columns of M^(-1) do not interact with one another
    at all, you are **essentially solving the inhomogeneous equation Mv = r for r
    given by each of the three columns of the identity matrix,** and **arranging the
    results next to each other.**
  prefs: []
  type: TYPE_NORMAL
- en: What we are saying here then is that to solve the equations for general **r**
    it is sufficient to **solve it for each of the columns of I, and then the solution
    for a general linear combination r of these columns is the same linear combination
    of the corresponding solutions.**
  prefs: []
  type: TYPE_NORMAL
- en: '**What matrices have inverses?**'
  prefs: []
  type: TYPE_NORMAL
- en: Not every matrix has an inverse.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, when the rows of M are **linearly dependent,** the equations
    that M defines do not have unique solutions, which means that for some right hand
    sides there are lots of solutions and for some there are none. If so the **matrix
    M does not have an inverse.**
  prefs: []
  type: TYPE_NORMAL
- en: One way to characterize the **linear dependence** of the rows (or columns, if
    the rows are linearly dependent and the matrix is square, then the columns are
    linearly dependent as well) in three dimensions is that the volume of the parallelepiped
    formed by the rows (or columns) of M is zero.
  prefs: []
  type: TYPE_NORMAL
- en: The volume of the parallelepiped formed by the rows of M is not changed under
    the second kind of row operation, adding a multiple of a row to another, though
    it is changes by a factor |c| if you multiply each element of a row by c.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that **volume is always positive** so that the absolute value |c|
    appears here is a bit awkward, and so it is customary to define a quantity that
    **when positive is this volume but has the property of linearity: if you multiply
    a column by c it changes by a factor of c rather than by |c|.** This quantity
    (and an analogue holds in any dimension) is called **the determinant of M.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus the absolute value of the determinant of M is:'
  prefs: []
  type: TYPE_NORMAL
- en: In **one dimension the absolute value of the single matrix element of M.**
  prefs: []
  type: TYPE_NORMAL
- en: In **two dimensions the area of the parallelogram with sides given by the rows
    (or if you prefer, columns) of M.**
  prefs: []
  type: TYPE_NORMAL
- en: In **three dimensions the volume of the parallelepiped with sides given by the
    rows (or alternately the columns) of M.**
  prefs: []
  type: TYPE_NORMAL
- en: In **higher dimensions the "hypervolume" or higher dimensional analogue of volume
    of the region with sides given by the rows (or columns) of M.**
  prefs: []
  type: TYPE_NORMAL
- en: In **0 dimensions we give whatever it is the value 1.**
  prefs: []
  type: TYPE_NORMAL
- en: And the determinant of M in any dimension is **linear in each of its rows or
    columns and is unchanged upon replacing one row , say q, by the sum of q and any
    multiple of any other row.**
  prefs: []
  type: TYPE_NORMAL
- en: These statements specify the determinant up to a sign. **The sign is determined
    by convention to be positive for the identity matrix I whose determinant is always
    1.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition that M has an inverse is: **the determinant of M is not zero.**'
  prefs: []
  type: TYPE_NORMAL
- en: We will soon see how to calculate determinants, and how to express the inverse
    of a matrix in terms of its determinant.
  prefs: []
  type: TYPE_NORMAL
- en: 32.4 More on Determinants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have defined **the determinant of a matrix to be a linear function of its
    rows or columns whose magnitude is the hypervolume of the region with edges given
    by its columns, or by its rows.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The determinant has a number of important properties as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will list them then offer proofs of them.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. **Linearity in columns:** if we have column n-vectors c(k) and d(k), for
    k = 1 to n, and pick any j in this range then the n dimensional determinant obeys
    the condition
  prefs: []
  type: TYPE_NORMAL
- en: det (c(1), …c(j - 1), **a*c(j) + b*d(j)**, c(j + 1),…,c(n)) = **a***det (c(1),
    …c(j - 1), **c(j)**, c(j + 1),...,c(n)) + **b***det (c(1), …c(j-1), **d(j)**,
    c(j + 1),...,c(n)).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Linearity in rows:** write this one out yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **The determinant is 0 if two columns are the same.** (Likewise for rows.)
    Equivalently, it changes sign if you interchange two rows (or columns).
  prefs: []
  type: TYPE_NORMAL
- en: '**4.** The determinant can be evaluated by a process **like row reduction.**
    You can add multiples of rows to one another until all elements on one side of
    the main diagonal are 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Then the product of the diagonal elements is the determinant.**'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. **The determinant of the matrix product of two matrices is the product of
    their determinants.**
  prefs: []
  type: TYPE_NORMAL
- en: 6\. In terms of the elements of a matrix M in any one column say M[1j], M[2j],
    ...
  prefs: []
  type: TYPE_NORMAL
- en: The determinant can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '**det M = M[1j]*C(1, j) + M[2j]*C(2, j) + ...**'
  prefs: []
  type: TYPE_NORMAL
- en: The quantities **C(i, j)** that occur here are called **co-factors** of the
    **matrix M.**
  prefs: []
  type: TYPE_NORMAL
- en: '**C(i, j)** must be **linear in all the rows of M except the i-th and in all
    the columns of M except the j-th, and it must be 0 if two of those rows or columns
    are the same;** so it is **proportional to the determinant of the matrix obtained
    by removing the i-th row and j-th column from M. The proportionality constant
    turns out to be (-1)^(i+j).**'
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. The inverse of the matrix M is the matrix whose (i, j)-th element is
    ![](../Images/d3936880283c27a9cd884203d9ca53fe.jpg).**'
  prefs: []
  type: TYPE_NORMAL
- en: 8\. If you have a set of equations of the form M**v** = **c**, then the i-th
    component of **v** is given by **the ratio of the determinant of the matrix obtained
    by taking M and substituting c for the i-th column of M, divided by the determinant
    of M itself.** (This statement is called Cramer's Rule.)
  prefs: []
  type: TYPE_NORMAL
- en: 9\. The condition that the determinant of a matrix is 0 means that the **hyper-volume
    of the region determined by the columns is 0** which means that **they are linearly
    dependent,** and it means that **there is a non-zero linear combination of the
    columns that is the zero vector.** Which means that for this vector **v**, we
    have M**v** = **0**.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. The determinant is unchanged by rotations of coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 11\. The polynomial of degree n in x defined by **det(M - xI)** is called **the
    characteristic polynomial of M.** Its roots (solutions to it = 0) are called **the
    eigenvalues** of M.
  prefs: []
  type: TYPE_NORMAL
- en: We now comment on these claims.
  prefs: []
  type: TYPE_NORMAL
- en: The first three follow immediately from the definition of the determinant as
    a linear version of hyper-volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'It follows from these that you can add a multiple of one row to another without
    changing the determinant: because by linearity the change would have to be a multiple
    of the determinant of a matrix with two identical rows.'
  prefs: []
  type: TYPE_NORMAL
- en: But then you can do this until the matrix is diagonal, at which point the determinant,
    again by linearity, is the product of the diagonal elements times the determinant
    of the identity matrix (which is 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'The statement **that the determinant of a product of two matrices is the product
    of the determinants** is important and useful. It follows by these two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. If the **matrix A is diagonal,** then det A is the product of the diagonal
    elements of A.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the rows **of AB are just the rows of B each multiplied by
    the corresponding diagonal element of A.**
  prefs: []
  type: TYPE_NORMAL
- en: By linearity then, the **determinant of AB is the product of the diagonal elements
    of A times the determinant of B,** that is, it is **the product of the determinant
    of A and that of B,** as we have claimed.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. If we **apply a row operation (no multiplying rows by constants allowed)
    as discussed in property 4 above, on A to obtain a new matrix A' and apply the
    same row operation to (AB) to obtain (AB)' we will have**
  prefs: []
  type: TYPE_NORMAL
- en: (A'B) = (AB)'
  prefs: []
  type: TYPE_NORMAL
- en: and we will have det A = det A' , and det AB = det A'B.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this until A is diagonal, at which point we can use the first statement
    here to tell us: (det A'') * (det B) = det A''B, from which our conclusion follows.'
  prefs: []
  type: TYPE_NORMAL
- en: The statements about cofactors merely make explicit what it means to be linear
    in each row and column.
  prefs: []
  type: TYPE_NORMAL
- en: The sign factor can be deduced from the fact that it is 1 if you consider the
    first row and column, (think of the identity matrix) and you can switch rows and
    columns with their neighbors i - 1 and j - 1 times to rearrange things so that
    the i-th row and j-th column become the first and everything else is in their
    original orders.
  prefs: []
  type: TYPE_NORMAL
- en: This will cause i + j - 2 sign changes, which gives the sign factor noted.
  prefs: []
  type: TYPE_NORMAL
- en: As already noted somewhere the **cofactor formula for the inverse** is a statement
    about the dot products of the rows of the inverse with the columns of the original
    matrix. The diagonal products must be 1 which follows for **![](../Images/d3936880283c27a9cd884203d9ca53fe.jpg)**
    from the cofactor formula for the determinant, and the off diagonal ones must
    be zero because by that same formula they represent the determinants of matrices
    with two identical columns or rows.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cramer''s rule** is the observation that by the **definition of the inverse,**
    the desired coefficient is **the dot product of the i-th row of the inverse of
    M with the vector c.** But by the cofactor formula this is **the dot product of
    the i-th column of the cofactor matrix with the vector c, divided by the determinant
    of M,** and that is the ratio of the two determinants of Cramer''s rule.'
  prefs: []
  type: TYPE_NORMAL
- en: 32.5 Matrices and Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**The most important use of matrices lies in representing linear transformations
    on a vector space.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**How?**'
  prefs: []
  type: TYPE_NORMAL
- en: A matrix represents the tranformation which takes **the first basis vector into
    first column of the matrix, second basis vector into the second column of the
    matrix, j-th basis vector into j-th column.**
  prefs: []
  type: TYPE_NORMAL
- en: '**What does it do to other vectors?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that any other vector, say **v** can be expressed as **a linear combination
    of the basis vectors: v** gets transformed by the transformation to **that same
    linear combination of the column vectors of the matrix.**'
  prefs: []
  type: TYPE_NORMAL
- en: For example, the sum of the first two basis vectors gets mapped into the sum
    of the first two columns of the matrix; the average of the two basis vectors into
    the average of the columns, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notice that the same transformation acting on vectors will usually be described
    by a different matrix if you use a different basis.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Example](example01.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 32.6 Invariants of Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the same transformation can often be represented by many different matrices,
    depending upon the basis chosen to describe them, the following questions can
    be raised:'
  prefs: []
  type: TYPE_NORMAL
- en: '**What properties of a matrix are the same independent of the basis, being
    intrinsic properties of the transformation the matrices represent?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**When do two matrices represent the same transformation with different bases?**'
  prefs: []
  type: TYPE_NORMAL
- en: There are actually several questions that can be raised for each of these, because
    we can be describing matrices whose elements are **all real, or we can permit
    complex elements,** and we can insist that we stick to **bases that are orthonormal
    (the dot product of any basis vector with itself is 1 and its dot product with
    any other basis vector is 0) or allow more general bases including those with
    complex components.**
  prefs: []
  type: TYPE_NORMAL
- en: The answers are a bit different depending on which context we consider, but
    they are fundamentally similar.
  prefs: []
  type: TYPE_NORMAL
- en: We will here consider **real matrices and real orthonormal bases** only.
  prefs: []
  type: TYPE_NORMAL
- en: A matrix which takes our original basis vectors into another orthonormal set
    of basis vectors is called an **orthogonal matrix;** its columns must be **mutually
    orthogonal and have dot products 1 with themselves, since these columns must form
    an orthonormal basis.**
  prefs: []
  type: TYPE_NORMAL
- en: These conditions mean that an orthogonal matrix has its [transpose](#Transpose)
    as its inverse! **(The condition for two matrices to be inverses of one another
    is that the rows of one are orthogonal to the columns of the other, except that
    rows and columns with the same index have dot product one with one another.)**
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question we address is: **what happens to a matrix M when an orthogonal
    transformation A is applied to the original basis vectors?**'
  prefs: []
  type: TYPE_NORMAL
- en: A transforms the initial basis to A's columns. We want to know what the matrix
    M does to these column vectors. That is **exactly what the matrix MA** does to
    the original column basis vectors. **A takes them into the new basis vectors and
    M then transforms these into whatever it does to them.**
  prefs: []
  type: TYPE_NORMAL
- en: However the product MA expresses what M does to the new basis vectors **in terms
    of the old ones; its columns give the effect of M on the new basis vectors as
    linear combinations of the old basis vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: We want to re-express these columns as linear combinations of the new basis
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we do this?**'
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to see is to **look at what happens when M is the identity matrix,
    I. This is the matrix which takes any vector into itself. After the change of
    basis, it must still take any vector into itself, so it must still be the identity
    matrix.**
  prefs: []
  type: TYPE_NORMAL
- en: But if M = I then MA is just IA or A itself and that is what I becomes in terms
    of the old basis That is columns of A tell what the new basis vectors look like
    in terms of the old ones.
  prefs: []
  type: TYPE_NORMAL
- en: To re-express I in terms of the new basis you must do something which takes
    AI back into I. **The way to do this is to multiply on the left by A^(-1) which
    is A^T.**
  prefs: []
  type: TYPE_NORMAL
- en: We deduce that multiplying on the left by A^(-1) performs the desired re-expression
    **for I and therefore for any matrix M.** We conclude that in the new basis the
    matrix M becomes **A^TMA.**
  prefs: []
  type: TYPE_NORMAL
- en: '**The transpose of a matrix is the matrix obtained by interchanging its rows
    and columns.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**A matrix is symmetric if it the same after such a transformation.**'
  prefs: []
  type: TYPE_NORMAL
- en: We have just seen that **an orthogonal transformation** changes a matrix M to
    one of the form A^TMA, where A^TA = I, and the **matrix A has columns given by
    the new orthonormal basis expressed in terms of the old basis.**
  prefs: []
  type: TYPE_NORMAL
- en: A nice feature of such a transformation is that **if M is symmetric, it stays
    symmetric after any such transformation.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 32.4 Prove this claim: that M is symmetric if and only if A^TMA
    is symmetric.**'
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that the only matrices that can possibly be made diagonal by such
    transformations are **symmetric;** since **when they are diagonal they are trivially
    symmetric.**
  prefs: []
  type: TYPE_NORMAL
- en: '**If a matrix is diagonal, its eigenvectors are the basis vectors. So we have
    shown that only symmetric matrices have real orthonormal bases.**'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **any matrix that is symmetric can be made diagonal by an
    orthogonal transformation.** Another way to say this is there is an orthonormal
    basis of real eigenvectors for every symmetric matrix. [Proof of this claim](proof01.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'We have answered our first question: **which matrices can be put in diagonal
    form by choosing a new orthonormal basis? The answer being any symmetric matrix.**
    And the way to put a matrix in such form is to find its eigenvectors and choose
    an orthonormal set of these.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second question was: **when will two such matrices be representations of
    the same transformation in a different basis**. And the answer is, when their
    characteristic equations are the same, so that their eigenvalues are the same
    and have the same multiplicities.'
  prefs: []
  type: TYPE_NORMAL
- en: 32.7 Other Notions of Diagonalizability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have noted that our first question has a number of variants, and we will
    note the changes in the answers when the variants are used.
  prefs: []
  type: TYPE_NORMAL
- en: When we allow complex matrix elements, and complex vectors, we can diagonalize
    a wider class of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: When a vector has complex valued entries, we still want to interpret its length
    as the square root of its dot product with itself. **We want this to be positive.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Therefore we redefine the dot product to make this so: the dot product of
    a complex vector with itself is the sum of the absolute value squared of its entries.**'
  prefs: []
  type: TYPE_NORMAL
- en: We generalize this to the dot product of a row vector and a column vector by
    making it **the sum of the products of the complex conjugate of the component
    of the row vector with the corresponding component of the column vector.**
  prefs: []
  type: TYPE_NORMAL
- en: Thus the dot product of the column vector with entries (a + ib, c + id) with
    the same row vector is
  prefs: []
  type: TYPE_NORMAL
- en: (a - ib) * (a + ib) + (c - id) * (c + id)
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: a² + b² + c² + d²
  prefs: []
  type: TYPE_NORMAL
- en: The dot product of the same column vector with (e + if, g + ih) is instead
  prefs: []
  type: TYPE_NORMAL
- en: (e - if) * (a + ib) + (g - ih) * (c + id)
  prefs: []
  type: TYPE_NORMAL
- en: Notice that with this definition the dot product is no longer symmetric. However
    it does not change if you interchange row and column and also take the complex
    conjugate, since the asymmetry lies in taking the complex conjugate of the row
    and not the column.
  prefs: []
  type: TYPE_NORMAL
- en: With complex vectors we define an **orthonormal basis** to be one for which
    **the dot product of each column with the complex conjugate of the entries in
    the other columns are zero.**
  prefs: []
  type: TYPE_NORMAL
- en: This means that with this definition, a matrix that takes a given basis into
    another **orthonormal basis in this context has the property that its complex
    conjugate transpose is its inverse.**
  prefs: []
  type: TYPE_NORMAL
- en: Such a matrix is called a **unitary matrix,** and **the linear transformation
    which takes one orthonormal complex basis to another is called a unitary transformation.**
  prefs: []
  type: TYPE_NORMAL
- en: The effect of a unitary transformation described by the unitary matrix U on
    a matrix M is now U^t * MU as can be shown by the same argument as before. (Of
    course real unitary matrices are orthogonal.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Again we can ask, what matrices can be diagonalized by a unitary transformation?**
    A preliminary question is: **which matrices can be diagonalized so that its eigenvalues,
    which are what appear on the diagonal when it is diagonalized, are all real?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer now is that any matrix that **is its own transpose complex conjugate**
    will have this property: which implies if M is n by n, **M has n real eigenvalues
    and an orthonormal basis of eigenvectors.**'
  prefs: []
  type: TYPE_NORMAL
- en: Such matrices are called Hermitian matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Again the necessity of this condition follows from the fact that **"Hermitivity"
    is preserved by unitary transformations and real diagonal matrices are Hermitian.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Hermitian matrices are of particular importance** because they have the possibility
    of representing measurable real observables in physical systems. They do so in
    quantum mechanics.'
  prefs: []
  type: TYPE_NORMAL
- en: Answer to the general question, **without reference to real eigenvalues is that
    the matrix must commute with its complex conjugate transpose.**
  prefs: []
  type: TYPE_NORMAL
- en: This condition is again preserved under unitary transformations, and it is a
    property of diagonal matrices, since all diagonal matrices commute with one another,
    so it is definitely necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Still another question is, **when can a matrix be diagonalized by any change
    of basis, without any requirement about orthonormality; that is when does there
    exist any kind of a basis of eigenvectors for the matrix M?**
  prefs: []
  type: TYPE_NORMAL
- en: There is an easy answer which again can easily be seen to be necessary. Suppose
    a[1], a[2], ..., a[k] are the **distinct eigenvalues of M.**
  prefs: []
  type: TYPE_NORMAL
- en: Any vector can be written as a sum of basis vectors.
  prefs: []
  type: TYPE_NORMAL
- en: If each basis vector is an eigenvector of M, say corresponding to eigenvalue
    a[j], then M - a[j]I acting on it will be the zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand M - a[h]I for a[h] different from a[j], acting on it merely
    multiplies it by a[j] - a[h].
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if there is a basis consisting of eigenvectors of M then the **product
    over all j from 1 to k of (M - a[j]I)** must be the zero matrix, **since it must
    give 0 in acting on every basis vector.**
  prefs: []
  type: TYPE_NORMAL
- en: This product is called the **minimal polynomial of M** and the equation that
    it is the zero matrix is called the minimal equation for M. Thus **if M obeys
    its own minimal equation then it has a basis of eigenvectors.**
  prefs: []
  type: TYPE_NORMAL
- en: By the way an interesting and curious fact is that **every matrix obeys its
    own characteristic equation** (that is if you substitute M for the variable x
    in it, you get the 0 matrix).
  prefs: []
  type: TYPE_NORMAL
- en: 32.8 Computing Eigenvalues and Eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We here address the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. How can we actually compute eigenvalues and eigenvectors of a given matrix?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. How can we apply what we know about matrices to quadratic functions (also
    called quadratic forms)? And to critical points and saddle points.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. We describe an eigenvector game: learn how to just look at a matrix and
    guess an eigenvector!'
  prefs: []
  type: TYPE_NORMAL
- en: How to compute eigenvalues and eigenvectors for large matrices is an important
    question in numerical analysis. We will merely scratch the surface for small matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an obvious way to look for real eigenvalues of a real matrix: you
    need only write out its characteristic polynomial, plot it and find its solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: This is quite easy to do in two dimensions, not difficult in three or four dimensions,
    and not really difficult for a computer in many more dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This is very straightforward and dull.
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions the characteristic equation is
  prefs: []
  type: TYPE_NORMAL
- en: x² - tr(M)x + det(M) = 0
  prefs: []
  type: TYPE_NORMAL
- en: This equation can be solved using the quadratic formula and the eigenvalues
    can be obtained by explicit formulae.
  prefs: []
  type: TYPE_NORMAL
- en: In three dimensions the characteristic equation is
  prefs: []
  type: TYPE_NORMAL
- en: x³ - tr(M)x² + Ax - det(M) = 0
  prefs: []
  type: TYPE_NORMAL
- en: where A is the sum of pairs of diagonal elements minus the products of each
    opposite pair of off diagonal elements
  prefs: []
  type: TYPE_NORMAL
- en: A = M[11] * M[22] + M[11] * M[33] + M[22] * M[33] - M[12] * M[21] - M[13] *
    M[31] - M[23] * M[32]
  prefs: []
  type: TYPE_NORMAL
- en: There is a cubic formula for solving this equation but it is probably easier
    to find one solution, say z, numerically, whereupon the other two obey the quadratic
    equation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcdf31ff51d8ba43fca4aae2411d804a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the characteristic polynomial is cubic, it goes in opposite directions
    for large arguments positive versus negative, and so by starting at same and homing
    in (by the divide and conquer approach) you can find a solution to any desired
    accuracy with relative ease.
  prefs: []
  type: TYPE_NORMAL
- en: So how do we find an eigenvector given an eigenvalue z? There is a very simple
    answer that usually works. A column eigenvector can be obtained by taking the
    cofactors of any row of M - zI and arranging them as a column vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercises:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**32.5 When will this approach fail?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**32.6 Prove that if the cofactors don''t all vanish they provide a column
    eigenvector.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**32.7 Choose a random 3 by 3 matrix and find an eigenvalue and corresponding
    eigenvector.**'
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to find eigenvectors and eigenvalues that often work.
  prefs: []
  type: TYPE_NORMAL
- en: One approach is to raise the matrix to a high power. This is easier to do than
    it sounds.
  prefs: []
  type: TYPE_NORMAL
- en: You can then notice that the high power of the matrix will tend to have rank
    1, usually, and you can read off a row and a column eigenvector from it.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily deduce the corresponding eigenvalue by having the matrix act
    on the eigenvector you find.
  prefs: []
  type: TYPE_NORMAL
- en: If there is an eigenvalue that has greater magnitude than any other and it has
    only one eigenvector, (it is not a multiple root of the characteristic equation
    for M) then this method will usually find it.
  prefs: []
  type: TYPE_NORMAL
- en: You can apply the same approach to the inverse matrix to M to find an eigenvalue
    smallest in magnitude and its eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: These actions are relatively easy on Excel spreadsheets, because these have
    functions that take the product of two matrices (called mmult), which finds the
    inverse of a matrix (minverse) and takes the determinant of a matrix (mdeterm).
  prefs: []
  type: TYPE_NORMAL
- en: Using mmult it is quite easy to square a matrix, copying the procedure to raise
    it to the fourth power, copy both procedures to raise it to the eighth and then
    sixteenth power; copy the whole mess to raise to the 256^(th) power etc.
  prefs: []
  type: TYPE_NORMAL
- en: For a four by four matrix once you have two eigenvalues, then you can get the
    rest by solving quadratics and you can usually get the largest and smallest in
    magnitude by raising A and A^(-1) to high powers.
  prefs: []
  type: TYPE_NORMAL
- en: Of course once you have the eigenvalue that is largest in magnitude, you could
    look for the second largest. This can be accomplished by projecting the columns
    of M to vectors normal to the first row eigenvector, and working with the resulting
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Of course you run into trouble with this approach if there are two eigenvectors
    with the same largest magnitude of eigenvalue or nearly the same one.
  prefs: []
  type: TYPE_NORMAL
- en: How do we find the matrix A which we can use to diagonalize M?
  prefs: []
  type: TYPE_NORMAL
- en: A's columns are normalized eigenvectors of M.
  prefs: []
  type: TYPE_NORMAL
- en: <applet code="LinearTransformations3D" codebase="../applets/" archive="linearTransformations3D.jar,go.jar,goText.jar,mk_lib.jar,parser_math.jar,jcbwt363.jar,jama.jar"
    width="760" height="450"></applet>
  prefs: []
  type: TYPE_NORMAL
- en: 32.9 Application to Quadratic Forms and Spring Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another place in which matrices have appeared in previous chapters was in the
    discussion of the behavior of functions of several variables at a critical point
    (at which the gradient of the function is the **0** vector).
  prefs: []
  type: TYPE_NORMAL
- en: We then noticed that the behavior of the function could be described by the
    matrix of second derivatives of the function at that point. This is the matrix
    whose ij-th element is the second partial derivative of the function with respect
    to the i-th and j-th variable.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric matrices each have a an orthonormal basis of real eigenvectors as
    we proved above, obtainable by an orthogonal transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we examine the structure of the matrix for the form using this basis, we
    find that it is diagonal, and so the conditions for an extremum become simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '**If all the eigenvalues of the second derivative matrix have the same sign
    the function has a local maximum or minimum, with a minimum when they are all
    positive.**'
  prefs: []
  type: TYPE_NORMAL
- en: There is a saddle if the signs are mixed, and you must sometimes look at higher
    derivatives when some of the eigenvalues are 0's.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about the matrix of second derivatives we are really talking about
    the quadratic form which describes the quadratic terms in the Taylor series expansion
    of our function about the critical point.
  prefs: []
  type: TYPE_NORMAL
- en: If we focus on quadratic forms we realize that we can use a wider class of transformations
    in order to change their appearance than we can when dealing with transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Thus we can make changes of scale of the individual variables to make any positive
    diagonal quadratic into one that has all (non-zero) eigenvalues the same. (Thus
    we can change ![](../Images/4638051d3cef96abffe3c6cb680c6c6a.jpg) by setting ![](../Images/060ade7be3680b8729c5429cb2474691.jpg))
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to diagonalize two distinct quadratic forms simultaneously. You
    can make the matrix of one into an identity matrix, and then diagonalize the other.
  prefs: []
  type: TYPE_NORMAL
- en: This is in contrast to what happens with transformations. Two transformations
    must commute to be simultaneously diagonal with the same basis (obviously necessary
    since all diagonal matrices commute with one another).
  prefs: []
  type: TYPE_NORMAL
- en: '(This statement is proven as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Diagonalize the matrix M. You can then observe that the condition that M and
    N commute is the condition that all the off diagonal elements of N say the ij-th
    link indices whose diagonal elements of M are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Thus if the ij-th entry of N is non-zero then the i-th and j-th eigenvalues
    of M must be the same if N and M are to commute.
  prefs: []
  type: TYPE_NORMAL
- en: If they are the same then as far as diagonalizing N is concerned, the problem
    breaks up into parts for each eigenvalue of M; and for each part M is a multiple
    of the identity matrix and will stay diagonal when the corresponding block of
    N is diagonalized.)
  prefs: []
  type: TYPE_NORMAL
- en: Given a system of springs and masses, there will be one quadratic form that
    represents the kinetic energy of the system in terms of momentum variables, and
    another which represents the potential energy of the system in position variables.
  prefs: []
  type: TYPE_NORMAL
- en: The remarks above tell us that it is always possible to choose a normalization
    and basis of coordinates so that both of these forms are diagonal. This means
    that the entire system can be analyzed as a bunch of independent simple one dimensional
    springs (each of which can represent a complex combination of original coordinates).
    The corresponding eigenvalues determine the "normal modes" of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 32.10 Computing Eigenvalues and Eigenvectors on a Spreadsheet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can build a spreadsheet that will find same for any 3 by 3 matrix that has
    three real eigenvalues, as follows. It is very worthwhile for you to attempt to
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: First find the trace determinant and second invariant (A) of the given matrix.
  prefs: []
  type: TYPE_NORMAL
- en: How?
  prefs: []
  type: TYPE_NORMAL
- en: The trace is easy, the determinant is a single command in excel.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the second invariant in excel is also easy: extend the matrix by making
    a fourth row and column which are the same as the first ones, and make the 44
    entry the same as the 11 entry.'
  prefs: []
  type: TYPE_NORMAL
- en: Then find the two by two diagonal matrices in columns and rows 1, 2 in columns
    and rows 2, 3 and in columns and rows 3, 4\. The sum of these three will be A.
  prefs: []
  type: TYPE_NORMAL
- en: Then solve the characteristic equation. This can be done by starting with very
    large values say +1000 and -1000 and homing in on a solution using the divide
    and conquer approach.
  prefs: []
  type: TYPE_NORMAL
- en: Then find the other two eigenvalues by solving the quadratic equation previously
    described.
  prefs: []
  type: TYPE_NORMAL
- en: They will not always exist, since the roots of the quadratic could be complex;
    if so change your matrix to make them real.
  prefs: []
  type: TYPE_NORMAL
- en: It is barely possible that your matrix is not diagonalizable, in which case
    it does not have three eigenvectors, but this can only happen if two of the eigenvalues
    are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Now find eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: How?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a good try: write down the matrix M - zI where z is one of your eigenvalues.'
  prefs: []
  type: TYPE_NORMAL
- en: Extend the matrix to a fourth and fifth column by copying the first column to
    the fourth and second to fifth.
  prefs: []
  type: TYPE_NORMAL
- en: Then take the two by two determinants given by the first two rows and columns
    23, 34 and 45\. Arrange these as a column. This should be your eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: It could be that these two by two determinants are all 0\. If so you can try
    again with the second and third rows and you could even copy the first row to
    a fourth row and do the same for the third and fourth rows.
  prefs: []
  type: TYPE_NORMAL
- en: If you always fail that means that you had a double eigenvalue (at least two
    of your three eigenvalues are the same). Eigenvectors are actually easier to find
    in this case, when they exist.
  prefs: []
  type: TYPE_NORMAL
- en: If all the eigenvalues are the same then M was a multiple of the identity, and
    every vector is an eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise you can find a column eigenvector for that eigenvalue as described,
    and find a row eigenvector by doing the same thing interchanging rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: Then the column eigenvectors for your double eigenvalue will be any vectors
    normal to your row eigenvector for the other eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have three column eigenvectors, you can form them into a matrix, A
    and examine A^(-1) and A^(-1)MA, which should come out to be diagonal. (These
    are very easy to find in excel using the mmult and minverse functions.)
  prefs: []
  type: TYPE_NORMAL
- en: 32.11 Guessing Eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is a game you can set up on a spreadsheet. Enter an arbitrary matrix M
    somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Three by three is a good way to start.
  prefs: []
  type: TYPE_NORMAL
- en: Enter a 3 component column vector **v** and use the mmult command (or do it
    out yourself) to compute M**v** and for each component of **v** compute the ratio
    of ![](../Images/22cc4f958539c007ba1cb9778692ae92.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the variance of these ratios (that is, the sum of their squares minus
    the square of their sum).
  prefs: []
  type: TYPE_NORMAL
- en: The players can take turns generating the original M and **v**; then they take
    turns modifying **v** by changing **one** of its components.
  prefs: []
  type: TYPE_NORMAL
- en: If the variance of the ratios decreases the player scores a point, otherwise
    loses one. The game ends when the variance becomes negligible, say less than 10^(-10).
  prefs: []
  type: TYPE_NORMAL
- en: The ratios then will be more or less the same and hence the eigenvalue associated
    with the eigenvector produced.
  prefs: []
  type: TYPE_NORMAL
- en: If you get too good at this, you can try with a 5 by 5 matrix, though it is
    boring to enter one at the start.
  prefs: []
  type: TYPE_NORMAL
