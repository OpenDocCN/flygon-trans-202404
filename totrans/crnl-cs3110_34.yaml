- en: 'Lecture 25: Amortized Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The claim that hash tables have *O*(1) expected performance for lookup and insert
    is based on the assumption that the number of elements stored in the table is
    comparable to the number of buckets. If a hash table has many more elements than
    buckets, the number of elements stored at each bucket will become large. For instance,
    with a constant number of buckets and *O*(*n*) elements, the lookup time is *O*(*n*)
    and not *O*(1).
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem is to increase the size of the table when the number
    of elements in the table gets too large compared to the size of the table. If
    we let the <def>load factor</def> be the ratio of the number of elements to the
    number of buckets, when the load factor exceeds some small constant (typically
    2 for chaining and .75 for linear probing), we allocate a new table, typically
    double the size of the old table, and rehash all the elements into the new table.
    This operation is not constant time, but rather linear in the number of elements
    at the time the table is grown.
  prefs: []
  type: TYPE_NORMAL
- en: The linear running time of a resizing operation is not as much of a problem
    as it might sound (although it can be an issue for some real-time computing systems).
    If the table is doubled in size every time it is needed, then the resizing operation
    occurs with exponentially decreasing frequency. As a consequence, the insertion
    of *n* elements into an empty array takes only *O*(*n*) time in all, including
    the cost of resizing. We say that the insertion operation has *O*(1) <def>amortized
    run time</def> because the time required to insert an element is *O*(1) *on average*,
    even though some elements trigger a lengthy rehashing of all the elements of the
    hash table.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial that the array size grow geometrically (doubling). It might be
    tempting to grow the array by a fixed increment (e.g., 100 elements at time),
    but this results in asymptotic linear rather than constant amortized running time.
  prefs: []
  type: TYPE_NORMAL
- en: Now we turn to a more detailed description of amortized analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Amortized Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amortized analysis is a worst-case analysis of a a *sequence* of operations
    — to obtain a tighter bound on the overall or average cost per operation in the
    sequence than is obtained by separately analyzing each operation in the sequence.
    For instance, when we considered the union and find operations for the disjoint
    set data abstraction earlier in the semester, we were able to bound the running
    time of individual operations by *O*(log *n*). However, for a sequence of *n*
    operations, it is possible to obtain tighter than an *O*(*n* log *n*) bound (although
    that analysis is more appropriate to 4820 than to this course). Here we will consider
    a simplified version of the hash table problem above, and show that a sequence
    of *n* insert operations has overall time *O*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main techniques used for amortized analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: The <def>aggregate method</def> , where the total running time for a sequence
    of operations is analyzed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The <def>accounting</def> (or <def>banker's</def> ) method, where we impose
    an extra charge on inexpensive operations and use it to pay for expensive operations
    later on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The <def>potential</def> (or <def>physicist's</def> ) method, in which we derive
    a *potential function* characterizing the amount of extra work we can do in each
    step. This potential either increases or decreases with each successive operation,
    but cannot be negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider an extensible array that can store an arbitrary number of integers,
    like an `ArrayList` or `Vector` in Java. These are implemented in terms of ordinary
    (non-extensible) arrays. Each `add` operation inserts a new element after all
    the elements previously inserted. If there are no empty cells left, a new array
    of double the size is allocated, and all the data from the old array is copied
    to the corresponding entries in the new array. For instance, consider the following
    sequence of insertions, starting with an array of length 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The table is doubled in the second, third, and fifth steps. As each insertion
    takes *O*(*n*) time in the worst case, a simple analysis would yield a bound of
    *O*(*n*²) time for *n* insertions. But it is not this bad. Let's analyze a sequence
    of *n* operations using the three methods.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let *c[i]* be the cost of the *i*-th insertion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consider the size of the table *s[i]* and the cost *c[i]* for the first
    few insertions in a sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Alteratively we can see that *c[i]*=1+*d[i]* where *d[i]* is the cost of doubling
    the table size. That is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then summing over the entire sequence, all the 1's sum to *O*(*n*), and all
    the *d[i]* also sum to *O*(*n*). That is,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: where *m* = log(*n* − 1). Both terms on the right hand side of the inequality
    are *O*(*n*), so the total running time of *n* insertions is *O*(*n*).
  prefs: []
  type: TYPE_NORMAL
- en: Accounting (Banker's) Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aggregate method directly seeks a bound on the overall running time of a
    sequence of operations. In contrast, the accounting method seeks to find a *payment*
    of a number of extra time units charged to each individual operation such that
    the sum of the payments is an upper bound on the total actual cost. Intuitively,
    one can think of maintaining a bank account. Low-cost operations are charged a
    little bit more than their true cost, and the surplus is deposited into the bank
    account for later use. High-cost operations can then be charged less than their
    true cost, and the deficit is paid for by the savings in the bank account. In
    that way we spread the cost of high-cost operations over the entire sequence.
    The charges to each operation must be set large enough that the balance in the
    bank account always remains positive, but small enough that no one operation is
    charged significantly more than its actual cost.
  prefs: []
  type: TYPE_NORMAL
- en: We emphasize that the extra time charged to an operation does not mean that
    the operation really takes that much time. It is just a method of accounting that
    makes the analysis easier.
  prefs: []
  type: TYPE_NORMAL
- en: If we let *c'[i]* be the charge for the *i*-th operation and *c[i]* be the true
    cost, then we would like
  prefs: []
  type: TYPE_NORMAL
- en: Σ*[1≤i≤n] c[i]* ≤ Σ*[1≤i≤n] c'[i]*
  prefs: []
  type: TYPE_NORMAL
- en: for all *n*, which says that the <def>amortized time</def> Σ*[1≤i≤n] c'[i]*
    for that sequence of *n* operations is a bound on the true time Σ*[1≤i≤n] c[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to the example of the extensible array. Say it costs 1 unit to insert
    an element and 1 unit to move it when the table is doubled. Clearly a charge of
    1 unit per insertion is not enough, because there is nothing left over to pay
    for the moving. A charge of 2 units per insertion again is not enough, but a charge
    of 3 appears to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: where *b[i]* is the balance after the *i*-th insertion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, this is enough in general. Let *m* refer to the *m*-th element inserted.
    The three units charged to *m* are spent as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: One unit is used to insert *m* immediately into the table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One unit is used to move *m* the first time the table is grown after *m* is
    inserted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One unit is donated to element *m − 2^k*, where *2^k* is the largest power of
    2 not exceeding *m*, and is used to move that element the first time the table
    is grown after *m* is inserted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now whenever an element is moved, the move is already paid for. The first time
    an element is moved, it is paid for by one of its own time units that was charged
    to it when it was inserted; and all subsequent moves are paid for by donations
    from elements inserted later.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we can do slightly better, by charging just 1 for the first insertion
    and then 3 for each insertion after that, because for the first insertion there
    are no elements to copy. This will yield a zero balance after the first insertion
    and then a positive one thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: Potential (Physicist's) Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Above we saw the aggregate method and the banker's method for dealing with extensible
    arrays. Now let us look at the physicist's method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we can define a <def>potential function</def> Φ (read "Phi") on states
    of a data structure with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Φ(*h*[0]) = 0, where *h*[0] is the initial state of the data structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Φ(*h*[*t*]) ≥ 0 for all states *h*[*t*] of the data structure occurring during
    the course of the computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuitively, the potential function will keep track of the precharged time at
    any point in the computation. It measures how much saved-up time is available
    to pay for expensive operations. It is analogous to the bank balance in the banker's
    method. But interestingly, it depends only on the current state of the data structure,
    irrespective of the history of the computation that got it into that state.
  prefs: []
  type: TYPE_NORMAL
- en: We then define the <def>amortized time</def> of an operation as
  prefs: []
  type: TYPE_NORMAL
- en: '*c* + Φ(*h*'') − Φ(*h*),'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: where *c* is the actual cost of the operation and *h* and *h*' are the states
    of the data structure before and after the operation, respectively. Thus the amortized
    time is the actual time plus the change in potential. Ideally, Φ should be defined
    so that the amortized time of each operation is small. Thus the change in potential
    should be positive for low-cost operations and negative for high-cost operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider a sequence of *n* operations taking actual times *c*[0], *c*[1],
    *c*[2], ..., *c*[*n*−1] and producing data structures *h*[1], *h*[2], ..., *h*[*n*]
    starting from *h*[0]. The total amortized time is the sum of the individual amortized
    times:'
  prefs: []
  type: TYPE_NORMAL
- en: (*c*[0] + Φ(*h*[1]) − Φ(*h*[0])) + (*c*[1] + Φ(*h*[2]) − Φ(*h*[1])) + ... +
    (*c*[*n*−1] + Φ(*h*[*n*]) − Φ(*h*[*n*−1]))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: = *c*[0] + *c*[1] + ... + *c*[*n*−1] + Φ(*h*[*n*]) − Φ(*h*[0])
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: = *c*[0] + *c*[1] + ... + *c*[*n*−1] + Φ(*h*[*n*]).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore the amortized time for a sequence of operations overestimates of the
    actual time by Φ(*h*[*n*]), which by assumption is always positive. Thus the total
    amortized time is always an upper bound on the actual time.
  prefs: []
  type: TYPE_NORMAL
- en: For dynamically resizable arrays with resizing by doubling, we can use the potential
    function
  prefs: []
  type: TYPE_NORMAL
- en: Φ(*h*) = 2*n* − *m*,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: where *n* is the current number of elements and *m* is the current length of
    the array. If we start with an array of length 0 and allocate an array of length
    1 when the first element is added, and thereafter double the array size whenever
    we need more space, we have Φ(*h*[0]) = 0 and Φ(*h*[*t*]) ≥ 0 for all *t*. The
    latter inequality holds because the number of elements is always at least half
    the size of the array.
  prefs: []
  type: TYPE_NORMAL
- en: Now we would like to show that adding an element takes amortized constant time.
    There are two cases.
  prefs: []
  type: TYPE_NORMAL
- en: If *n* < *m*, then the actual cost is 1, *n* increases by 1, and *m* does not
    change. Then the potential increases by 2, so the amortized time is 1 + 2 = 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *n* = *m*, then the array is doubled, so the actual time is *n* + 1. But
    the potential drops from *n* to 2, so amortized time is *n* + 1 + (2 − *n*) =
    3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, the amortized time is O(1).
  prefs: []
  type: TYPE_NORMAL
- en: The key to amortized analysis with the physicist's method is to define the right
    potential function. The potential function needs to save up enough time to be
    used later when it is needed. But it cannot save so much time that it causes the
    amortized time of the current operation to be too high.
  prefs: []
  type: TYPE_NORMAL
