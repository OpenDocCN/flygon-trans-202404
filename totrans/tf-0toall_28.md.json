["```\n\"\"\"\nIn this file, we will implement back propagations by hands\n\nWe will use the Sigmoid Cross Entropy loss function.\nThis is equivalent to tf.nn.sigmoid_softmax_with_logits(logits, labels)\n\n[References]\n\n1) Tensorflow Document (tf.nn.sigmoid_softmax_with_logits)\n    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n\n2) Neural Net Backprop in one slide! by Sung Kim\n    https://docs.google.com/presentation/d/1_ZmtfEjLmhbuM_PqbDYMXXLAqeWN0HwuhcSKnUQZ6MM/edit#slide=id.g1ec1d04b5a_1_83\n\n3) Back Propagation with Tensorflow by Dan Aloni\n    http://blog.aloni.org/posts/backprop-with-tensorflow/\n\n4) Yes you should understand backprop by Andrej Karpathy\n    https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.cockptkn7\n\n[Network Architecture]\n\nInput: x\nLayer1: x * W + b\nOutput layer = \u03c3(Layer1)\n\nLoss_i = - y * log(\u03c3(Layer1)) - (1 - y) * log(1 - \u03c3(Layer1))\nLoss = tf.reduce_sum(Loss_i)\n\nWe want to compute that\n\ndLoss/dW = ???\ndLoss/db = ???\n\nplease read \"Neural Net Backprop in one slide!\" for deriving formulas\n\n\"\"\"\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\n# Predicting animal type based on various features\nxy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\nX_data = xy[:, 0:-1]\nN = X_data.shape[0]\ny_data = xy[:, [-1]]\n\n# y_data has labels from 0 ~ 6\nprint(\"y has one of the following values\")\nprint(np.unique(y_data))\n\n# X_data.shape = (101, 16) => 101 samples, 16 features\n# y_data.shape = (101, 1)  => 101 samples, 1 label\nprint(\"Shape of X data: \", X_data.shape)\nprint(\"Shape of y data: \", y_data.shape)\n\nnb_classes = 7  # 0 ~ 6\n\nX = tf.placeholder(tf.float32, [None, 16])\ny = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n\ntarget = tf.one_hot(y, nb_classes)  # one hot\ntarget = tf.reshape(target, [-1, nb_classes])\ntarget = tf.cast(target, tf.float32)\n\nW = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\nb = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n\ndef sigma(x):\n    # sigmoid function\n    # \u03c3(x) = 1 / (1 + exp(-x))\n    return 1. / (1. + tf.exp(-x))\n\ndef sigma_prime(x):\n    # derivative of the sigmoid function\n    # \u03c3'(x) = \u03c3(x) * (1 - \u03c3(x))\n    return sigma(x) * (1. - sigma(x))\n\n# Forward propagtion\nlayer_1 = tf.matmul(X, W) + b\ny_pred = sigma(layer_1)\n\n# Loss Function (end of forwad propagation)\nloss_i = - target * tf.log(y_pred) - (1. - target) * tf.log(1. - y_pred)\nloss = tf.reduce_sum(loss_i)\n\n# Dimension Check\nassert y_pred.shape.as_list() == target.shape.as_list()\n\n# Back prop (chain rule)\n# How to derive? please read \"Neural Net Backprop in one slide!\"\nd_loss = (y_pred - target) / (y_pred * (1. - y_pred) + 1e-7)\nd_sigma = sigma_prime(layer_1)\nd_layer = d_loss * d_sigma\nd_b = d_layer\nd_W = tf.matmul(tf.transpose(X), d_layer)\n\n# Updating network using gradients\nlearning_rate = 0.01\ntrain_step = [\n    tf.assign(W, W - learning_rate * d_W),\n    tf.assign(b, b - learning_rate * tf.reduce_sum(d_b)),\n]\n\n# Prediction and Accuracy\nprediction = tf.argmax(y_pred, 1)\nacct_mat = tf.equal(tf.argmax(y_pred, 1), tf.argmax(target, 1))\nacct_res = tf.reduce_mean(tf.cast(acct_mat, tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(500):\n        sess.run(train_step, feed_dict={X: X_data, y: y_data})\n\n        if step % 10 == 0:\n            # Within 300 steps, you should see an accuracy of 100%\n            step_loss, acc = sess.run([loss, acct_res], feed_dict={\n                                      X: X_data, y: y_data})\n            print(\"Step: {:5}\\t Loss: {:10.5f}\\t Acc: {:.2%}\" .format(\n                step, step_loss, acc))\n\n    # Let's see if we can predict\n    pred = sess.run(prediction, feed_dict={X: X_data})\n    for p, y in zip(pred, y_data):\n        msg = \"[{}]\\t Prediction: {:d}\\t True y: {:d}\"\n        print(msg.format(p == int(y[0]), p, int(y[0])))\n\n\"\"\"\nOutput Example\n\nStep:     0      Loss:  453.74799        Acc: 38.61%\nStep:    20      Loss:   95.05664        Acc: 88.12%\nStep:    40      Loss:   66.43570        Acc: 93.07%\nStep:    60      Loss:   53.09288        Acc: 94.06%\n...\nStep:   290      Loss:   18.72972        Acc: 100.00%\nStep:   300      Loss:   18.24953        Acc: 100.00%\nStep:   310      Loss:   17.79592        Acc: 100.00%\n...\n[True]   Prediction: 0   True y: 0\n[True]   Prediction: 0   True y: 0\n[True]   Prediction: 3   True y: 3\n[True]   Prediction: 0   True y: 0\n...\n\"\"\" \n```"]