["```\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib import rnn\n\ntf.set_random_seed(777)  # reproducibility\n\nsentence = (\"if you want to build a ship, don't drum up people together to \"\n            \"collect wood and don't assign them tasks and work, but rather \"\n            \"teach them to long for the endless immensity of the sea.\")\n\nchar_set = list(set(sentence))\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\ndata_dim = len(char_set)\nhidden_size = len(char_set)\nnum_classes = len(char_set)\nsequence_length = 10  # Any arbitrary number\nlearning_rate = 0.1\n\ndataX = []\ndataY = []\nfor i in range(0, len(sentence) - sequence_length):\n    x_str = sentence[i:i + sequence_length]\n    y_str = sentence[i + 1: i + sequence_length + 1]\n    print(i, x_str, '->', y_str)\n\n    x = [char_dic[c] for c in x_str]  # x str to index\n    y = [char_dic[c] for c in y_str]  # y str to index\n\n    dataX.append(x)\n    dataY.append(y)\n\nbatch_size = len(dataX)\n\nX = tf.placeholder(tf.int32, [None, sequence_length])\nY = tf.placeholder(tf.int32, [None, sequence_length])\n\n# One-hot encoding\nX_one_hot = tf.one_hot(X, num_classes)\nprint(X_one_hot)  # check out the shape\n\n# Make a lstm cell with hidden_size (each unit output vector size)\ndef lstm_cell():\n    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n    return cell\n\nmulti_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n\n# outputs: unfolding size x hidden size, state = hidden size\noutputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n\n# FC layer\nX_for_fc = tf.reshape(outputs, [-1, hidden_size])\noutputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n\n# reshape out for sequence_loss\noutputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n\n# All weights are 1 (equal weights)\nweights = tf.ones([batch_size, sequence_length])\n\nsequence_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nmean_loss = tf.reduce_mean(sequence_loss)\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(500):\n    _, l, results = sess.run(\n        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n    for j, result in enumerate(results):\n        index = np.argmax(result, axis=1)\n        print(i, j, ''.join([char_set[t] for t in index]), l)\n\n# Let's print the last char of each result to check it works\nresults = sess.run(outputs, feed_dict={X: dataX})\nfor j, result in enumerate(results):\n    index = np.argmax(result, axis=1)\n    if j is 0:  # print all for the first result to make a sentence\n        print(''.join([char_set[t] for t in index]), end='')\n    else:\n        print(char_set[index[-1]], end='')\n\n'''\n0 167 tttttttttt 3.23111\n0 168 tttttttttt 3.23111\n0 169 tttttttttt 3.23111\n\u2026\n499 167  of the se 0.229616\n499 168 tf the sea 0.229616\n499 169   the sea. 0.229616\n\ng you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n\n''' \n```"]