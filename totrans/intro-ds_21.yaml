- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Classification](classification.htm)
    > K Nearest Neighbors'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '[地图](data_mining_map.htm) > [数据科学](data_mining.htm) > [预测未来](predicting_the_future.htm)
    > [建模](modeling.htm) > [分类](classification.htm) > K 最近邻'
- en: K Nearest Neighbors - Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K 最近邻 - 分类
- en: K nearest neighbors is a simple algorithm that stores all available cases and
    classifies new cases based on a similarity measure (e.g., distance functions).
    KNN has been used in statistical estimation and pattern recognition already in
    the beginning of 1970�s as a non-parametric technique.  **Algorithm**A case is
    classified by a majority vote of its neighbors, with the case being assigned to
    the class most common amongst its K nearest neighbors measured by a distance function.
    If K = 1, then the case is simply assigned to the class of its nearest neighbor.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: K 最近邻是一种简单的算法，它存储所有可用案例，并根据相似度度量（例如，距离函数）对新案例进行分类。KNN 在 1970 年代初期就已经被用于统计估计和模式识别中，作为一种非参数技术。
    **算法**案例被其邻居的多数投票分类，通过距离函数测量其 K 个最近邻居中最常见的类别来分配。如果 K = 1，则案例简单地分配给其最近邻居的类别。
- en: '![](../Images/823cfbf8638e639628498c708e1847b6.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/823cfbf8638e639628498c708e1847b6.jpg)'
- en: It should also be noted that all three distance measures are only valid for
    continuous variables. In the instance of categorical variables the Hamming distance
    must be used. It also brings up the issue of standardization of the numerical
    variables between 0 and 1 when there is a mixture of numerical and categorical
    variables in the dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该注意到，这三个距离度量仅适用于连续变量。在分类变量的情况下，必须使用 Hamming 距离。这也提出了在数据集中存在数值和分类变量混合时，对数值变量进行标准化到
    0 和 1 之间的问题。
- en: '![](../Images/15dce582d6676e4936ea3118e1c80eee.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15dce582d6676e4936ea3118e1c80eee.jpg)'
- en: Choosing the optimal value for K is best done by first inspecting the data.
    In general, a large K value is more precise as it reduces the overall noise but
    there is no guarantee. Cross-validation is another way to retrospectively determine
    a good K value by using an independent dataset to validate the K value. Historically,
    the optimal K for most datasets has been between 3-10\. That produces much better
    results than 1NN.  *Example*:Consider the following data concerning credit default.
    Age and Loan are two numerical variables (predictors) and Default is the target.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的 K 值最好是首先检查数据。一般来说，较大的 K 值更精确，因为它减少了总体噪音，但并没有保证。交叉验证是另一种通过使用独立数据集来验证 K
    值的良好方法。在大多数数据集的历史上，最佳的 K 值在 3-10 之间。这比 1NN 产生了更好的结果。 *示例*：考虑以下关于信用违约的数据。年龄和贷款是两个数值变量（预测变量），违约是目标。
- en: '![](../Images/2aca298a8a771f1ac7bb59a0cef929d8.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2aca298a8a771f1ac7bb59a0cef929d8.jpg)'
- en: We can now use the training set to classify an unknown case (Age=48 and Loan=$142,000)
    using Euclidean distance. If K=1 then the nearest neighbor is the last case in
    the training set with Default=Y.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用训练集使用欧几里得距离对未知案例（年龄=48，贷款=$142,000）进行分类。如果 K=1，则最近邻是训练集中最后一个具有默认=Y
    的案例。
- en: D = Sqrt[(48-33)^2 + (142000-150000)^2] = 8000.01  >> Default=Y
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: D = Sqrt[(48-33)^2 + (142000-150000)^2] = 8000.01  >> 默认=Y
- en: '![](../Images/8868f55675973ca7e115c021c5612b30.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8868f55675973ca7e115c021c5612b30.jpg)'
- en: With K=3, there are two Default=Y and one Default=N out of three closest neighbors.
    The prediction for the unknown case is again Default=Y. **Standardized Distance**One
    major drawback in calculating distance measures directly from the training set
    is in the case where variables have different measurement scales or there is a
    mixture of numerical and categorical variables. For example, if one variable is
    based on annual income in dollars, and the other is based on age in years then
    income will have a much higher influence on the distance calculated. One solution
    is to standardize the training set as shown below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当 K=3 时，三个最接近的邻居中有两个默认为 Y，一个默认为 N。对于未知案例的预测再次为默认=Y。 **标准化距离**直接从训练集计算距离度量的一个主要缺点是，当变量具有不同的测量尺度或数据集中存在数值和分类变量的混合时。例如，如果一个变量基于年收入（以美元计）
    ，而另一个基于年龄（以年为单位），那么收入将对计算的距离具有更高的影响。一种解决方案是如下所示地对训练集进行标准化。
- en: '![](../Images/a1eb9b8366539e0e41835e823426effd.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1eb9b8366539e0e41835e823426effd.jpg)'
- en: Using the standardized distance on the same training set, the unknown case returned
    a different neighbor which is not a good sign of robustness.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的训练集上使用标准化距离，未知情况返回了一个不同的邻居，这不是鲁棒性的一个好迹象。
- en: '| [Exercise](knn_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Knn.txt)
    | ![](../Images/dc9f5f2d562c6ce8cb7def0d0596abff.jpg) [KNN Interactive](flash/KNN_flash.html)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [练习](knn_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Knn.txt)
    | ![](../Images/dc9f5f2d562c6ce8cb7def0d0596abff.jpg) [KNN 交互式](flash/KNN_flash.html)
    |'
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a new KNN
    algorithm using [Linear Correlation](numerical_numerical.htm).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) 尝试使用[线性相关](numerical_numerical.htm)发明一个新的KNN算法。'
