- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Classification](classification.htm)
    > K Nearest Neighbors'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: K Nearest Neighbors - Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K nearest neighbors is a simple algorithm that stores all available cases and
    classifies new cases based on a similarity measure (e.g., distance functions).
    KNN has been used in statistical estimation and pattern recognition already in
    the beginning of 1970�s as a non-parametric technique.  **Algorithm**A case is
    classified by a majority vote of its neighbors, with the case being assigned to
    the class most common amongst its K nearest neighbors measured by a distance function.
    If K = 1, then the case is simply assigned to the class of its nearest neighbor.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/823cfbf8638e639628498c708e1847b6.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: It should also be noted that all three distance measures are only valid for
    continuous variables. In the instance of categorical variables the Hamming distance
    must be used. It also brings up the issue of standardization of the numerical
    variables between 0 and 1 when there is a mixture of numerical and categorical
    variables in the dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15dce582d6676e4936ea3118e1c80eee.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: Choosing the optimal value for K is best done by first inspecting the data.
    In general, a large K value is more precise as it reduces the overall noise but
    there is no guarantee. Cross-validation is another way to retrospectively determine
    a good K value by using an independent dataset to validate the K value. Historically,
    the optimal K for most datasets has been between 3-10\. That produces much better
    results than 1NN.  *Example*:Consider the following data concerning credit default.
    Age and Loan are two numerical variables (predictors) and Default is the target.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2aca298a8a771f1ac7bb59a0cef929d8.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: We can now use the training set to classify an unknown case (Age=48 and Loan=$142,000)
    using Euclidean distance. If K=1 then the nearest neighbor is the last case in
    the training set with Default=Y.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: D = Sqrt[(48-33)^2 + (142000-150000)^2] = 8000.01  >> Default=Y
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8868f55675973ca7e115c021c5612b30.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: With K=3, there are two Default=Y and one Default=N out of three closest neighbors.
    The prediction for the unknown case is again Default=Y. **Standardized Distance**One
    major drawback in calculating distance measures directly from the training set
    is in the case where variables have different measurement scales or there is a
    mixture of numerical and categorical variables. For example, if one variable is
    based on annual income in dollars, and the other is based on age in years then
    income will have a much higher influence on the distance calculated. One solution
    is to standardize the training set as shown below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1eb9b8366539e0e41835e823426effd.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Using the standardized distance on the same training set, the unknown case returned
    a different neighbor which is not a good sign of robustness.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的训练集上使用标准化距离，未知情况返回了一个不同的邻居，这不是鲁棒性的一个好迹象。
- en: '| [Exercise](knn_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Knn.txt)
    | ![](../Images/dc9f5f2d562c6ce8cb7def0d0596abff.jpg) [KNN Interactive](flash/KNN_flash.html)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| [练习](knn_exercise.htm) | [![](../Images/a890baab528b0ca069f7f2599c0c5e39.jpg)](datasets/Knn.txt)
    | ![](../Images/dc9f5f2d562c6ce8cb7def0d0596abff.jpg) [KNN 交互式](flash/KNN_flash.html)
    |'
- en: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) Try to invent a new KNN
    algorithm using [Linear Correlation](numerical_numerical.htm).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/04c11d11a10b9a2348a1ab8beb8ecdd8.jpg) 尝试使用[线性相关](numerical_numerical.htm)发明一个新的KNN算法。'
