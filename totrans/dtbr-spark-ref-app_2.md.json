["```\n val tweetStream = TwitterUtils.createStream(ssc, Utils.getAuth)\n  .map(gson.toJson(_))\n\ntweetStream.foreachRDD((rdd, time) => {\n  val count = rdd.count()\n  if (count > 0) {\n    val outputRDD = rdd.repartition(partitionsEachInterval)\n    outputRDD.saveAsTextFile(\n      outputDirectory + \"/tweets_\" + time.milliseconds.toString)\n    numTweetsCollected += count\n    if (numTweetsCollected > numTweetsToCollect) {\n      System.exit(0)\n    }\n  }\n}) \n```", "```\n %  ${YOUR_SPARK_HOME}/bin/spark-submit \\\n     --class \"com.databricks.apps.twitter_classifier.Collect\" \\\n     --master ${YOUR_SPARK_MASTER:-local[4]} \\\n     target/scala-2.10/spark-twitter-lang-classifier-assembly-1.0.jar \\\n     ${YOUR_OUTPUT_DIR:-/tmp/tweets} \\\n     ${NUM_TWEETS_TO_COLLECT:-10000} \\\n     ${OUTPUT_FILE_INTERVAL_IN_SECS:-10} \\\n     ${OUTPUT_FILE_PARTITIONS_EACH_INTERVAL:-1} \\\n     --consumerKey ${YOUR_TWITTER_CONSUMER_KEY} \\\n     --consumerSecret ${YOUR_TWITTER_CONSUMER_SECRET} \\\n     --accessToken ${YOUR_TWITTER_ACCESS_TOKEN}  \\\n     --accessTokenSecret ${YOUR_TWITTER_ACCESS_SECRET} \n```", "```\nval tweets = sc.textFile(tweetInput)\nfor (tweet <- tweets.take(5)) {\n  println(gson.toJson(jsonParser.parse(tweet)))\n} \n```", "```\nval tweetTable = sqlContext.jsonFile(tweetInput)\ntweetTable.registerTempTable(\"tweetTable\")\ntweetTable.printSchema() \n```", "```\nsqlContext.sql(\n    \"SELECT text FROM tweetTable LIMIT 10\")\n    .collect().foreach(println) \n```", "```\nsqlContext.sql(\n    \"SELECT user.lang, user.name, text FROM tweetTable LIMIT 10\")\n    .collect().foreach(println) \n```", "```\nsqlContext.sql(\n    \"SELECT user.lang, COUNT(*) as cnt FROM tweetTable \" +\n    \"GROUP BY user.lang ORDER BY cnt DESC limit 1000\")\n    .collect.foreach(println) \n```", "```\nobject Utils {\n  ...\n\n  val numFeatures = 1000\n  val tf = new HashingTF(numFeatures)\n\n  /**\n   * Create feature vectors by turning each tweet into bigrams of\n   * characters (an n-gram model) and then hashing those to a\n   * length-1000 feature vector that we can pass to MLlib.\n   * This is a common way to decrease the number of features in a\n   * model while still getting excellent accuracy. (Otherwise every\n   * pair of Unicode characters would potentially be a feature.)\n   */\n  def featurize(s: String): Vector = {\n    tf.transform(s.sliding(2).toSeq)\n  }\n\n  ...\n} \n```", "```\nval texts = sqlContext.sql(\"SELECT text from tweetTable\").map(_.head.toString)\n// Caches the vectors since it will be used many times by KMeans.\nval vectors = texts.map(Utils.featurize).cache()\nvectors.count()  // Calls an action to create the cache.\nval model = KMeans.train(vectors, numClusters, numIterations)\nsc.makeRDD(model.clusterCenters, numClusters).saveAsObjectFile(outputModelDir) \n```", "```\nval some_tweets = texts.take(100)\nfor (i <- 0 until numClusters) {\n  println(s\"\\nCLUSTER $i:\")\n  some_tweets.foreach { t =>\n    if (model.predict(Utils.featurize(t)) == i) {\n      println(t)\n    }\n  }\n} \n```", "```\n%  ${YOUR_SPARK_HOME}/bin/spark-submit \\\n     --class \"com.databricks.apps.twitter_classifier.ExamineAndTrain\" \\\n     --master ${YOUR_SPARK_MASTER:-local[4]} \\\n     target/scala-2.10/spark-twitter-lang-classifier-assembly-1.0.jar \\\n     \"${YOUR_TWEET_INPUT:-/tmp/tweets/tweets*/part-*}\" \\\n     ${OUTPUT_MODEL_DIR:-/tmp/tweets/model} \\\n     ${NUM_CLUSTERS:-10} \\\n     ${NUM_ITERATIONS:-20} \n```", "```\nprintln(\"Initializing Streaming Spark Context...\")\nval conf = new SparkConf().setAppName(this.getClass.getSimpleName)\nval ssc = new StreamingContext(conf, Seconds(5))\n\nprintln(\"Initializing Twitter stream...\")\nval tweets = TwitterUtils.createStream(ssc, Utils.getAuth)\nval statuses = tweets.map(_.getText)\n\nprintln(\"Initializing the KMeans model...\")\nval model = new KMeansModel(ssc.sparkContext.objectFile[Vector](\n    modelFile.toString).collect())\n\nval filteredTweets = statuses\n  .filter(t => model.predict(Utils.featurize(t)) == clusterNumber)\nfilteredTweets.print() \n```", "```\n %  ${YOUR_SPARK_HOME}/bin/spark-submit \\\n     --class \"com.databricks.apps.twitter_classifier.Predict\" \\\n     --master ${YOUR_SPARK_MASTER:-local[4]} \\\n     target/scala-2.10/spark-twitter-lang-classifier-assembly-1.0.jar \\\n     ${YOUR_MODEL_DIR:-/tmp/tweets/model} \\\n     ${CLUSTER_TO_FILTER:-7} \\\n     --consumerKey ${YOUR_TWITTER_CONSUMER_KEY} \\\n     --consumerSecret ${YOUR_TWITTER_CONSUMER_SECRET} \\\n     --accessToken ${YOUR_TWITTER_ACCESS_TOKEN}  \\\n     --accessTokenSecret ${YOUR_TWITTER_ACCESS_SECRET} \n```"]