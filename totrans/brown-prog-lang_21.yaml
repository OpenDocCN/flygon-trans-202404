- en: 21Algorithms That Exploit State
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21利用状态的算法
- en: '|     [21.1 Disjoint Sets Redux](#%28part._union-find%29) |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|     [21.1 并查集再谈](#%28part._union-find%29) |'
- en: '|       [21.1.1 Optimizations](#%28part._.Optimizations%29) |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|       [21.1.1 优化](#%28part._.Optimizations%29) |'
- en: '|       [21.1.2 Analysis](#%28part._.Analysis%29) |'
  id: totrans-3
  prefs: []
  type: TYPE_TB
  zh: '|       [21.1.2 分析](#%28part._.Analysis%29) |'
- en: '|     [21.2 Set Membership by Hashing Redux](#%28part._hash-tables%29) |'
  id: totrans-4
  prefs: []
  type: TYPE_TB
  zh: '|     [21.2 哈希再谈](#%28part._hash-tables%29) |'
- en: '|       [21.2.1 Improving Access Time](#%28part._.Improving_.Access_.Time%29)
    |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '|       [21.2.1 提高访问时间](#%28part._.Improving_.Access_.Time%29) |'
- en: '|       [21.2.2 Better Hashing](#%28part._.Better_.Hashing%29) |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '|       [21.2.2 更好的哈希](#%28part._.Better_.Hashing%29) |'
- en: '|       [21.2.3 Bloom Filters](#%28part._.Bloom_.Filters%29) |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '|       [21.2.3 布隆过滤器](#%28part._.Bloom_.Filters%29) |'
- en: '|     [21.3 Avoiding Recomputation by Remembering Answers](#%28part._.Avoiding_.Recomputation_by_.Remembering_.Answers%29)
    |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '|     [21.3 通过记住答案避免重新计算](#%28part._.Avoiding_.Recomputation_by_.Remembering_.Answers%29)
    |'
- en: '|       [21.3.1 An Interesting Numeric Sequence](#%28part._.An_.Interesting_.Numeric_.Sequence%29)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|       [21.3.1 一个有趣的数列](#%28part._.An_.Interesting_.Numeric_.Sequence%29)
    |'
- en: '|         [21.3.1.1 Using State to Remember Past Answers](#%28part._.Using_.State_to_.Remember_.Past_.Answers%29)
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '|         [21.3.1.1 利用状态记住过去的答案](#%28part._.Using_.State_to_.Remember_.Past_.Answers%29)
    |'
- en: '|         [21.3.1.2 From a Tree of Computation to a DAG](#%28part._.From_a_.Tree_of_.Computation_to_a_.D.A.G%29)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|         [21.3.1.2 从计算树到DAG](#%28part._.From_a_.Tree_of_.Computation_to_a_.D.A.G%29)
    |'
- en: '|         [21.3.1.3 The Complexity of Numbers](#%28part._numbers-not-constant%29)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|         [21.3.1.3 数字的复杂性](#%28part._numbers-not-constant%29) |'
- en: '|         [21.3.1.4 Abstracting Memoization](#%28part._.Abstracting_.Memoization%29)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|         [21.3.1.4 抽象化记忆化](#%28part._.Abstracting_.Memoization%29) |'
- en: '|       [21.3.2 Edit-Distance for Spelling Correction](#%28part._levenshtein%29)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|       [21.3.2 拼写纠正的编辑距离](#%28part._levenshtein%29) |'
- en: '|       [21.3.3 Nature as a Fat-Fingered Typist](#%28part._smith-waterman%29)
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|       [21.3.3 自然作为一个肥手指的打字员](#%28part._smith-waterman%29) |'
- en: '|       [21.3.4 Dynamic Programming](#%28part._.Dynamic_.Programming%29) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|       [21.3.4 动态规划](#%28part._.Dynamic_.Programming%29) |'
- en: '|         [21.3.4.1 Catalan Numbers with Dynamic Programming](#%28part._.Catalan_.Numbers_with_.Dynamic_.Programming%29)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|         [21.3.4.1 动态规划求解卡塔兰数](#%28part._.Catalan_.Numbers_with_.Dynamic_.Programming%29)
    |'
- en: '|         [21.3.4.2 Levenshtein Distance and Dynamic Programming](#%28part._.Levenshtein_.Distance_and_.Dynamic_.Programming%29)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|         [21.3.4.2 编辑距离与动态规划](#%28part._.Levenshtein_.Distance_and_.Dynamic_.Programming%29)
    |'
- en: '|       [21.3.5 Contrasting Memoization and Dynamic Programming](#%28part._memo-vs-dp%29)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|       [21.3.5 记忆化和动态规划的对比](#%28part._memo-vs-dp%29) |'
- en: 21.1Disjoint Sets Redux
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.1并查集再谈
- en: Here’s how we can use this to implement union-find afresh. We will try to keep
    things as similar to the previous version ([Checking Component Connectedness](graphs.html#%28part._union-find-functional%29))
    as possible, to enhance comparison.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何使用这个来重新实现并查集。我们将尽量保持与之前版本 ([检查组件连接性](graphs.html#%28part._union-find-functional%29))
    尽可能相似，以便进行比较。
- en: 'First, we have to update the definition of an element, making the parent field
    be mutable:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须更新元素的定义，使父节点字段可变：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To determine whether two elements are in the same set, we will still rely on
    fynd. However, as we will soon see, fynd no longer needs to be given the entire
    set of elements. Because the only reason is-in-same-set consumed that set was
    to pass it on to fynd, we can remove it from here. Nothing else changes:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定两个元素是否属于同一个集合，我们仍然依赖于查找操作。然而，正如我们很快会看到的那样，查找操作不再需要整个元素集合。因为确定两个元素是否属于同一个集合的唯一原因是为了将其传递给查找操作，所以我们可以从这里将其删除。除此之外，没有其他变化：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Updating is now the crucial difference: we use mutation to change the value
    of the parent:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在更新已经成为关键的区别：我们使用突变来改变父节点的值：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In parent: some(parent), the first parent is the name of the field, while the
    second one is the parameter name. In addition, we must use some to satisfy the
    option type. Naturally, it is not none because the entire point of this mutation
    is to change the parent to be the other element, irrespective of what was there
    before.Given this definition, union also stays largely unchanged, other than the
    change to the return type. Previously, it needed to return the updated set of
    elements; now, because the update is performed by mutation, there is no longer
    any need to return anything:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 parent：some(parent) 中，第一个父元素是字段的名称，而第二个父元素是参数的名称。此外，我们必须使用 some 来满足选项类型。自然，它不是
    none，因为这种变异的整个目的是将父元素更改为其他元素，而不管之前是什么。在给定这个定义后，联合操作也基本保持不变，除了返回类型的变化。以前，它需要返回更新后的元素集；现在，因为更新是通过变异执行的，所以不再需要返回任何东西：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, fynd. Its implementation is now remarkably simple. There is no longer
    any need to search through the set. Previously, we had to search because after
    union operations have occurred, the parent reference might have no longer been
    valid. Now, any such changes are automatically reflected by mutation. Hence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，fynd。它的实现现在非常简单。不再需要搜索集合。以前，我们必须搜索，因为联合操作发生后，父引用可能不再有效。现在，任何这样的变化都会自动反映在变异中。因此：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 21.1.1Optimizations
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.1.1 优化
- en: Look again at fynd. In the some case, the element bound to e is not the set
    name; that is obtained by recursively traversing parent references. As this value
    returns, however, we don’t do anything to reflect this new knowledge! Instead,
    the next time we try to find the parent of this element, we’re going to perform
    this same recursive traversal all over again.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看看 fynd。在某些情况下，与 e 绑定的元素不是集合名称；这是通过递归遍历父引用获得的。然而，当这个值返回时，我们却没有做任何事情来反映这个新知识！相反，下次我们尝试找到这个元素的父元素时，我们将再次执行相同的递归遍历。
- en: 'Using mutation helps address this problem. The idea is as simple as can be:
    compute the value of the parent, and update it.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变异有助于解决这个问题。这个想法非常简单：计算父元素的值，并更新它。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that this update will apply to every element in the recursive chain to
    find the set name. Therefore, applying fynd to any of those elements the next
    time around will benefit from this update. This idea is called path compression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此更新将应用于递归链中的每个元素，以找到集合名称。因此，下次应用 fynd 到其中任何一个元素时都将受益于此更新。这个想法被称为路径压缩。
- en: There is one more interesting idea we can apply. This is to maintain a rank
    of each element, which is roughly the depth of the tree of elements for which
    that element is their set name. When we union two elements, we then make the one
    with larger rank the parent of the one with the smaller rank. This has the effect
    of avoiding growing very tall paths to set name elements, instead tending towards
    “bushy” trees. This too reduces the number of parents that must be traversed to
    find the representative.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以应用另一个有趣的想法。这就是维护每个元素的等级，它大致是该元素的元素树的深度，这个树的元素是它们的集合名称。当我们联合两个元素时，我们将等级较大的元素作为等级较小的元素的父元素。这样做的效果是避免使通向集合名称元素的路径变得非常长，而是倾向于“树状”树。这也减少了必须遍历以找到代表的父元素的数量。
- en: 21.1.2Analysis
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.1.2 分析
- en: This optimized union-find data structure has a remarkble analysis. In the worst
    case, of course, we must traverse the entire chain of parents to find the name
    element, which takes time proportional to the number of elements in the set. However,
    once we apply the above optimizations, we never need to traverse that same chain
    again! In particular, if we conduct an amortized analysis over a sequence of set
    equality tests after a collection of union operations, we find that the cost for
    subsequent checks is very small—<wbr>indeed, about as small a function can get
    without being constant. The [actual analysis](http://en.wikipedia.org/wiki/Disjoint-set_data_structure)
    is quite sophisticated; it is also one of the most remarkable algorithm analyses
    in all of computer science.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个优化后的并查集数据结构有着非凡的分析。在最坏的情况下，当然，我们必须遍历整个父链才能找到名称元素，这需要的时间与集合中的元素数量成正比。然而，一旦我们应用了上述优化，我们就再也不需要再次遍历同样的链了！特别是，如果我们对一系列集合相等性测试进行摊销分析，然后进行一系列的联合操作，我们发现后续检查的成本非常小——实际上，几乎是一个函数可以达到的最小值，而不是常数。[实际分析](http://en.wikipedia.org/wiki/Disjoint-set_data_structure)非常复杂；它也是计算机科学中最显著的算法分析之一。
- en: 21.2Set Membership by Hashing Redux
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.2 通过哈希重新设置成员资格
- en: 'We have already seen solutions to set membership. First we saw how to represent
    sets as lists ([Representing Sets by Lists](set-representations.html#%28part._rep-sets-as-lists%29)),
    then as (balanced) binary trees ([A Fine Balance: Tree Surgery](set-representations.html#%28part._sets-from-balanced-trees%29)).Don’t
    confuse this with union-find, which is a different kind of problem on sets ([Disjoint
    Sets Redux](#%28part._union-find%29)). With this we were able to reduce insertion
    and membership to logarithmic time in the number of elements. Along the way, we
    also learned that the essence of using these representations was to reduce any
    datatype to a comparable, ordered element—<wbr>for efficiency, usually a number
    ([Converting Values to Ordered Values](set-representations.html#%28part._hashing-values%29))—<wbr>which
    we called hashing.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了关于集合成员资格的解决方案。首先，我们看到了如何将集合表示为列表（[通过列表表示集合](set-representations.html#%28part._rep-sets-as-lists%29)），然后是作为（平衡的）二叉树（[一个很好的平衡：树手术](set-representations.html#%28part._sets-from-balanced-trees%29)）。不要将此与并查集混淆，后者是关于集合的不同类型的问题（[不相交集合
    Redux](#%28part._union-find%29)）。通过这样做，我们能够将插入和成员资格降低到元素数量的对数时间。在这个过程中，我们还学会了使用这些表示的本质是将任何数据类型转换为可比较的、有序的元素—为了效率，通常是一个数字（[将值转换为有序值](set-representations.html#%28part._hashing-values%29））—我们称之为哈希。
- en: Let us now ask whether we can use these numbers in any other way. Suppose our
    set has only five elements, which map densely to the values between 0 and 4\.
    We can then have a five element list of boolean values, where the boolean at each
    index of the list indicates whether the element corresponding to that position
    is in the set or not. Both membership and insertion, however, require traversing
    potentially the entire list, giving us solutions linear in the number of elements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们问一下，我们是否可以以其他方式使用这些数字。假设我们的集合只有五个元素，它们在0到4之间的值密集映射。然后我们可以有一个五元素的布尔值列表，列表中每个索引处的布尔值表示该位置对应的元素是否在集合中。但是，成员资格和插入都需要遍历可能的整个列表，这给我们带来了线性的解决方案，因为它们是基于元素数量的。
- en: That’s not all. Unless we can be certain that there will be only five elements,
    we can’t be sure to bound the size of the representation. Also, we haven’t yet
    shown how to actually hash in a way that makes the representation dense; barring
    that, our space consumption gets much worse, in turn affecting time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不是全部。除非我们能确定只有五个元素，否则我们无法确保限制表示的大小。此外，我们还没有展示如何以使表示密集的方式进行哈希; 如果没有这样做，我们的空间消耗会变得更糟，进而影响时间。
- en: 'There is, actually, a relatively simple solution to the problem of reducing
    numbers densely to a range: given the hash, we apply modular arithmetic. That
    is, if we want to use a list of five elements to represent the set, we simply
    compute the hash’s modulo five. This gives us an easy solution to that problem.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有一个相对简单的解决方案可以将数字密集地减少到一个范围内：给定哈希，我们应用模运算。也就是说，如果我们想要使用一个五元素列表来表示集合，我们只需计算哈希的模五。这给了我们一个简单的解决方案。
- en: 'Except, of course, not quite: two different hashes could easily have the same
    modulus. That is, suppose we need to record that the set contains the (hash) value
    5; the resulting list would be'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，不完全是这样：两个不同的哈希很容易具有相同的模数。也就是说，假设我们需要记录集合包含（哈希）值5; 结果列表将是
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now suppose we want to ask whether the value 15 is in the set; we cannot tell
    from this representation whether it’s in the set or not, because we can’t tell
    whether the true represents 5, 15, 25, or any other value whose modulus 5 is 0\.
    Therefore, we have to record the actual elements in the set:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想要询问值15是否在集合中；我们无法从这个表示中得知它是否在集合中，因为我们无法确定真正表示的是5、15、25还是任何其他模5为0的值。因此，我们必须记录集合中的实际元素：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we can tell that 5 is in the set while 4 is not. However, this now makes
    it impossible to have both 5 and 10 in the set; therefore, our real representation
    needs to be a list at each position:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以知道5在集合中而4不在。然而，现在这使得在集合中同时拥有5和10变得不可能; 因此，我们真正的表示需要在每个位置上都是一个列表：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we also add 10 to the set, we get:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还把10添加到集合中，我们会得到：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: and now we can tell that both 5 and 10 are in the set, but 15 is not. These
    sub-lists are known as buckets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以知道5和10都在集合中，但15不在。这些子列表被称为桶。
- en: Good; now we have another way of representing sets so we can check for membership.
    However, in the worst case one of those lists is going to contain all elements
    in the set, and we may have to traverse the entire list to find an element in
    it, which means membership testing will take time linear in the number of elements.
    Insertion, in turn, takes time proportional to the size of the modulus because
    we may have to traverse the entire outer list to get to the right sub-list.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 很好；现在我们有了另一种表示集合的方式，所以我们可以检查成员资格。但是，在最坏的情况下，其中一个列表将包含集合中的所有元素，我们可能必须遍历整个列表才能在其中找到一个元素，这意味着成员资格测试将花费与元素数量成正比的时间。反过来，插入的时间与模的大小成正比，因为我们可能必须遍历整个外部列表才能到达正确的子列表。
- en: Can we improve on this?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能改进这个吗？
- en: 21.2.1Improving Access Time
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.2.1改进访问时间
- en: Given that we currently have no way of ensuring we won’t get hash collisions,
    for now we’re stuck with a list of elements at each position that could be the
    size of the set we are trying to represent. Therefore, we can’t get around that
    (yet). But, we’re currently paying time in the size of the outer list just to
    insert an element, and surely we can do better than that!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们目前没有办法确保不会发生哈希碰撞，目前我们被困在每个位置都可能是要表示的集合大小的元素列表中。因此，我们无法绕过这一点（至少目前如此）。但是，我们目前付出的时间在外部列表的大小上只是为了插入一个元素，肯定还有更好的方法！
- en: 'We can, but it requires a different data structure: the array.There are other
    data structures that will also do better, but the one we’re about to see is important
    and widely used. You can look up arrays in the Pyret documentation. The key characteristics
    of an array are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以，但需要不同的数据结构：数组。还有其他数据结构也可以做得更好，但我们即将看到的这个是重要且广泛使用的。您可以在 Pyret 文档中查找数组。数组的关键特征是：
- en: Accessing the nth element of an array takes constant, not linear, time in n.
    This is sometimes known as random-access, because it takes the same time to access
    any random element, as opposed to just a known element.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问数组的第 n 个元素花费的时间是常量，而不是线性的。这有时被称为随机访问，因为访问任何随机元素所花费的时间相同，而不仅仅是已知元素。
- en: Arrays are updated by mutation. Thus, a change to an array is seen by all references
    to the array.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数组通过突变进行更新。因此，对数组的更改被所有对数组的引用看到。
- en: 'The former property warrants some discussion: how can an array provide random
    access whereas a list requires time linear in the index of the element we’re accessing?
    This is because of a trade-off: a list can be extended indefinitely as the program
    extends, but an array cannot. An array must declare its size up front, and cannot
    grow without copying all the elements into a larger array. Therefore, we should
    only use arrays when we have a clearly identifiable upper-bound on their size
    (and that bound is not too large, or else we may not even be able to find that
    much contiguous space in the system). But the problem we’re working on has exactly
    this characteristic.So let’s try defining sets afresh. We start with an array
    of a fixed size, with each element an empty list:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的性质值得讨论一下：一个数组如何提供随机访问，而列表需要线性时间来访问我们正在访问的元素的索引？这是因为一种权衡：列表可以随着程序的扩展而无限扩展，但是数组不能。数组必须在前面声明其大小，并且不能在没有将所有元素复制到较大数组中的情况下增长。因此，我们应该仅在对其大小具有明确可识别的上限时使用数组（而且该上限不太大，否则我们甚至可能找不到系统中那么多的连续空间）。但是我们正在解决的问题恰好具有这种特征。因此，让我们尝试重新定义集合。我们从具有固定大小的数组开始，其中每个元素都是一个空列表：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We need to use modular arithmetic to find the right bucket:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用模运算找到正确的桶：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With this, we can determine whether an element is in the set:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们可以确定一个元素是否在集合中：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To actually add an element to the set, we put it in the list associated with
    the appropriate bucket:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际将元素添加到集合中，我们将其放入与适当桶相关联的列表中：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Checking whether the element is already in the bucket is an important part of
    our complexity argument because we have implicitly assumed there won’t be duplicate
    elements in buckets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 检查元素是否已在桶中是我们复杂性论证的重要部分，因为我们隐含地假设桶中不会有重复元素。
- en: Exercise
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What impact do duplicate elements have on the complexity of operations?
  id: totrans-72
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重复元素对操作的复杂性有什么影响？
- en: The data structure we have defined above is known as a hash table (which is
    a slightly confusing name, because it isn’t really a table of hashes, but this
    is the name used conventionally in computer science).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面定义的数据结构称为哈希表（这是一个稍微令人困惑的名称，因为它实际上不是一个哈希表，但这是计算机科学中惯用的名称）。
- en: 21.2.2Better Hashing
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.2.2更好的哈希
- en: 'Using arrays therefore appears to address one issue: insertion. Finding the
    relevant bucket takes constant time, linking the new element takes constant time,
    and so the entire operation takes constant time...except, we have to also check
    whether the element is already in the bucket, to avoid storing duplicates. We
    have gotten rid of the traversal through the outer list representing the set,
    but the member operation on the inner list remains unchanged. In principle it
    won’t, but in practice we can make it much better.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数组似乎解决了一个问题：插入。找到相关的桶需要常量时间，链接新元素需要常量时间，因此整个操作需要常量时间……除非，我们还必须检查元素是否已经在桶中，以避免存储重复项。我们已经摆脱了遍历代表集合的外部列表，但是对内部列表的成员操作保持不变。原则上不会发生变化，但在实践中我们可以做得更好。
- en: Note that collisions are virtually inevitable. If we have uniformly distributed
    data, then collisions show up sooner than we might expect.This follows from the
    reasoning behind what is known as the [birthday problem](http://en.wikipedia.org/wiki/Birthday_problem),
    commonly presented as how many people need to be in a room before the likelihood
    that two of them share a birthday exceeds some percentage. For the likelihood
    to exceed half we need just 23 people! Therefore, it is wise to prepare for the
    possibility of collisions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，碰撞几乎是不可避免的。如果我们有均匀分布的数据，那么碰撞会比我们预期的要早出现。这是由于所谓的[生日问题](http://en.wikipedia.org/wiki/Birthday_problem)背后的推理，通常被表述为在房间里需要多少人才能使其中两个人共享生日的可能性超过某个百分比。为了使可能性超过一半，我们只需要23个人！因此，明智的做法是为碰撞的可能性做好准备。
- en: The key is to know something about the distribution of hash values. For instance,
    if we knew our hash values are all multiples of 10, then using a table size of
    10 would be a terrible idea (because all elements would hash to the same bucket,
    turning our hash table into a list). In practice, it is common to use uncommon
    prime numbers as the table size, since a random value is unlikely to have it as
    a divisor. This does not yield a theoretical improvement (unless you can make
    certain assumptions about the input, or work through the math very carefully),
    but it works well in practice. In particular, since the typical hashing function
    uses memory addresses for objects on the heap, and on most systems these addresses
    are multiples of 4, using a prime like 31 is often a fairly good bet.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要了解散列值的分布情况。例如，如果我们知道我们的散列值都是10的倍数，那么使用大小为10的表是一个糟糕的主意（因为所有元素都会散列到同一个桶中，将我们的散列表变成一个列表）。在实践中，通常使用不常见的素数作为表的大小，因为随机值不太可能具有它作为除数。这并不会产生理论上的改进（除非你可以对输入进行某些假设，或者非常仔细地进行数学推导），但在实践中效果很好。特别是，由于典型的散列函数使用堆上对象的内存地址，而大多数系统上这些地址都是4的倍数，因此使用31这样的素数通常是一个相当好的选择。
- en: 21.2.3Bloom Filters
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.2.3Bloom Filters
- en: Another way to improve the space and time complexity is to relax the properties
    we expect of the operations. Right now, set membership gives perfect answers,
    in that it answers true exactly when the element being checked was previously
    inserted into the set. But suppose we’re in a setting where we can accept a more
    relaxed notion of correctness, where membership tests can “lie” slightly in one
    direction or the other (but not both, because that makes the representation almost
    useless). Specifically, let’s say that “no means no” (i.e., if the set representation
    says the element isn’t present, it really isn’t) but “yes sometimes means no”
    (i.e., if the set representation says an element is present, sometimes it might
    not be). In short, if the set says the element isn’t in it, this should be guaranteed;
    but if the set says the element is present, it may not be. In the latter case,
    we either need some other—<wbr>more expensive—<wbr>technique to determine truth,
    or we might just not care.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种改善空间和时间复杂度的方法是放宽我们对操作期望的属性。现在，集合成员关系给出了完美的答案，即当被检查的元素先前插入到集合中时，它准确地回答true。但是假设我们处于一个可以接受更放松的正确性概念的环境中，在这种情况下，成员关系测试可能会在一定程度上“撒谎”（但不是两者都，因为这使得表示几乎没有用处）。具体来说，让我们说“不意味着没有”（即，如果集合表示说元素不存在，则确实不存在），但“是有时意味着没有”（即，如果集合表示一个元素存在，则有时可能不存在）。简而言之，如果集合说元素不在其中，这应该是有保证的；但是如果集合说元素存在，则可能不存在。在后一种情况下，我们需要一些其他——更昂贵的——技术来确定真相，或者我们可能根本不在乎。
- en: Where is such a data structure of use? Suppose we are building a Web site that
    uses password-based authentication. Because many passwords have been leaked in
    well-publicized breaches, it is safe to assume that hackers have them and will
    guess them. As a result, we want to not allow users to select any of these as
    passwords. We could use a hash-table to reject precisely the known leaked passwords.
    But for efficiency, we could use this imperfect hash instead. If it says “no”,
    then we allow the user to use that password. But if it says “yes”, then either
    they are using a password that has been leaked, or they have an entirely different
    password that, purely by accident, has the same hash value, but no matter; we
    can just disallow that password as well.A related use is for filtering out malicious
    Web sites. The URL shortening system, bitly, [uses it for this purpose](http://word.bitly.com/post/28558800777/dablooms-an-open-source-scalable-counting-bloom).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据结构在哪里有用？假设我们正在构建一个使用基于密码的身份验证的网站。由于许多密码在广为宣传的泄露中被泄露，可以安全地假设黑客已经获取了它们并会猜测它们。因此，我们不希望允许用户选择任何这些密码作为密码。我们可以使用哈希表来精确拒绝已知的泄露密码。但为了效率，我们可以使用这个不完美的哈希。如果它说“不”，那么我们允许用户使用该密码。但如果它说“是”，那么要么他们正在使用一个已泄露的密码，要么他们有一个完全不同的密码，纯粹是偶然的，具有相同的哈希值，但无论如何；我们可以禁止那个密码。一个相关的用途是用于过滤恶意网站。URL
    缩短系统 bitly，[用于此目的](http://word.bitly.com/post/28558800777/dablooms-an-open-source-scalable-counting-bloom)。
- en: 'Another example is in updating databases or memory stores. Suppose we have
    a database of records, which we update frequently. It is often more efficient
    to maintain a journal of changes: i.e., a list that sequentially records all the
    changes that have occurred. At some interval (say overnight), the journal is “flushed”,
    meaning all these changes are applied to the database proper. But that means every
    read operation has become highly inefficient, because it has to check the entire
    journal first (for updates) before accessing the database. Again, here we can
    use this faulty notion of a hash table: if the hash of the record locator says
    “no”, then the record certainly hasn’t been modified and we go directly to the
    database; if it says “yes” then we have to check the journal.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是更新数据库或内存存储。假设我们有一个记录的数据库，我们经常更新。通常更有效的方法是维护一份更改日志：即，按顺序记录所有已发生的更改的列表。在某个间隔（比如说每夜），日志被“刷新”，意味着所有这些更改都被应用到数据库中。但这意味着每个读取操作变得非常低效，因为它必须先检查整个日志（以查找更新）再访问数据库。同样，在这里我们可以使用这个错误的哈希表概念：如果记录定位器的哈希说“不”，那么记录肯定没有被修改，我们直接访问数据库；如果它说“是”，那么我们必须检查日志。
- en: 'We have already seen a simple example implementation of this idea earlier,
    when we used a single list (or array) of booleans, with modular arithmetic, to
    represent the set. When the set said 4 was not present, this was absolutely true;
    but when it said 5 and 10 are both present, only one of these was present. The
    advantage was a huge saving in space and time: we needed only one bit per bucket,
    and did not need to search through a list to answer for membership. The downside,
    of course, was a hugely inaccurate set data structure, and one with correlated
    failure tied to the modulus.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在此前看过这个想法的一个简单示例实现，当时我们使用了一个布尔值的单个列表（或数组），带有模运算，来表示集合。当集合说 4 不在时，这是绝对正确的；但当它说
    5 和 10 都存在时，只有其中一个存在。优点是节省了大量的空间和时间：我们只需要每个桶一个位，并且不需要搜索列表来回答成员资格。当然，缺点是一个极不准确的集合数据结构，以及与模数相关的失败。
- en: 'There is a simple way to improve this solution: instead of having just one
    array, have several (but a fixed number of them). When an element is added to
    the set, it is added to each array; when checking for membership, every array
    is consulted. The set only answers affirmatively to membership if all the arrays
    do so.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种简单的方法可以改进这个解决方案：不再只有一个数组，而是有几个（但数量固定）。当一个元素被添加到集合中时，它被添加到每个数组中；在检查成员资格时，要查看每个数组。只有当所有数组都同意时，集合才会肯定地回答成员资格。
- en: 'Naturally, using multiple arrays offers absolutely no advantage if the arrays
    are all the same size: since both insertion and lookup are deterministic, all
    will yield the same answer. However, there is a simple antidote to this: use different
    array sizes. In particular, by using array sizes that are relatively prime to
    one another, we minimize the odds of a clash (only hashes that are the product
    of all the array sizes will fool the array).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，如果多个数组的大小都相同，则使用多个数组不会提供任何优势：由于插入和查找都是确定性的，所有数组都将得到相同的答案。然而，有一个简单的解决方法：使用不同的数组大小。特别地，通过使用彼此相对质数的数组大小，我们最小化了冲突的可能性（只有所有数组大小的乘积才会欺骗数组）。
- en: This data structure, called a Bloom Filter, is a probabilistic data structure.
    Unlike our earlier set data structure, this one is not guaranteed to always give
    the right answer; but contrary to the [☛ space-time tradeoff](glossary.html#%28elem._glossary-space-time._tradeoff%29),
    we save both space and time by changing the problem slightly to accept incorrect
    answers. If we know something about the distribution of hash values, and we have
    some acceptable bound of error, we can design hash table sizes so that with high
    probability, the Bloom Filter will lie within the acceptable error bounds.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据结构，称为布隆过滤器，是一种概率性数据结构。与我们之前的集合数据结构不同，这个数据结构不能保证始终给出正确的答案；但与[☛时空权衡](glossary.html#%28elem._glossary-space-time._tradeoff%29)相反，通过稍微改变问题接受不正确的答案，我们既节省了空间又节省了时间。如果我们对哈希值的分布有所了解，并且有一些可接受的误差范围，我们可以设计哈希表大小，以便在很大概率下，布隆过滤器将在可接受的误差范围内。
- en: 21.3Avoiding Recomputation by Remembering Answers
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 21.3通过记忆答案避免重新计算
- en: We have on several instances already referred to a [☛ space-time tradeoff](glossary.html#%28elem._glossary-space-time._tradeoff%29).
    The most obvious tradeoff is when a computation “remembers” prior results and,
    instead of recomputing them, looks them up and returns the answers. This is an
    instance of the tradeoff because it uses space (to remember prior answers) in
    place of time (recomputing the answer). Let’s see how we can write such computations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在几个实例中提到了[☛时空权衡](glossary.html#%28elem._glossary-space-time._tradeoff%29)。最明显的权衡是当计算“记住”先前的结果，并且不重新计算它们，而是查找它们并返回答案时。这是权衡的一个例子，因为它使用空间（记住先前的答案）来代替时间（重新计算答案）。让我们看看如何编写这样的计算。
- en: 21.3.1An Interesting Numeric Sequence
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.1一个有趣的数字序列
- en: Suppose we want to create properly-parenthesized expressions, and ignore all
    non-parenthetical symbols. How many ways are there of creating parenthesized expressions
    given a certain number of opening (equivalently, closing) parentheses?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要创建正确括号表达式，并忽略所有非括号符号。给定一定数量的开括号（或者相同数量的闭括号），创建括号表达式的方式有多少种？
- en: If we have zero opening parentheses, the only expression we can create is the
    empty expression. If we have one opening parenthesis, the only one we can construct
    is “()” (there must be a closing parenthesis since we’re interested only in properly-parenthesized
    expressions). If we have two opening parentheses, we can construct “(())” and
    “()()”. Given three, we can construct “((()))”, “(())()”, “()(())”, “()()()”,
    and “(()())”, for a total of five. And so on. Observe that the solutions at each
    level use all the possible solutions at one level lower, combined in all the possible
    ways.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有开括号，我们唯一能够创建的表达式是空表达式。如果我们有一个开括号，我们唯一能够构建的是“()”（必须有一个闭括号，因为我们只关心正确括号的表达式）。如果我们有两个开括号，我们可以构建“(())”和“()()”。给定三个，我们可以构建“((()))”、“(())()”、“()(())”、“()()()”和“(()())”，总共五个。以此类推。观察到每个级别的解决方案都使用了低一级别的所有可能解决方案，并以所有可能的方式组合。
- en: 'There is actually a famous mathematical sequence that corresponds to the number
    of such expressions, called the [Catalan sequence](http://en.wikipedia.org/wiki/Catalan_number).
    It has the property of growing quite large very quickly: starting from the modest
    origins above, the tenth Catalan number (i.e., tenth element of the Catalan sequence)
    is 16796\. A simple recurrence formula gives us the Catalan number, which we can
    turn into a simple program:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有一个与这些表达式数量对应的著名数学序列，称为[卡塔兰数列](http://en.wikipedia.org/wiki/Catalan_number)。它具有快速增长的特性：从上述谦逊的起源开始，第十个卡塔兰数（即卡塔兰数列的第十个元素）为16796。一个简单的递推公式给出了卡塔兰数，我们可以将其转换为简单的程序：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This function’s tests look as follows—<wbr><catalan-tests> ::=
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的测试如下：<wbr><catalan-tests> ::=
- en: '|   check: |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|   check: |'
- en: '|     catalan(0) is 1 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(0) is 1 |'
- en: '|     catalan(1) is 1 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(1) is 1 |'
- en: '|     catalan(2) is 2 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(2) 是 2 |'
- en: '|     catalan(3) is 5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(3) 是 5 |'
- en: '|     catalan(4) is 14 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(4) 是 14 |'
- en: '|     catalan(5) is 42 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(5) 是 42 |'
- en: '|     catalan(6) is 132 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(6) 是 132 |'
- en: '|     catalan(7) is 429 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(7) 是 429 |'
- en: '|     catalan(8) is 1430 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(8) 是 1430 |'
- en: '|     catalan(9) is 4862 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(9) 是 4862 |'
- en: '|     catalan(10) is 16796 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(10) 是 16796 |'
- en: '|     catalan(11) is 58786 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|     catalan(11) 是 58786 |'
- en: '|   end |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: but beware! When we time the function’s execution, we find that the first few
    tests run very quickly, but somewhere between a value of 10 and 20—<wbr>depending
    on your machine and programming language implementation—<wbr>you ought to see
    things start to slow down, first a little, then with extreme effect.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但要注意！当我们计算函数的执行时间时，我们发现前几个测试运行速度非常快，但在值在10到20之间的某处—<wbr>取决于您的机器和编程语言实现—<wbr>您应该看到事情开始变慢，一开始是一点点，然后效果极其明显。
- en: Do Now!
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在做！
- en: ''
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Check at what value you start to observe a significant slowdown on your machine.
    Plot the graph of running time against input size. What does this suggest?
  id: totrans-111
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看您的计算机在何时开始观察到明显的减速。绘制运行时间与输入大小的图表。这意味着什么？
- en: 'The reason the Catalan computation takes so long is precisely because of what
    we alluded to earlier: at each level, we depend on computing the Catalan number
    of all the smaller levels; this computation in turn needs the numbers of all of
    its smaller levels; and so on down the road.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所暗示的那样，使Catalan计算如此耗时的原因正是因为在每个级别，我们依赖于计算所有较小级别的Catalan数；这个计算反过来又需要它所有较小级别的数字；依此类推。
- en: Exercise
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Map the subcomputations of catalan to see why the computation time explodes
    as it does. What is the asymptotic time complexity of this function?
  id: totrans-115
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 映射catalan的子计算，以了解计算时间为何会爆炸。这个函数的渐近时间复杂度是多少？
- en: 21.3.1.1Using State to Remember Past Answers
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.1.1利用状态记住以前的答案
- en: Therefore, this is clearly a case where trading space for time is likely to
    be of help. How do we do this? We need a notion of memory that records all previous
    answers and, on subsequent attempts to compute them, checks whether they are already
    known and, if so, just returns them instead of recomputing them.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这显然是一种通过交换空间换取时间可能有所帮助的情况。我们如何做到这一点？我们需要一种记住所有先前答案并在后续尝试计算它们时检查它们是否已知的内存概念，如果是，只返回它们而不是重新计算它们。
- en: Do Now!
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在做！
- en: ''
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What critical assumption is this based on?
  id: totrans-120
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是基于什么关键假设？
- en: Naturally, this assumes that for a given input, the answer will always be the
    same. As we have seen, functions with state violate this liberally, so typical
    stateful functions cannot utilize this optimization. Ironically, we will use state
    to implement this optimization, so we will have a stateful function that always
    returns the same answer on a given input—<wbr>and thereby use state in a stateful
    function to simulate a stateless one. Groovy, dude!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这假设对于给定的输入，答案将始终相同。正如我们所见，具有状态的函数大量违反了这一假设，因此典型的有状态函数无法利用这种优化。具有讽刺意味的是，我们将使用状态来实现这种优化，因此我们将有一个有状态的函数，它在给定的输入上始终返回相同的答案—<wbr>从而使用状态在有状态的函数中模拟无状态的函数。酷毙了，伙计！
- en: 'First, then, we need some representation of memory. We can imagine several,
    but here’s a simple one:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一些内存的表示。我们可以想象几种，但这里有一个简单的：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now how does catalan need to change? We have to first look for whether the
    value is already in memory; if it is, we return it without any further computation,
    but if it isn’t, then we compute the result, store it in memory, and then return
    it:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如何修改catalan？我们首先要查看值是否已经存在于内存中；如果存在，则无需进一步计算，直接返回它，但如果不存在，则计算结果，将其存储在内存中，然后返回它：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: And that’s it! Now running our previous tests will reveal that the answer computes
    much quicker, but in addition we can dare to run bigger computations such as catalan(50).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在运行我们以前的测试会发现答案计算速度更快，但另外我们可以尝试运行更大的计算，比如catalan(50)。
- en: This process, of converting a function into a version that remembers its past
    answers, is called memoization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程，将一个函数转换为记住其过去答案的版本，称为记忆化。
- en: 21.3.1.2From a Tree of Computation to a DAG
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.1.2从计算树到DAG
- en: What we have subtly done is to convert a tree of computation into a DAG over
    the same computation, with equivalent calls being reused. Whereas previously each
    call was generating lots of recursive calls, which induced still more recursive
    calls, now we are reusing previous recursive calls—<wbr>i.e., sharing the results
    computed earlier. This, in effect, points the recursive call to one that had occurred
    earlier. Thus, the shape of computation converts from a tree to a DAG of calls.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们巧妙地做了一件事，那就是将计算树转换为相同计算的DAG，重复使用等价调用。而以前每个调用都会生成大量递归调用，这些调用又会导致更多的递归调用，现在我们重新使用以前的递归调用—<wbr>即，共享先前计算的结果。这实际上是将递归调用指向之前发生的递归调用。因此，计算形状从树转换为调用的DAG。
- en: This has an important complexity benefit. Whereas previously we were performing
    a super-exponential number of calls, now we perform only one call per input and
    share all previous calls—<wbr>thereby reducing catalan(n) to take a number of
    fresh calls proportional to n. Looking up the result of a previous call takes
    time proportional to the size of memory (because we’ve represented it as a list;
    better representations would improve on that), but that only contributes another
    linear multiplicative factor, reducing the overall complexity to quadratic in
    the size of the input. This is a dramatic reduction in overall complexity. In
    contrast, other uses of memoization may result in much less dramatic improvements,
    turning the use of this technique into a true engineering trade-off.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有重要的复杂性优势。而以前我们执行了超指数数量的调用，现在我们每个输入只执行一次调用，并共享所有以前的调用—<wbr>从而将catalan(n)减少到与n成比例的新调用数量。查找以前调用的结果需要与内存大小成比例的时间（因为我们将其表示为列表；更好的表示方法会改善这一点），但这只会增加另一个线性乘法因子，将整体复杂性降低到与输入大小的二次方成比例。这是整体复杂性的显著降低。相比之下，记忆化的其他用途可能导致改进远不及这么明显，将该技术的使用转变为真正的工程权衡。
- en: 21.3.1.3The Complexity of Numbers
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.1.3数字的复杂性
- en: As we start to run larger computations, however, we may start to notice that
    our computations are starting to take longer than linear growth. This is because
    our numbers are growing arbitrarily large—<wbr>for instance, catalan(100) is 896519947090131496687170070074100632420837521538745909320—<wbr>and
    computations on numbers can no longer be constant time, contrary to what we said
    earlier ([The Size of the Input](predicting-growth.html#%28part._size-of-input%29)).
    Indeed, when working on cryptographic problems, the fact that operations on numbers
    do not take constant time are absolutely critical to fundamental complexity results
    (and, for instance, the presumed unbreakability of contemporary cryptography).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们开始运行更大的计算，我们可能会注意到我们的计算所花费的时间开始超过线性增长。这是因为我们的数字正在任意变大—<wbr>例如，catalan(100)是896519947090131496687170070074100632420837521538745909320—<wbr>并且对数字的计算不再是常数时间，与我们之前所说的相反（[输入的大小](predicting-growth.html#%28part._size-of-input%29)）。事实上，在处理密码问题时，数字操作不是常数时间非常关键，这是基本复杂性结果的关键（例如，当代密码学的假设无法破解）。
- en: 21.3.1.4Abstracting Memoization
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.1.4记忆化的抽象
- en: 'Now we’ve achieved the desired complexity improvement, but there is still something
    unsatisfactory about the structure of our revised definition of catalan: the act
    of memoization is deeply intertwined with the definition of a Catalan number,
    even though these should be intellectually distinct. Let’s do that next.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了所期望的复杂性改进，但是我们修订后的catalan定义结构仍然有些不尽如人意：记忆化的行为与Catalan数的定义深度交织在一起，尽管这两者应该在智力上是不同的。我们接下来做这件事。
- en: In effect, we want to separate our program into two parts. One part defines
    a general notion of memoization, while the other defines catalan in terms of this
    general notion.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们想将我们的程序分成两部分。一部分定义了记忆化的一般概念，而另一部分则根据这个一般概念定义了catalan。
- en: 'What does the former mean? We want to encapsulate the idea of “memory” (since
    we presumably don’t want this stored in a variable that any old part of the program
    can modify). This should result in a function that takes the input we want to
    check; if it is found in the memory we return that answer, otherwise we compute
    the answer, store it, and return it. To compute the answer, we need a function
    that determines how to do so. Putting together these pieces:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前者的意思是什么？我们想要封装“内存”的概念（因为我们大概不希望将其存储在任何程序的任意部分都可以修改的变量中）。这应该会产生一个函数，它接受我们想要检查的输入；如果在内存中找到了它，我们就返回那个答案，否则我们计算答案，存储它，并返回它。为了计算答案，我们需要一个确定如何计算的函数。将这些部分组合在一起：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We use the name memoize-1 to indicate that this is a memoizer for single-argument
    functions. Observe that the code above is virtually identical to what we had before,
    except where we had the logic of Catalan number computation, we now have the parameter
    f determining what to do.With this, we can now define catalan as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`memoize-1`这个名称来指示这是一个针对单参数函数的记忆化器。请注意，上面的代码几乎与之前的代码相同，除了我们之前的卡特兰数计算逻辑，现在有了参数`f`决定要做什么。有了这个，我们现在可以这样定义卡特兰数：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note several things about this definition:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下关于这个定义的几点：
- en: 'We don’t write fun catalan(...): ...; because the procedure bound to catalan
    is produced by memoize-1.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们不写`fun catalan(...): ...;`，因为绑定到`catalan`的过程是由`memoize-1`生成的。'
- en: 'Note carefully that the recursive calls to catalan have to be to the function
    bound to the result of memoization, thereby behaving like an object ([Objects:
    Interpretation and Types](objects.html)). Failing to refer to this same shared
    procedure means the recursive calls will not be memoized, thereby losing the benefit
    of this process.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，对`catalan`的递归调用必须是对与记忆化结果绑定的函数，从而像对象一样行为（[对象：解释与类型](objects.html)）。未能引用同一个共享过程意味着递归调用将不会被记忆化，从而失去了这一过程的好处。
- en: We need to use rec for reasons we saw earlier [[Recursive Functions](State__Change__and_More_Equality.html#%28part._rec-for-recursive%29)].
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用`rec`，这是我们之前看到的原因[[递归函数](State__Change__and_More_Equality.html#%28part._rec-for-recursive%29)]。
- en: Each invocation of memoize-1 creates a new table of stored results. Therefore
    the memoization of different functions will each get their own tables rather than
    sharing tables, which is a bad idea!
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次调用`memoize-1`都会创建一个新的存储结果的表。因此，不同函数的记忆化将各自拥有自己的表，而不是共享表，这是一个不好的主意！
- en: 'If in doubt about how state interacts with functions, read [Interaction of
    Mutation with Closures: Counters](State__Change__and_More_Equality.html#%28part._state-closures-counter%29).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对状态如何与函数交互有疑问，请阅读[状态与闭包的交互：计数器](State__Change__and_More_Equality.html#%28part._state-closures-counter%29)。
- en: Exercise
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is sharing memoization tables a bad idea? Be concrete.
  id: totrans-148
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么共享记忆表是一个坏主意？具体点。
- en: 21.3.2Edit-Distance for Spelling Correction
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.2 拼写纠正的编辑距离
- en: 'Text editors, word processors, mobile phones, and various other devices now
    routinely implement spelling correction or offer suggestions on (mis-)spellings.
    How do they do this? Doing so requires two capabilities: computing the distance
    between words, and finding words that are nearby according to this metric. In
    this section we will study the first of these questions. (For the purposes of
    this discussion, we will not dwell on the exact definition of what a “word” is,
    and just deal with strings instead. A real system would need to focus on this
    definition in considerable detail.)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编辑器、文字处理器、手机以及各种其他设备现在通常都实现了拼写纠正或提供对（拼写错误的）建议。它们是如何做到这一点的呢？这需要两个能力：计算单词之间的距离，以及根据这个度量找到附近的单词。在本节中，我们将研究这两个问题中的第一个。（在讨论的目的上，我们不会深入讨论“单词”究竟是什么的确切定义，只是处理字符串。一个真实的系统需要详细关注这个定义。）
- en: Do Now!
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在动手吧！
- en: ''
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Think about how you might define the “distance between two words”. Does it define
    a [metric space](http://en.wikipedia.org/wiki/Metric_space)?
  id: totrans-153
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 想想你可能如何定义“两个单词之间的距离”。它定义了[度量空间](http://en.wikipedia.org/wiki/Metric_space)吗？
- en: Exercise
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Will the definition we give below define a metric space over the set of words?
  id: totrans-156
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们给出的定义将在单词集合上定义一个度量空间吗？
- en: 'Though there may be several legitimate ways to define distances between words,
    here we care about the distance in the very specific context of spelling mistakes.
    Given the distance measure, one use might be to compute the distance of a given
    word from all the words in a dictionary, and offer the closest word (i.e., the
    one with the least distance) as a proposed correction.Obviously, we can’t compute
    the distance to every word in a large dictionary on every single entered word.
    Making this process efficient constitutes the other half of this problem. Briefly,
    we need to quickly discard most words as unlikely to be close enough, for which
    a representation such as a [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model)
    (here, a bag of characters) can greatly help. Given such an intended use, we would
    like at least the following to hold:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能有几种合理的定义单词之间的距离的方法，但我们在这里关心的是在拼写错误的非常特定的上下文中的距离。给定距离度量，一个用途可能是计算给定单词与字典中所有单词的距离，并提供最接近的单词（即，距离最小的单词）作为建议的更正。显然，我们无法计算每个输入的单词与大型字典中每个单词之间的距离。使这个过程有效的另一半构成了这个问题的另一半。简而言之，我们需要快速地将大多数单词丢弃，因为它们不够接近，对于这一点，如[词袋模型](http://en.wikipedia.org/wiki/Bag-of-words_model)（这里是字符袋）这样的表示可以极大地帮助。考虑到这样的预期用途，我们至少希望以下内容保持不变：
- en: That the distance from a word to itself be zero.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词到自身的距离为零。
- en: That the distance from a word to any word other than itself be strictly positive.
    (Otherwise, given a word that is already in the dictionary, the “correction” might
    be a different dictionary word.)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词与除自身之外的任何单词的距离严格为正。（否则，给定一个已在字典中的单词，“修正”可能是另一个不同的字典单词。）
- en: That the distance between two words be symmetric, i.e., it shouldn’t matter
    in which order we pass arguments.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个单词之间的距离是对称的，即，传递参数的顺序不应该影响结果。
- en: Exercise
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Observe that we have not included the triangle inequality relative to the properties
    of a metric. Why not? If we don’t need the triangle inequality, does this let
    us define more interesting distance functions that are not metrics?
  id: totrans-163
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，相对于度量的属性，我们没有包含三角不等式。为什么不呢？如果我们不需要三角不等式，这是否让我们定义更有趣的不是度量的距离函数？
- en: 'Given a pair of words, the assumption is that we meant to type one but actually
    typed the other. Here, too, there are several possible definitions, but a popular
    one considers that there are three ways to be fat-fingered:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一对单词，假设我们打算输入一个单词，但实际上输入了另一个。在这里，也有几种可能的定义，但一个流行的定义认为有三种手指大的方法：
- en: we left out a character;
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们漏掉了一个字符；
- en: we typed a character twice; or,
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打了一个字符两次；或者，
- en: we typed one character when we meant another.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们的意思是另一个字符时，我们输入了一个字符。
- en: In particular, we are interested in the fewest edits of these forms that need
    to be performed to get from one word to the other. For natural reasons, this notion
    of distance is called the edit distance or, in honor of its creator, the Levenshtein
    distance.See more on [Wikipedia](http://en.wikipedia.org/wiki/Levenshtein_distance).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们对这些形式需要执行的最少编辑感兴趣，以从一个单词到另一个单词。出于自然的原因，这种距离概念被称为编辑距离或者，为了纪念其创建者，称为莱文斯坦距离。更多详情请见[Wikipedia](http://en.wikipedia.org/wiki/Levenshtein_distance)。
- en: There are several variations of this definition possible. For now, we will consider
    the simplest one, which assumes that each of these errors has equal cost. For
    certain input devices, we may want to assign different costs to these mistakes;
    we might also assign different costs depending on what wrong character was typed
    (two characters adjacent on a keyboard are much more likely to be a legitimate
    error than two that are far apart). We will return briefly to some of these considerations
    later ([Nature as a Fat-Fingered Typist](#%28part._smith-waterman%29)).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义有几种可能的变体。目前，我们将考虑最简单的一种，它假设每个错误都具有相等的成本。对于某些输入设备，我们可能希望对这些错误分配不同的成本；我们也可能根据输入了哪个错误字符来分配不同的成本（键盘上相邻的两个字符更有可能是合法错误，而不是相隔很远的两个字符）。我们稍后会简要回顾其中一些考虑（[自然作为手指大的打字员](#%28part._smith-waterman%29)）。
- en: Under this metric, the distance between “kitten” and “sitting” is 3 because
    we have to replace “k” with “s”, replace “e” with “i”, and insert “g” (or symmetrically,
    perform the opposite replacements and delete “g”). Here are more examples:<levenshtein-tests>
    ::=
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个度量下，“kitten”和“sitting”的距离是 3，因为我们必须用“s”替换“k”，用“i”替换“e”，并插入“g”（或者对称地，执行相反的替换并删除“g”）。以下是更多的例子：<levenshtein-tests>
    ::=
- en: '|   check: |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|   check: |'
- en: '|     levenshtein(empty, empty) is 0 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein(empty, empty) 是 0 |'
- en: '|     levenshtein([list:"x"], [list: "x"]) is 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein([list:"x"], [list: "x"]) 是 0 |'
- en: '|     levenshtein([list: "x"], [list: "y"]) is 1 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein([list: "x"], [list: "y"]) 是 1 |'
- en: '|     # one of about 600 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|     # 其中之一约为 600 |'
- en: '|     levenshtein( |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "b", "r", "i", "t", "n", "e", "y"], |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "b", "r", "i", "t", "n", "e", "y"], |'
- en: '|       [list: "b", "r", "i", "t", "t", "a", "n", "y"]) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "b", "r", "i", "t", "t", "a", "n", "y"]) |'
- en: '|       is 3 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|       是 3 |'
- en: '|     # http://en.wikipedia.org/wiki/Levenshtein_distance |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|     # http://en.wikipedia.org/wiki/Levenshtein_distance |'
- en: '|     levenshtein( |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "k", "i", "t", "t", "e", "n"], |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "k", "i", "t", "t", "e", "n"], |'
- en: '|       [list: "s", "i", "t", "t", "i", "n", "g"]) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "s", "i", "t", "t", "i", "n", "g"]) |'
- en: '|       is 3 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|       是 3 |'
- en: '|     levenshtein( |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "k", "i", "t", "t", "e", "n"], |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "k", "i", "t", "t", "e", "n"], |'
- en: '|       [list: "k", "i", "t", "t", "e", "n"]) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "k", "i", "t", "t", "e", "n"]) |'
- en: '|       is 0 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|       是 0 |'
- en: '|     # http://en.wikipedia.org/wiki/Levenshtein_distance |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|     # http://en.wikipedia.org/wiki/Levenshtein_distance |'
- en: '|     levenshtein( |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "S", "u", "n", "d", "a", "y"], |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "S", "u", "n", "d", "a", "y"], |'
- en: '|       [list: "S", "a", "t", "u", "r", "d", "a", "y"]) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "S", "a", "t", "u", "r", "d", "a", "y"]) |'
- en: '|       is 3 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|       是 3 |'
- en: '|     # http://www.merriampark.com/ld.htm |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|     # http://www.merriampark.com/ld.htm |'
- en: '|     levenshtein( |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "g", "u", "m", "b", "o"], |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "g", "u", "m", "b", "o"], |'
- en: '|       [list: "g", "a", "m", "b", "o", "l"]) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "g", "a", "m", "b", "o", "l"]) |'
- en: '|       is 2 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|       是 2 |'
- en: '|     # http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|     # http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html
    |'
- en: '|     levenshtein( |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "a", "c", "g", "t", "a", "c", "g", "t", "a", "c", "g", "t"],
    |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "a", "c", "g", "t", "a", "c", "g", "t", "a", "c", "g", "t"],
    |'
- en: '|       [list: "a", "c", "a", "t", "a", "c", "t", "t", "g", "t", "a", "c",
    "t"]) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "a", "c", "a", "t", "a", "c", "t", "t", "g", "t", "a", "c",
    "t"]) |'
- en: '|       is 4 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|       是 4 |'
- en: '|     levenshtein( |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|     levenshtein( |'
- en: '|       [list: "s", "u", "p", "e", "r", "c", "a", "l", "i", |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "s", "u", "p", "e", "r", "c", "a", "l", "i", |'
- en: '|         "f", "r", "a", "g", "i", "l", "i", "s", "t" ], |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|         "f", "r", "a", "g", "i", "l", "i", "s", "t" ], |'
- en: '|       [list: "s", "u", "p", "e", "r", "c", "a", "l", "y", |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|       [list: "s", "u", "p", "e", "r", "c", "a", "l", "y", |'
- en: '|         "f", "r", "a", "g", "i", "l", "e", "s", "t" ]) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|         "f", "r", "a", "g", "i", "l", "e", "s", "t" ]) |'
- en: '|       is 2 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|       是 2 |'
- en: '|   end |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: The basic algorithm is in fact very simple:<levenshtein> ::=
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基本算法实际上非常简单：<levenshtein> ::=
- en: '|   rec levenshtein :: (List<String>, List<String> -> Number) = |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|   rec levenshtein :: (List<String>, List<String> -> Number) = |'
- en: '|     [<levenshtein-body>](#%28elem._levenshtein-body%29) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|     [<levenshtein-body>](#%28elem._levenshtein-body%29) |'
- en: where, because there are two list inputs, there are four cases, of which two
    are symmetric:<levenshtein-body> ::=
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，因为有两个列表输入，有四种情况，其中两种是对称的：<levenshtein-body> ::=
- en: '|   lam(s, t): |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|   lam(s, t): |'
- en: '|     [<levenshtein-both-empty>](#%28elem._levenshtein-both-empty%29) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|     [<levenshtein-both-empty>](#%28elem._levenshtein-both-empty%29) |'
- en: '|     [<levenshtein-one-empty>](#%28elem._levenshtein-one-empty%29) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|     [<levenshtein-one-empty>](#%28elem._levenshtein-one-empty%29) |'
- en: '|     [<levenshtein-neither-empty>](#%28elem._levenshtein-neither-empty%29)
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|     [<levenshtein-neither-empty>](#%28elem._levenshtein-neither-empty%29)
    |'
- en: '|   end |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: If both inputs are empty, the answer is simple:<levenshtein-both-empty> ::=
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个输入都为空，则答案很简单：<levenshtein-both-empty> ::=
- en: '|   if is-empty(s) and is-empty(t): 0 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|   if is-empty(s) and is-empty(t): 0 |'
- en: When one is empty, then the edit distance corresponds to the length of the other,
    which needs to inserted (or deleted) in its entirety (so we charge a cost of one
    per character):<levenshtein-one-empty> ::=
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当其中一个为空时，编辑距离对应于另一个的长度，需要完全插入（或删除）（因此我们每个字符收取一次费用）：<levenshtein-one-empty> ::=
- en: '|   else if is-empty(s): t.length() |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|   else if is-empty(s): t.length() |'
- en: '|   else if is-empty(t): s.length() |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|   else if is-empty(t): s.length() |'
- en: If neither is empty, then each has a first character. If they are the same,
    then there is no edit cost associated with this character (which we reflect by
    recurring on the rest of the words without adding to the edit cost). If they are
    not the same, however, we consider each of the possible edits:<levenshtein-neither-empty>
    ::=
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两者都不为空，则每个都有一个第一个字符。 如果它们相同，则与此字符相关联的编辑成本为零（我们在不增加编辑成本的情况下反复对单词的其余部分进行反射）。
    但是，如果它们不同，我们考虑每种可能的编辑：<levenshtein-neither-empty> ::=
- en: '|   else: |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|   else: |'
- en: '|     if s.first == t.first: |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|     如果 s.first == t.first: |'
- en: '|       levenshtein(s.rest, t.rest) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|       levenshtein(s.rest, t.rest) |'
- en: '|     else: |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|     else: |'
- en: '|       min3( |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|       min3( |'
- en: '|         1 + levenshtein(s.rest, t), |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|         1 + levenshtein(s.rest, t), |'
- en: '|         1 + levenshtein(s, t.rest), |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|         1 + levenshtein(s, t.rest), |'
- en: '|         1 + levenshtein(s.rest, t.rest)) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|         1 + levenshtein(s.rest, t.rest)) |'
- en: '|     end |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|     end |'
- en: '|   end |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: 'In the first case, we assume s has one too many characters, so we compute the
    cost as if we’re deleting it and finding the lowest cost for the rest of the strings
    (but charging one for this deletion); in the second case, we symmetrically assume
    t has one too many; and in the third case, we assume one character got replaced
    by another, so we charge one but consider the rest of both words (e.g., assume
    “s” was typed for “k” and continue with “itten” and “itting”). This uses the following
    helper function:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，我们假设s多了一个字符，因此我们计算成本，就像我们要删除它并为其余的字符串找到最低成本一样（但为此删除收取一个成本）；在第二种情况下，我们对称地假设t多了一个字符；在第三种情况下，我们假设一个字符被另一个字符替换了，所以我们收取一个成本，但考虑两个单词的其余部分（例如，假设“s”被输入为“k”，并继续处理“itten”和“itting”）。这使用了以下辅助函数：
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This algorithm will indeed pass all the tests we have written above, but with
    a problem: the running time grows exponentially. That is because, each time we
    find a mismatch, we recur on three subproblems. In principle, therefore, the algorithm
    takes time proportional to three to the power of the length of the shorter word.
    In practice, any prefix that matches causes no branching, so it is mismatches
    that incur branching (thus, confirming that the distance of a word with itself
    is zero only takes time linear in the size of the word).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法确实会通过我们上面编写的所有测试，但有一个问题：运行时间呈指数增长。这是因为每次我们找到一个不匹配时，我们都会在三个子问题上进行递归。因此，原则上，算法的时间与较短单词的长度的三次方成正比。实际上，任何与之匹配的前缀都不会引起分支，因此是不匹配引起分支（因此，确认一个单词与自身的距离为零仅需要与单词大小成线性时间）。
- en: Observe, however, that many of these subproblems are the same. For instance,
    given “kitten” and “sitting”, the mismatch on the initial character will cause
    the algorithm to compute the distance of “itten” from “itting” but also “itten”
    from “sitting” and “kitten” from “itting”. Those latter two distance computations
    will also involve matching “itten” against “itting”. Thus, again, we want the
    computation tree to turn into a DAG of expressions that are actually evaluated.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，然而，许多这些子问题是相同的。例如，给定“kitten”和“sitting”，初始字符的不匹配会导致算法计算“itten”与“itting”的距离，但也会计算“itten”与“sitting”以及“kitten”与“itting”的距离。后两个距离计算也将涉及将“itten”与“itting”进行匹配。因此，我们再次希望计算树变成实际计算的表达式的DAG。
- en: 'The solution, therefore, is naturally to memoize. First, we need a memoizer
    that works over two arguments rather than one:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，解决方案自然是使用记忆化。首先，我们需要一个可以处理两个参数而不是一个的记忆器：
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Most of the code is unchanged, except that we store two arguments rather than
    one, and correspondingly look up both.With this, we can redefine levenshtein to
    use memoization:<levenshtein-memo> ::=
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数代码都没有变化，只是我们存储了两个参数而不是一个，并相应地查找两个。有了这个，我们可以重新定义levenshtein以使用记忆化：<levenshtein-memo>
    ::=
- en: '|   rec levenshtein :: (List<String>, List<String> -> Number) = |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|   rec levenshtein :: (List<String>, List<String> -> Number) = |'
- en: '|     memoize-2( |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|     memoize-2( |'
- en: '|       lam(s, t): |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|       lam(s, t): |'
- en: '|         if is-empty(s) and is-empty(t): 0 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|         if is-empty(s) and is-empty(t): 0 |'
- en: '|         else if is-empty(s): t.length() |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|         else if is-empty(s): t.length() |'
- en: '|         else if is-empty(t): s.length() |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|         else if is-empty(t): s.length() |'
- en: '|         else: |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|         else: |'
- en: '|           if s.first == t.first: |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|           if s.first == t.first: |'
- en: '|             levenshtein(s.rest, t.rest) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|             levenshtein(s.rest, t.rest) |'
- en: '|           else: |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|           else: |'
- en: '|             min3( |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|             min3( |'
- en: '|               1 + levenshtein(s.rest, t), |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|               1 + levenshtein(s.rest, t), |'
- en: '|               1 + levenshtein(s, t.rest), |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|               1 + levenshtein(s, t.rest), |'
- en: '|               1 + levenshtein(s.rest, t.rest)) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|               1 + levenshtein(s.rest, t.rest)) |'
- en: '|           end |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|           end |'
- en: '|         end |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|         end |'
- en: '|       end) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|       end) |'
- en: where the argument to memoize-2 is precisely what we saw earlier as [<levenshtein-body>](#%28elem._levenshtein-body%29)
    (and now you know why we defined levenshtein slightly oddly, not using fun).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中memoize-2的参数正是我们先前见过的[<levenshtein-body>](#%28elem._levenshtein-body%29)（现在你知道为什么我们略微奇怪地定义了levenshtein，而不是使用fun）。
- en: 'The complexity of this algorithm is still non-trivial. First, let’s introduce
    the term suffix: the suffix of a string is the rest of the string starting from
    any point in the string. (Thus “kitten”, “itten”, “ten”, “n”, and “” are all suffixes
    of “kitten”.) Now, observe that in the worst case, starting with every suffix
    in the first word, we may need to perform a comparison against every suffix in
    the second word. Fortunately, for each of these suffixes we perform a constant
    computation relative to the recursion. Therefore, the overall time complexity
    of computing the distance between strings of length \(m\) and \(n\) is \(O([m,
    n \rightarrow m \cdot n])\). (We will return to space consumption later [[Contrasting
    Memoization and Dynamic Programming](#%28part._memo-vs-dp%29)].)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的复杂性仍然不容忽视。首先，让我们介绍后缀这个术语：字符串的后缀是从字符串的任何点开始的剩余部分。（因此，“kitten”，“itten”，“ten”，“n”和“”都是“kitten”的后缀。）现在，请注意，在最坏的情况下，从第一个单词的每个后缀开始，我们可能需要对第二个单词的每个后缀执行比较。幸运的是，对于这些后缀中的每一个，我们相对于递归执行一个恒定的计算。因此，计算长度为\(m\)和\(n\)的字符串之间的距离的总体时间复杂度是\(O([m,
    n \rightarrow m \cdot n])\)。（我们稍后会回到空间消耗问题[[对比备忘录和动态规划](#%28part._memo-vs-dp%29)]。）
- en: Exercise
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Modify the above algorithm to produce an actual (optimal) sequence of edit operations.
    This is sometimes known as the traceback.
  id: totrans-264
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 修改上述算法以产生一个实际（最佳）的编辑操作序列。有时这被称为回溯。
- en: 21.3.3Nature as a Fat-Fingered Typist
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.3自然界如同一个手残的打字员
- en: 'We have talked about how to address mistakes made by humans. However, humans
    are not the only bad typists: nature is one, too!'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经谈论了如何解决人类的错误。然而，人类并不是唯一的糟糕的打字员：自然也是之一！
- en: 'When studying living matter we obtain sequences of amino acids and other such
    chemicals that comprise molecules, such as DNA, that hold important and potentially
    determinative information about the organism. These sequences consist of similar
    fragments that we wish to identify because they represent relationships in the
    organism’s behavior or evolution.This section may need to be skipped in [some
    states and countries](http://en.wikipedia.org/wiki/Creation_and_evolution_in_public_education).
    Unfortunately, these sequences are never identical: like all low-level programmers,
    nature slips up and sometimes makes mistakes in copying (called—<wbr>wait for
    it—<wbr>mutations). Therefore, looking for strict equality would rule out too
    many sequences that are almost certainly equivalent. Instead, we must perform
    an alignment step to find these equivalent sequences. As you might have guessed,
    this process is very much a process of computing an edit distance, and using some
    threshold to determine whether the edit is small enough.To be precise, we are
    performing local [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment).
    This algorithm is named, after its creators, Smith-Waterman, and because it is
    essentially identical, has the same complexity as the Levenshtein algorithm.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当研究生物体时，我们获得了由氨基酸和其他化学物质组成的分子的序列，如DNA，这些分子包含有关生物体的重要且潜在决定性的信息。这些序列由我们希望识别的相似片段组成，因为它们代表了生物体行为或进化中的关系。[某些州和国家](http://en.wikipedia.org/wiki/Creation_and_evolution_in_public_education)中可能需要跳过本节。不幸的是，这些序列永远不会完全相同：就像所有低级程序员一样，自然有时会在复制过程中出错（称为——等待它——突变）。因此，寻找严格的相等性会排除太多几乎可以肯定是等价的序列。相反，我们必须执行一个对齐步骤来找到这些等价序列。正如你可能已经猜到的那样，这个过程非常像计算编辑距离的过程，并使用某个阈值来确定编辑是否足够小。准确地说，我们正在执行局部[序列对齐](http://en.wikipedia.org/wiki/Sequence_alignment)。该算法以其创造者Smith-Waterman的名字命名，并且因为它本质上是相同的，所以与Levenshtein算法具有相同的复杂性。
- en: 'The only difference between traditional presentations Levenshtein and Smith-Waterman
    is something we alluded to earlier: why is every edit given a distance of one?
    Instead, in the Smith-Waterman presentation, we assume that we have a function
    that gives us the gap score, i.e., the value to assign every character’s alignment,
    i.e., scores for both matches and edits, with scores driven by biological considerations.
    Of course, as we have already noted, this need is not peculiar to biology; we
    could just as well use a “gap score” to reflect the likelihood of a substitution
    based on keyboard characteristics.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 传统演示与Levenshtein和Smith-Waterman的唯一区别是我们之前提到的一些事情：为什么每次编辑都被赋予距离为一？相反，在Smith-Waterman演示中，我们假设有一个函数给出了我们的间隙分数，即，赋予每个字符对齐的值，即，匹配和编辑的分数，分数受生物学考虑驱动。当然，正如我们已经指出的那样，这种需要并不特有于生物学；我们完全可以利用“间隙分数”来反映基于键盘特性的替换可能性。
- en: 21.3.4Dynamic Programming
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.4 动态规划
- en: We have used memoization as our canonical means of saving the values of past
    computations to reuse later. There is another popular technique for doing this
    called dynamic programming. This technique is closely related to memoization;
    indeed, it can be viewed as the dual method for achieving the same end. First
    we will see dynamic programming at work, then discuss how it differs from memoization.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用备忘录作为保存过去计算值以供以后重用的标准方法。还有另一种称为动态规划的流行技术来实现这一点。这种技术与备忘录密切相关；事实上，它可以被视为实现相同目标的双重方法。首先我们将看到动态规划的工作原理，然后讨论它与备忘录的区别。
- en: Dynamic programming also proceeds by building up a memory of answers, and looking
    them up instead of recomputing them. As such, it too is a process for turning
    a computation’s shape from a tree to a DAG of actual calls. The key difference
    is that instead of starting with the largest computation and recurring to smaller
    ones, it starts with the smallest computations and builds outward to larger ones.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划也是通过建立答案的记忆，并在需要时查找它们而不是重新计算它们。因此，它也是将计算形状从树转换为实际调用的DAG的过程。关键的区别在于，它不是从最大的计算开始并递归到较小的计算，而是从最小的计算开始，向外构建到较大的计算。
- en: We will revisit our previous examples in light of this approach.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这种方法的指导下重新审视我们之前的例子。
- en: 21.3.4.1Catalan Numbers with Dynamic Programming
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.4.1 动态规划的卡塔兰数
- en: To begin with, we need to define a data structure to hold answers. Following
    convention, we will use an array.What happens when we run out of space? We can
    use the doubling technique we studied for [Halloween Analysis](amortized-analysis.html).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一个数据结构来保存答案。按照惯例，我们将使用一个数组。当我们的空间不够用时会发生什么？我们可以使用我们在[万圣节分析](amortized-analysis.html)中学到的加倍技术。
- en: '[PRE21]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, the catalan function simply looks up the answer in this array:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，catalan 函数只需在这个数组中查找答案：
- en: '[PRE22]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'But how do we fill the array? We initialize the one known value, and use the
    formula to compute the rest in incremental order:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何填充数组呢？我们初始化一个已知值，并使用公式按顺序逐步计算其余部分：
- en: '[PRE23]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The resulting program obeys the tests in [<catalan-tests>](#%28elem._catalan-tests%29).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的程序遵循了[<catalan-tests>](#%28elem._catalan-tests%29)中的测试。
- en: Notice that we have had to undo the natural recursive definition—<wbr>which
    proceeds from bigger values to smaller ones—<wbr>to instead use a loop that goes
    from smaller values to larger ones. In principle, the program has the danger that
    when we apply catalan to some value, that index of answers will have not yet been
    initialized, resultingin an error. In fact, however, we know that because we fill
    all smaller indices in answers before computing the next larger one, we will never
    actually encounter this error. Note that this requires careful reasoning about
    our program, which we did not need to perform when using memoization because there
    we made precisely the recursive call we needed, which either looked up the value
    or computed it afresh.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不得不撤销自然的递归定义，—从较大的值到较小的值的递归—，改为使用从较小的值到较大的值的循环。原则上，程序存在一个危险，即当我们将卡塔兰应用到某个值时，答案的索引可能尚未初始化，从而导致错误。实际上，我们知道，因为我们在计算下一个较大值之前填充了所有较小的索引，所以我们永远不会真正遇到这个错误。请注意，这需要对我们的程序进行仔细的推理，而我们在使用备忘录时不需要进行这样的推理，因为在那里，我们精确地进行了所需的递归调用，它们要么查找值，要么重新计算它。
- en: 21.3.4.2Levenshtein Distance and Dynamic Programming
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.4.2 Levenshtein距离和动态规划
- en: Now let’s take on rewriting the Levenshtein distance computation:<levenshtein-dp>
    ::=
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们着手重写Levenshtein距离计算：<levenshtein-dp> ::=
- en: '|   fun levenshtein(s1 :: List<String>, s2 :: List<String>): |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|   fun levenshtein(s1 :: List<String>, s2 :: List<String>): |'
- en: '|     [<levenshtein-dp/1>](#%28elem._levenshtein-dp%2F1%29) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|     [<levenshtein-dp/1>](#%28elem._levenshtein-dp%2F1%29) |'
- en: '|   end |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: 'We will use a table representing the edit distance for each prefix of each
    word. That is, we will have a two-dimensional table with as many rows as the length
    of s1 and as many columns as the length of s2. At each position, we will record
    the edit distance for the prefixes of s1 and s2 up to the indices represented
    by that position in the table.Note that index arithmetic will be a constant burden:
    if a word is of length \(n\), we have to record the edit distance to its \(n +
    1\) positions, the extra one corresponding to the empty word. This will hold for
    both words:<levenshtein-dp/1> ::='
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个表示每个单词前缀的编辑距离的表。也就是说，我们将有一个二维表，行数等于s1的长度，列数等于s2的长度。在每个位置，我们将记录该位置处表中所代表的s1和s2的前缀的编辑距离。请注意，索引算术将是一个不断的负担：如果一个单词的长度为\(n\)，我们必须记录到其\(n
    + 1\)个位置的编辑距离，额外的一个对应于空单词。这对于两个单词都成立：<levenshtein-dp/1> ::=
- en: '|   s1-len = s1.length() |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|   s1-len = s1.length() |'
- en: '|   s2-len = s2.length() |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|   s2-len = s2.length() |'
- en: '|   answers = array2d(s1-len + 1, s2-len + 1, none) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|   answers = array2d(s1-len + 1, s2-len + 1, none) |'
- en: '|   [<levenshtein-dp/2>](#%28elem._levenshtein-dp%2F2%29) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|   [<levenshtein-dp/2>](#%28elem._levenshtein-dp%2F2%29) |'
- en: Observe that by creating answers inside levenshtein, we can determine the exact
    size it needs to be based on the inputs, rather than having to over-allocate or
    dynamically grow the array.We have initialized the table with none, so we will
    get an error if we accidentally try to use an uninitialized entry.Which proved
    to be necessary when writing and debugging this code! It will therefore be convenient
    to create helper functions that let us pretend the table contains only numbers:<levenshtein-dp/2>
    ::=
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过在levenshtein内部创建answers，我们可以根据输入确定它需要的确切大小，而不是必须过度分配或动态增长数组。我们已经用none初始化了表，因此如果我们意外地尝试使用未初始化的条目，我们将收到错误消息。编写和调试此代码时证明是必要的！因此，创建让我们假设表只包含数字的辅助函数将是方便的：<levenshtein-dp/2>
    ::=
- en: '|   fun put(s1-idx :: Number, s2-idx :: Number, n :: Number): |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|   fun put(s1-idx :: Number, s2-idx :: Number, n :: Number): |'
- en: '|     answers.set(s1-idx, s2-idx, some(n)) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|     answers.set(s1-idx, s2-idx, some(n)) |'
- en: '|   end |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: '|   fun lookup(s1-idx :: Number, s2-idx :: Number) -> Number: |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|   fun lookup(s1-idx :: Number, s2-idx :: Number) -> Number: |'
- en: '|     a = answers.get(s1-idx, s2-idx) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|     a = answers.get(s1-idx, s2-idx) |'
- en: '|     cases (Option) a: |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|     cases (Option) a: |'
- en: '|       &#124; none => raise("looking at uninitialized value") |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|       &#124; none => raise("looking at uninitialized value") |'
- en: '|       &#124; some(v) => v |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|       &#124; some(v) => v |'
- en: '|     end |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|     end |'
- en: '|   end |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: Now we have to populate the array. First, we initialize the row representing
    the edit distances when s2 is empty, and the column where s1 is empty. At \((0,
    0)\), the edit distance is zero; at every position thereafter, it is the distance
    of that position from zero, because that many characters must be added to one
    or deleted from the other word for the two to coincide:<levenshtein-dp/3> ::=
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须填充数组。首先，我们初始化了当s2为空时的编辑距离的行，以及当s1为空时的列。在\((0, 0)\)，编辑距离为零；在之后的每个位置，它是该位置与零的距离，因为必须将这么多字符添加到一个单词或从另一个单词中删除才能使两个单词重合：<levenshtein-dp/3>
    ::=
- en: '|   for each(s1i from range(0, s1-len + 1)): |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|   for each(s1i from range(0, s1-len + 1)): |'
- en: '|     put(s1i, 0, s1i) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|     put(s1i, 0, s1i) |'
- en: '|   end |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: '|   for each(s2i from range(0, s2-len + 1)): |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|   for each(s2i from range(0, s2-len + 1)): |'
- en: '|     put(0, s2i, s2i) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|     put(0, s2i, s2i) |'
- en: '|   end |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: '|   [<levenshtein-dp/4>](#%28elem._levenshtein-dp%2F4%29) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|   [<levenshtein-dp/4>](#%28elem._levenshtein-dp%2F4%29) |'
- en: Now we finally get to the heart of the computation. We need to iterate over
    every character in each word. these characters are at indices 0 to s1-len - 1
    and s2-len - 1, which are precisely the ranges of values produced by range(0,
    s1-len) and range(0, s2-len).<levenshtein-dp/4> ::=
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于来到了计算的核心。我们需要迭代每个单词中的每个字符。这些字符位于索引0到s1-len - 1和s2-len - 1，这恰好是range(0,
    s1-len)和range(0, s2-len)产生的值范围。<levenshtein-dp/4> ::=
- en: '|   for each(s1i from range(0, s1-len)): |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|   for each(s1i from range(0, s1-len)): |'
- en: '|     for each(s2i from range(0, s2-len)): |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|     for each(s2i from range(0, s2-len)): |'
- en: '|     [<levenshtein-dp/compute-dist>](#%28elem._levenshtein-dp%2Fcompute-dist%29)
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|     [<levenshtein-dp/compute-dist>](#%28elem._levenshtein-dp%2Fcompute-dist%29)
    |'
- en: '|     end |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|     end |'
- en: '|   end |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|   end |'
- en: '|   [<levenshtein-dp/get-result>](#%28elem._levenshtein-dp%2Fget-result%29)
    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|   [<levenshtein-dp/get-result>](#%28elem._levenshtein-dp%2Fget-result%29)
    |'
- en: Note that we’re building our way “out” from small cases to large ones, rather
    than starting with the large input and working our way “down”, recursively, to
    small ones.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在从小案例“向外”构建我们的方式，而不是从大输入开始，递归地“向下”工作到小案例。
- en: Do Now!
  id: totrans-319
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在做！
- en: ''
  id: totrans-320
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is this strictly true?
  id: totrans-321
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: No, it isn’t. We did first fill in values for the “borders” of the table. This
    is because doing so in the midst of [<levenshtein-dp/compute-dist>](#%28elem._levenshtein-dp%2Fcompute-dist%29)
    would be much more annoying. By initializing all the known values, we keep the
    core computation cleaner. But it does mean the order in which we fill in the table
    is fairly complex.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s return to computing the distance. For each pair of positions, we
    want the edit distance between the pair of words up to and including those positions.
    This distance is given by checking whether the characters at the pair of positions
    are identical. If they are, then the distance is the same as it was for the previous
    pair of prefixes; otherwise we have to try the three different kinds of edits:<levenshtein-dp/compute-dist>
    ::=
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '|   dist = |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '|     if index(s1, s1i) == index(s2, s2i): |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '|       lookup(s1i, s2i) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '|     else: |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '|       min3( |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '|         1 + lookup(s1i, s2i + 1), |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '|         1 + lookup(s1i + 1, s2i), |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '|         1 + lookup(s1i, s2i)) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '|     end |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '|   put(s1i + 1, s2i + 1, dist) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: As an aside, this sort of “off-by-one” coordinate arithmetic is traditional
    when using tabular representations, because we write code in terms of elements
    that are not inherently present, and therefore have to create a padded table to
    hold values for the boundary conditions. The alternative would be to allow the
    table to begin its addressing from -1 so that the main computation looks traditional.At
    any rate, when this computation is done, the entire table has been filled with
    values. We still have to read out the answer, with lies at the end of the table:<levenshtein-dp/get-result>
    ::=
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '|   lookup(s1-len, s2-len) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: Even putting aside the helper functions we wrote to satiate our paranoia about
    using undefined values, we end up with:As of this writing, the [current version](http://en.wikipedia.org/w/index.php?title=Levenshtein_distance&oldid=581406185#Iterative_with_full_matrix)
    of the [Wikipedia page](http://en.wikipedia.org/wiki/Levenshtein_distance) on
    the Levenshtein distance features a dynamic programming version that is very similar
    to the code above. By writing in pseudocode, it avoids address arithmetic issues
    (observe how the words are indexed starting from 1 instead of 0, which enables
    the body of the code to look more “normal”), and by initializing all elements
    to zero it permits subtle bugs because an uninitialized table element is indistinguishable
    from a legitimate entry with edit distance of zero. The page also shows the [recursive](http://en.wikipedia.org/w/index.php?title=Levenshtein_distance&oldid=581406185#Recursive)
    solution and alludes to memoization, but does not show it in code.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: which is worth contrasting with the memoized version ([<levenshtein-memo>](#%28elem._levenshtein-memo%29)).For
    more examples of canonical dynamic programming problems, see [this page](http://people.csail.mit.edu/bdean/6.046/dp/)
    and think about how each can be expressed as a direct recursion.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这与记忆化版本（[<levenshtein-memo>](#%28elem._levenshtein-memo%29)）值得对比。有关典型动态规划问题的更多示例，请参阅[此页面](http://people.csail.mit.edu/bdean/6.046/dp/)，并思考如何将每个问题表示为直接递归。
- en: 21.3.5Contrasting Memoization and Dynamic Programming
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 21.3.5记忆化和动态规划的对比
- en: 'Now that we’ve seen two very techniques for avoiding recomputation, it’s worth
    contrasting them. The important thing to note is that memoization is a much simpler
    technique: write the natural recursive definition; determine its space complexity;
    decide whether this is problematic enough to warrant a space-time trade-off; and
    if it is, apply memoization. The code remains clean, and subsequent readers and
    maintainers will be grateful for that. In contrast, dynamic programming requires
    a reorganization of the algorithm to work bottom-up, which can often make the
    code harder to follow and full of subtle invariants about boundary conditions
    and computation order.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了两种避免重新计算的技术，值得对它们进行对比。值得注意的是，记忆化是一种更简单的技术：编写自然递归定义；确定其空间复杂度；决定是否存在足够的问题以值得进行空间-时间折衷；如果有必要，应用记忆化。代码保持清晰，后续的读者和维护者会对此表示感谢。相比之下，动态规划需要重新组织算法以自底向上工作，这往往会使代码更难以理解，并且充满了关于边界条件和计算顺序的微妙不变量。
- en: That said, the dynamic programming solution can sometimes be more computationally
    efficient. For instance, in the Levenshtein case, observe that at each table element,
    we (at most) only ever use the ones that are from the previous row and column.
    That means we never need to store the entire table; we can retain just the fringe
    of the table, which reduces space to being proportional to the sum, rather than
    product, of the length of the words. In a computational biology setting (when
    using Smith-Waterman), for instance, this saving can be substantial. This optimization
    is essentially impossible for memoization.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，动态规划解决方案有时可能更加计算效率高。例如，在莱文斯坦案例中，注意到在每个表元素中，我们（最多）仅使用来自前一行和前一列的元素。这意味着我们从不需要存储整个表；我们只需保留表的边缘，这将空间降低到长度之和，而不是长度之积。在计算生物学环境中（例如使用Smith-Waterman），这种节省可能是相当大的。这种优化对于记忆化来说基本上是不可能的。
- en: 'In more detail, here’s the contrast:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，这里是对比：
- en: '| Memoization |  | Dynamic Programming |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 记忆化 |  | 动态规划 |'
- en: '| Top-down |  | Bottom-up |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 自顶向下 |  | 自底向上 |'
- en: '| Depth-first |  | Breadth-first |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 深度优先 |  | 广度优先 |'
- en: '| Black-box |  | Requires code reorganization |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 |  | 需要代码重组 |'
- en: '| All stored calls are necessary |  | May do unnecessary computation |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 所有存储的调用都是必要的 |  | 可能会进行不必要的计算 |'
- en: '| Cannot easily get rid of unnecessary data |  | Can more easily get rid of
    unnecessary data |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 无法轻易消除不必要的数据 |  | 可以更容易地消除不必要的数据 |'
- en: '| Can never accidentally use an uninitialized answer |  | Can accidentally
    use an uninitialized answer |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 永远不会意外使用未初始化的答案 |  | 可能会意外使用未初始化的答案 |'
- en: '| Needs to check for the presence of an answer |  | Can be designed to not
    need to check for the presence of an answer |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 需要检查是否存在答案 |  | 可以设计为无需检查是否存在答案 |'
- en: As this table should make clear, these are essentialy dual approaches. What
    is perhaps left unstated in most dynamic programming descriptions is that it,
    too, is predicated on the computation always producing the same answer for a given
    input—<wbr>i.e., being a pure function.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个表所表明的那样，这些基本上是对偶方法。在大多数动态规划描述中可能没有明确说明的是，它也是建立在对于给定输入始终产生相同答案的计算的假设上，即，它是一个纯函数。
- en: From a software design perspective, there are two more considerations.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件设计的角度来看，还有两个考虑因素。
- en: 'First, the performance of a memoized solution can trail that of dynamic programming
    when the memoized solution uses a generic data structure to store the memo table,
    whereas a dynamic programming solution will invariably use a custom data structure
    (since the code needs to be rewritten against it anyway). Therefore, before switching
    to dynamic programming for performance reasons, it makes sense to try to create
    a custom memoizer for the problem: the same knowledge embodied in the dynamic
    programming version can often be encoded in this custom memoizer (e.g., using
    an array instead of list to improve access times). This way, the program can enjoy
    speed comparable to that of dynamic programming while retaining readability and
    maintainability.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当记忆化解决方案使用通用数据结构来存储备忘录表时，其性能可能落后于动态规划，而动态规划解决方案无论如何都将使用自定义数据结构（因为代码需要根据其重新编写）。因此，在出于性能原因切换到动态规划之前，尝试为问题创建一个自定义的记忆器是有道理的：动态规划版本中所体现的相同知识通常可以编码到这个自定义的记忆器中（例如，使用数组而不是列表来提高访问速度）。这样，程序可以在保留可读性和可维护性的同时享受与动态规划相媲美的速度。
- en: Second, suppose space is an important consideration and the dynamic programming
    version can make use of significantly less space. Then it does make sense to employ
    dynamic programming instead. Does this mean the memoized version is useless?
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，假设空间是一个重要的考虑因素，并且动态规划版本可以使用显著较少的空间。那么使用动态规划确实是有意义的。这是否意味着记忆化版本无用？
- en: Do Now!
  id: totrans-355
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在开始！
- en: ''
  id: totrans-356
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What do you think? Do we still have use for the memoized version?
  id: totrans-357
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你觉得呢？我们还需要记忆化版本吗？
- en: Yes, of course we do! It can serve as an oracle [REF] for the dynamic programming
    version, since the two are supposed to produce identical answers anyway—<wbr>and
    the memoized version would be a much more efficient oracle than the purely recursive
    implemenation, and can therefore be used to test the dynamic programming version
    on much larger inputs.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，当然了！它可以作为动态规划版本的[参考](REF)oracle，因为两者应该产生相同的答案——<wbr>而记忆化版本将是一个比纯递归实现更高效的oracle，因此可以用于测试更大输入上的动态规划版本。
- en: In short, always first produce the memoized version. If you need more performance,
    consider customizing the memoizer’s data structure. If you need to also save space,
    and can arrive at a more space-efficient dynamic programming solution, then keep
    both versions around, using the former to test the latter (the person who inherits
    your code and needs to alter it will thank you!).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，始终先生成记忆化版本。如果需要更高的性能，请考虑定制化记忆器的数据结构。如果还需要节省空间，并且可以得到更节省空间的动态规划解决方案，那么请保留两个版本，使用前者测试后者（继承您代码并需要更改它的人会感谢您！）。
- en: Exercise
  id: totrans-360
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 练习
- en: ''
  id: totrans-361
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We have characterized the fundamental difference between memoization and dynamic
    programming as that between top-down, depth-first and bottom-up, breadth-first
    computation. This should naturally raise the question, what about:'
  id: totrans-362
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们已经将记忆化和动态规划之间的根本区别描述为自顶向下，深度优先和自底向上，广度优先计算之间的区别。这自然会引发一个问题，那就是：
- en: ''
  id: totrans-363
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: top-down, breadth-first
  id: totrans-364
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自顶向下，广度优先
- en: ''
  id: totrans-365
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-366
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: bottom-up, depth-first
  id: totrans-367
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自底向上，深度优先
- en: ''
  id: totrans-368
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-369
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-370
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: orders of computation. Do they also have special names that we just happen to
    not know? Are they uninteresting? Or do they not get discussed for a reason?
  id: totrans-371
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 计算顺序。它们也有特殊的名称吗，我们只是碰巧不知道吗？它们不重要吗？还是由于某种原因而没有讨论？
