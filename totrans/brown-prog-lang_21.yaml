- en: 21Algorithms That Exploit State
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|     [21.1 Disjoint Sets Redux](#%28part._union-find%29) |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.1.1 Optimizations](#%28part._.Optimizations%29) |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.1.2 Analysis](#%28part._.Analysis%29) |'
  prefs: []
  type: TYPE_TB
- en: '|     [21.2 Set Membership by Hashing Redux](#%28part._hash-tables%29) |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.2.1 Improving Access Time](#%28part._.Improving_.Access_.Time%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.2.2 Better Hashing](#%28part._.Better_.Hashing%29) |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.2.3 Bloom Filters](#%28part._.Bloom_.Filters%29) |'
  prefs: []
  type: TYPE_TB
- en: '|     [21.3 Avoiding Recomputation by Remembering Answers](#%28part._.Avoiding_.Recomputation_by_.Remembering_.Answers%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.3.1 An Interesting Numeric Sequence](#%28part._.An_.Interesting_.Numeric_.Sequence%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|         [21.3.1.1 Using State to Remember Past Answers](#%28part._.Using_.State_to_.Remember_.Past_.Answers%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|         [21.3.1.2 From a Tree of Computation to a DAG](#%28part._.From_a_.Tree_of_.Computation_to_a_.D.A.G%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|         [21.3.1.3 The Complexity of Numbers](#%28part._numbers-not-constant%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|         [21.3.1.4 Abstracting Memoization](#%28part._.Abstracting_.Memoization%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.3.2 Edit-Distance for Spelling Correction](#%28part._levenshtein%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.3.3 Nature as a Fat-Fingered Typist](#%28part._smith-waterman%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.3.4 Dynamic Programming](#%28part._.Dynamic_.Programming%29) |'
  prefs: []
  type: TYPE_TB
- en: '|         [21.3.4.1 Catalan Numbers with Dynamic Programming](#%28part._.Catalan_.Numbers_with_.Dynamic_.Programming%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|         [21.3.4.2 Levenshtein Distance and Dynamic Programming](#%28part._.Levenshtein_.Distance_and_.Dynamic_.Programming%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [21.3.5 Contrasting Memoization and Dynamic Programming](#%28part._memo-vs-dp%29)
    |'
  prefs: []
  type: TYPE_TB
- en: 21.1Disjoint Sets Redux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s how we can use this to implement union-find afresh. We will try to keep
    things as similar to the previous version ([Checking Component Connectedness](graphs.html#%28part._union-find-functional%29))
    as possible, to enhance comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to update the definition of an element, making the parent field
    be mutable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To determine whether two elements are in the same set, we will still rely on
    fynd. However, as we will soon see, fynd no longer needs to be given the entire
    set of elements. Because the only reason is-in-same-set consumed that set was
    to pass it on to fynd, we can remove it from here. Nothing else changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Updating is now the crucial difference: we use mutation to change the value
    of the parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In parent: some(parent), the first parent is the name of the field, while the
    second one is the parameter name. In addition, we must use some to satisfy the
    option type. Naturally, it is not none because the entire point of this mutation
    is to change the parent to be the other element, irrespective of what was there
    before.Given this definition, union also stays largely unchanged, other than the
    change to the return type. Previously, it needed to return the updated set of
    elements; now, because the update is performed by mutation, there is no longer
    any need to return anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, fynd. Its implementation is now remarkably simple. There is no longer
    any need to search through the set. Previously, we had to search because after
    union operations have occurred, the parent reference might have no longer been
    valid. Now, any such changes are automatically reflected by mutation. Hence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 21.1.1Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Look again at fynd. In the some case, the element bound to e is not the set
    name; that is obtained by recursively traversing parent references. As this value
    returns, however, we don’t do anything to reflect this new knowledge! Instead,
    the next time we try to find the parent of this element, we’re going to perform
    this same recursive traversal all over again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using mutation helps address this problem. The idea is as simple as can be:
    compute the value of the parent, and update it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that this update will apply to every element in the recursive chain to
    find the set name. Therefore, applying fynd to any of those elements the next
    time around will benefit from this update. This idea is called path compression.
  prefs: []
  type: TYPE_NORMAL
- en: There is one more interesting idea we can apply. This is to maintain a rank
    of each element, which is roughly the depth of the tree of elements for which
    that element is their set name. When we union two elements, we then make the one
    with larger rank the parent of the one with the smaller rank. This has the effect
    of avoiding growing very tall paths to set name elements, instead tending towards
    “bushy” trees. This too reduces the number of parents that must be traversed to
    find the representative.
  prefs: []
  type: TYPE_NORMAL
- en: 21.1.2Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This optimized union-find data structure has a remarkble analysis. In the worst
    case, of course, we must traverse the entire chain of parents to find the name
    element, which takes time proportional to the number of elements in the set. However,
    once we apply the above optimizations, we never need to traverse that same chain
    again! In particular, if we conduct an amortized analysis over a sequence of set
    equality tests after a collection of union operations, we find that the cost for
    subsequent checks is very small—<wbr>indeed, about as small a function can get
    without being constant. The [actual analysis](http://en.wikipedia.org/wiki/Disjoint-set_data_structure)
    is quite sophisticated; it is also one of the most remarkable algorithm analyses
    in all of computer science.
  prefs: []
  type: TYPE_NORMAL
- en: 21.2Set Membership by Hashing Redux
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already seen solutions to set membership. First we saw how to represent
    sets as lists ([Representing Sets by Lists](set-representations.html#%28part._rep-sets-as-lists%29)),
    then as (balanced) binary trees ([A Fine Balance: Tree Surgery](set-representations.html#%28part._sets-from-balanced-trees%29)).Don’t
    confuse this with union-find, which is a different kind of problem on sets ([Disjoint
    Sets Redux](#%28part._union-find%29)). With this we were able to reduce insertion
    and membership to logarithmic time in the number of elements. Along the way, we
    also learned that the essence of using these representations was to reduce any
    datatype to a comparable, ordered element—<wbr>for efficiency, usually a number
    ([Converting Values to Ordered Values](set-representations.html#%28part._hashing-values%29))—<wbr>which
    we called hashing.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us now ask whether we can use these numbers in any other way. Suppose our
    set has only five elements, which map densely to the values between 0 and 4\.
    We can then have a five element list of boolean values, where the boolean at each
    index of the list indicates whether the element corresponding to that position
    is in the set or not. Both membership and insertion, however, require traversing
    potentially the entire list, giving us solutions linear in the number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: That’s not all. Unless we can be certain that there will be only five elements,
    we can’t be sure to bound the size of the representation. Also, we haven’t yet
    shown how to actually hash in a way that makes the representation dense; barring
    that, our space consumption gets much worse, in turn affecting time.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is, actually, a relatively simple solution to the problem of reducing
    numbers densely to a range: given the hash, we apply modular arithmetic. That
    is, if we want to use a list of five elements to represent the set, we simply
    compute the hash’s modulo five. This gives us an easy solution to that problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Except, of course, not quite: two different hashes could easily have the same
    modulus. That is, suppose we need to record that the set contains the (hash) value
    5; the resulting list would be'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now suppose we want to ask whether the value 15 is in the set; we cannot tell
    from this representation whether it’s in the set or not, because we can’t tell
    whether the true represents 5, 15, 25, or any other value whose modulus 5 is 0\.
    Therefore, we have to record the actual elements in the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can tell that 5 is in the set while 4 is not. However, this now makes
    it impossible to have both 5 and 10 in the set; therefore, our real representation
    needs to be a list at each position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If we also add 10 to the set, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: and now we can tell that both 5 and 10 are in the set, but 15 is not. These
    sub-lists are known as buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Good; now we have another way of representing sets so we can check for membership.
    However, in the worst case one of those lists is going to contain all elements
    in the set, and we may have to traverse the entire list to find an element in
    it, which means membership testing will take time linear in the number of elements.
    Insertion, in turn, takes time proportional to the size of the modulus because
    we may have to traverse the entire outer list to get to the right sub-list.
  prefs: []
  type: TYPE_NORMAL
- en: Can we improve on this?
  prefs: []
  type: TYPE_NORMAL
- en: 21.2.1Improving Access Time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given that we currently have no way of ensuring we won’t get hash collisions,
    for now we’re stuck with a list of elements at each position that could be the
    size of the set we are trying to represent. Therefore, we can’t get around that
    (yet). But, we’re currently paying time in the size of the outer list just to
    insert an element, and surely we can do better than that!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, but it requires a different data structure: the array.There are other
    data structures that will also do better, but the one we’re about to see is important
    and widely used. You can look up arrays in the Pyret documentation. The key characteristics
    of an array are:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the nth element of an array takes constant, not linear, time in n.
    This is sometimes known as random-access, because it takes the same time to access
    any random element, as opposed to just a known element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arrays are updated by mutation. Thus, a change to an array is seen by all references
    to the array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The former property warrants some discussion: how can an array provide random
    access whereas a list requires time linear in the index of the element we’re accessing?
    This is because of a trade-off: a list can be extended indefinitely as the program
    extends, but an array cannot. An array must declare its size up front, and cannot
    grow without copying all the elements into a larger array. Therefore, we should
    only use arrays when we have a clearly identifiable upper-bound on their size
    (and that bound is not too large, or else we may not even be able to find that
    much contiguous space in the system). But the problem we’re working on has exactly
    this characteristic.So let’s try defining sets afresh. We start with an array
    of a fixed size, with each element an empty list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to use modular arithmetic to find the right bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we can determine whether an element is in the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To actually add an element to the set, we put it in the list associated with
    the appropriate bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Checking whether the element is already in the bucket is an important part of
    our complexity argument because we have implicitly assumed there won’t be duplicate
    elements in buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What impact do duplicate elements have on the complexity of operations?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: The data structure we have defined above is known as a hash table (which is
    a slightly confusing name, because it isn’t really a table of hashes, but this
    is the name used conventionally in computer science).
  prefs: []
  type: TYPE_NORMAL
- en: 21.2.2Better Hashing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using arrays therefore appears to address one issue: insertion. Finding the
    relevant bucket takes constant time, linking the new element takes constant time,
    and so the entire operation takes constant time...except, we have to also check
    whether the element is already in the bucket, to avoid storing duplicates. We
    have gotten rid of the traversal through the outer list representing the set,
    but the member operation on the inner list remains unchanged. In principle it
    won’t, but in practice we can make it much better.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that collisions are virtually inevitable. If we have uniformly distributed
    data, then collisions show up sooner than we might expect.This follows from the
    reasoning behind what is known as the [birthday problem](http://en.wikipedia.org/wiki/Birthday_problem),
    commonly presented as how many people need to be in a room before the likelihood
    that two of them share a birthday exceeds some percentage. For the likelihood
    to exceed half we need just 23 people! Therefore, it is wise to prepare for the
    possibility of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: The key is to know something about the distribution of hash values. For instance,
    if we knew our hash values are all multiples of 10, then using a table size of
    10 would be a terrible idea (because all elements would hash to the same bucket,
    turning our hash table into a list). In practice, it is common to use uncommon
    prime numbers as the table size, since a random value is unlikely to have it as
    a divisor. This does not yield a theoretical improvement (unless you can make
    certain assumptions about the input, or work through the math very carefully),
    but it works well in practice. In particular, since the typical hashing function
    uses memory addresses for objects on the heap, and on most systems these addresses
    are multiples of 4, using a prime like 31 is often a fairly good bet.
  prefs: []
  type: TYPE_NORMAL
- en: 21.2.3Bloom Filters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to improve the space and time complexity is to relax the properties
    we expect of the operations. Right now, set membership gives perfect answers,
    in that it answers true exactly when the element being checked was previously
    inserted into the set. But suppose we’re in a setting where we can accept a more
    relaxed notion of correctness, where membership tests can “lie” slightly in one
    direction or the other (but not both, because that makes the representation almost
    useless). Specifically, let’s say that “no means no” (i.e., if the set representation
    says the element isn’t present, it really isn’t) but “yes sometimes means no”
    (i.e., if the set representation says an element is present, sometimes it might
    not be). In short, if the set says the element isn’t in it, this should be guaranteed;
    but if the set says the element is present, it may not be. In the latter case,
    we either need some other—<wbr>more expensive—<wbr>technique to determine truth,
    or we might just not care.
  prefs: []
  type: TYPE_NORMAL
- en: Where is such a data structure of use? Suppose we are building a Web site that
    uses password-based authentication. Because many passwords have been leaked in
    well-publicized breaches, it is safe to assume that hackers have them and will
    guess them. As a result, we want to not allow users to select any of these as
    passwords. We could use a hash-table to reject precisely the known leaked passwords.
    But for efficiency, we could use this imperfect hash instead. If it says “no”,
    then we allow the user to use that password. But if it says “yes”, then either
    they are using a password that has been leaked, or they have an entirely different
    password that, purely by accident, has the same hash value, but no matter; we
    can just disallow that password as well.A related use is for filtering out malicious
    Web sites. The URL shortening system, bitly, [uses it for this purpose](http://word.bitly.com/post/28558800777/dablooms-an-open-source-scalable-counting-bloom).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is in updating databases or memory stores. Suppose we have
    a database of records, which we update frequently. It is often more efficient
    to maintain a journal of changes: i.e., a list that sequentially records all the
    changes that have occurred. At some interval (say overnight), the journal is “flushed”,
    meaning all these changes are applied to the database proper. But that means every
    read operation has become highly inefficient, because it has to check the entire
    journal first (for updates) before accessing the database. Again, here we can
    use this faulty notion of a hash table: if the hash of the record locator says
    “no”, then the record certainly hasn’t been modified and we go directly to the
    database; if it says “yes” then we have to check the journal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen a simple example implementation of this idea earlier,
    when we used a single list (or array) of booleans, with modular arithmetic, to
    represent the set. When the set said 4 was not present, this was absolutely true;
    but when it said 5 and 10 are both present, only one of these was present. The
    advantage was a huge saving in space and time: we needed only one bit per bucket,
    and did not need to search through a list to answer for membership. The downside,
    of course, was a hugely inaccurate set data structure, and one with correlated
    failure tied to the modulus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a simple way to improve this solution: instead of having just one
    array, have several (but a fixed number of them). When an element is added to
    the set, it is added to each array; when checking for membership, every array
    is consulted. The set only answers affirmatively to membership if all the arrays
    do so.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, using multiple arrays offers absolutely no advantage if the arrays
    are all the same size: since both insertion and lookup are deterministic, all
    will yield the same answer. However, there is a simple antidote to this: use different
    array sizes. In particular, by using array sizes that are relatively prime to
    one another, we minimize the odds of a clash (only hashes that are the product
    of all the array sizes will fool the array).'
  prefs: []
  type: TYPE_NORMAL
- en: This data structure, called a Bloom Filter, is a probabilistic data structure.
    Unlike our earlier set data structure, this one is not guaranteed to always give
    the right answer; but contrary to the [☛ space-time tradeoff](glossary.html#%28elem._glossary-space-time._tradeoff%29),
    we save both space and time by changing the problem slightly to accept incorrect
    answers. If we know something about the distribution of hash values, and we have
    some acceptable bound of error, we can design hash table sizes so that with high
    probability, the Bloom Filter will lie within the acceptable error bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3Avoiding Recomputation by Remembering Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have on several instances already referred to a [☛ space-time tradeoff](glossary.html#%28elem._glossary-space-time._tradeoff%29).
    The most obvious tradeoff is when a computation “remembers” prior results and,
    instead of recomputing them, looks them up and returns the answers. This is an
    instance of the tradeoff because it uses space (to remember prior answers) in
    place of time (recomputing the answer). Let’s see how we can write such computations.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.1An Interesting Numeric Sequence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we want to create properly-parenthesized expressions, and ignore all
    non-parenthetical symbols. How many ways are there of creating parenthesized expressions
    given a certain number of opening (equivalently, closing) parentheses?
  prefs: []
  type: TYPE_NORMAL
- en: If we have zero opening parentheses, the only expression we can create is the
    empty expression. If we have one opening parenthesis, the only one we can construct
    is “()” (there must be a closing parenthesis since we’re interested only in properly-parenthesized
    expressions). If we have two opening parentheses, we can construct “(())” and
    “()()”. Given three, we can construct “((()))”, “(())()”, “()(())”, “()()()”,
    and “(()())”, for a total of five. And so on. Observe that the solutions at each
    level use all the possible solutions at one level lower, combined in all the possible
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is actually a famous mathematical sequence that corresponds to the number
    of such expressions, called the [Catalan sequence](http://en.wikipedia.org/wiki/Catalan_number).
    It has the property of growing quite large very quickly: starting from the modest
    origins above, the tenth Catalan number (i.e., tenth element of the Catalan sequence)
    is 16796\. A simple recurrence formula gives us the Catalan number, which we can
    turn into a simple program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This function’s tests look as follows—<wbr><catalan-tests> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   check: |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(0) is 1 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(1) is 1 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(2) is 2 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(3) is 5 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(4) is 14 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(5) is 42 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(6) is 132 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(7) is 429 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(8) is 1430 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(9) is 4862 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(10) is 16796 |'
  prefs: []
  type: TYPE_TB
- en: '|     catalan(11) is 58786 |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: but beware! When we time the function’s execution, we find that the first few
    tests run very quickly, but somewhere between a value of 10 and 20—<wbr>depending
    on your machine and programming language implementation—<wbr>you ought to see
    things start to slow down, first a little, then with extreme effect.
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Check at what value you start to observe a significant slowdown on your machine.
    Plot the graph of running time against input size. What does this suggest?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The reason the Catalan computation takes so long is precisely because of what
    we alluded to earlier: at each level, we depend on computing the Catalan number
    of all the smaller levels; this computation in turn needs the numbers of all of
    its smaller levels; and so on down the road.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Map the subcomputations of catalan to see why the computation time explodes
    as it does. What is the asymptotic time complexity of this function?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 21.3.1.1Using State to Remember Past Answers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Therefore, this is clearly a case where trading space for time is likely to
    be of help. How do we do this? We need a notion of memory that records all previous
    answers and, on subsequent attempts to compute them, checks whether they are already
    known and, if so, just returns them instead of recomputing them.
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What critical assumption is this based on?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Naturally, this assumes that for a given input, the answer will always be the
    same. As we have seen, functions with state violate this liberally, so typical
    stateful functions cannot utilize this optimization. Ironically, we will use state
    to implement this optimization, so we will have a stateful function that always
    returns the same answer on a given input—<wbr>and thereby use state in a stateful
    function to simulate a stateless one. Groovy, dude!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, then, we need some representation of memory. We can imagine several,
    but here’s a simple one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now how does catalan need to change? We have to first look for whether the
    value is already in memory; if it is, we return it without any further computation,
    but if it isn’t, then we compute the result, store it in memory, and then return
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Now running our previous tests will reveal that the answer computes
    much quicker, but in addition we can dare to run bigger computations such as catalan(50).
  prefs: []
  type: TYPE_NORMAL
- en: This process, of converting a function into a version that remembers its past
    answers, is called memoization.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.1.2From a Tree of Computation to a DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What we have subtly done is to convert a tree of computation into a DAG over
    the same computation, with equivalent calls being reused. Whereas previously each
    call was generating lots of recursive calls, which induced still more recursive
    calls, now we are reusing previous recursive calls—<wbr>i.e., sharing the results
    computed earlier. This, in effect, points the recursive call to one that had occurred
    earlier. Thus, the shape of computation converts from a tree to a DAG of calls.
  prefs: []
  type: TYPE_NORMAL
- en: This has an important complexity benefit. Whereas previously we were performing
    a super-exponential number of calls, now we perform only one call per input and
    share all previous calls—<wbr>thereby reducing catalan(n) to take a number of
    fresh calls proportional to n. Looking up the result of a previous call takes
    time proportional to the size of memory (because we’ve represented it as a list;
    better representations would improve on that), but that only contributes another
    linear multiplicative factor, reducing the overall complexity to quadratic in
    the size of the input. This is a dramatic reduction in overall complexity. In
    contrast, other uses of memoization may result in much less dramatic improvements,
    turning the use of this technique into a true engineering trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.1.3The Complexity of Numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we start to run larger computations, however, we may start to notice that
    our computations are starting to take longer than linear growth. This is because
    our numbers are growing arbitrarily large—<wbr>for instance, catalan(100) is 896519947090131496687170070074100632420837521538745909320—<wbr>and
    computations on numbers can no longer be constant time, contrary to what we said
    earlier ([The Size of the Input](predicting-growth.html#%28part._size-of-input%29)).
    Indeed, when working on cryptographic problems, the fact that operations on numbers
    do not take constant time are absolutely critical to fundamental complexity results
    (and, for instance, the presumed unbreakability of contemporary cryptography).
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.1.4Abstracting Memoization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we’ve achieved the desired complexity improvement, but there is still something
    unsatisfactory about the structure of our revised definition of catalan: the act
    of memoization is deeply intertwined with the definition of a Catalan number,
    even though these should be intellectually distinct. Let’s do that next.'
  prefs: []
  type: TYPE_NORMAL
- en: In effect, we want to separate our program into two parts. One part defines
    a general notion of memoization, while the other defines catalan in terms of this
    general notion.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does the former mean? We want to encapsulate the idea of “memory” (since
    we presumably don’t want this stored in a variable that any old part of the program
    can modify). This should result in a function that takes the input we want to
    check; if it is found in the memory we return that answer, otherwise we compute
    the answer, store it, and return it. To compute the answer, we need a function
    that determines how to do so. Putting together these pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the name memoize-1 to indicate that this is a memoizer for single-argument
    functions. Observe that the code above is virtually identical to what we had before,
    except where we had the logic of Catalan number computation, we now have the parameter
    f determining what to do.With this, we can now define catalan as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Note several things about this definition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t write fun catalan(...): ...; because the procedure bound to catalan
    is produced by memoize-1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note carefully that the recursive calls to catalan have to be to the function
    bound to the result of memoization, thereby behaving like an object ([Objects:
    Interpretation and Types](objects.html)). Failing to refer to this same shared
    procedure means the recursive calls will not be memoized, thereby losing the benefit
    of this process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to use rec for reasons we saw earlier [[Recursive Functions](State__Change__and_More_Equality.html#%28part._rec-for-recursive%29)].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each invocation of memoize-1 creates a new table of stored results. Therefore
    the memoization of different functions will each get their own tables rather than
    sharing tables, which is a bad idea!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If in doubt about how state interacts with functions, read [Interaction of
    Mutation with Closures: Counters](State__Change__and_More_Equality.html#%28part._state-closures-counter%29).'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is sharing memoization tables a bad idea? Be concrete.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 21.3.2Edit-Distance for Spelling Correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text editors, word processors, mobile phones, and various other devices now
    routinely implement spelling correction or offer suggestions on (mis-)spellings.
    How do they do this? Doing so requires two capabilities: computing the distance
    between words, and finding words that are nearby according to this metric. In
    this section we will study the first of these questions. (For the purposes of
    this discussion, we will not dwell on the exact definition of what a “word” is,
    and just deal with strings instead. A real system would need to focus on this
    definition in considerable detail.)'
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Think about how you might define the “distance between two words”. Does it define
    a [metric space](http://en.wikipedia.org/wiki/Metric_space)?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Will the definition we give below define a metric space over the set of words?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Though there may be several legitimate ways to define distances between words,
    here we care about the distance in the very specific context of spelling mistakes.
    Given the distance measure, one use might be to compute the distance of a given
    word from all the words in a dictionary, and offer the closest word (i.e., the
    one with the least distance) as a proposed correction.Obviously, we can’t compute
    the distance to every word in a large dictionary on every single entered word.
    Making this process efficient constitutes the other half of this problem. Briefly,
    we need to quickly discard most words as unlikely to be close enough, for which
    a representation such as a [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model)
    (here, a bag of characters) can greatly help. Given such an intended use, we would
    like at least the following to hold:'
  prefs: []
  type: TYPE_NORMAL
- en: That the distance from a word to itself be zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the distance from a word to any word other than itself be strictly positive.
    (Otherwise, given a word that is already in the dictionary, the “correction” might
    be a different dictionary word.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the distance between two words be symmetric, i.e., it shouldn’t matter
    in which order we pass arguments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Observe that we have not included the triangle inequality relative to the properties
    of a metric. Why not? If we don’t need the triangle inequality, does this let
    us define more interesting distance functions that are not metrics?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Given a pair of words, the assumption is that we meant to type one but actually
    typed the other. Here, too, there are several possible definitions, but a popular
    one considers that there are three ways to be fat-fingered:'
  prefs: []
  type: TYPE_NORMAL
- en: we left out a character;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: we typed a character twice; or,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: we typed one character when we meant another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In particular, we are interested in the fewest edits of these forms that need
    to be performed to get from one word to the other. For natural reasons, this notion
    of distance is called the edit distance or, in honor of its creator, the Levenshtein
    distance.See more on [Wikipedia](http://en.wikipedia.org/wiki/Levenshtein_distance).
  prefs: []
  type: TYPE_NORMAL
- en: There are several variations of this definition possible. For now, we will consider
    the simplest one, which assumes that each of these errors has equal cost. For
    certain input devices, we may want to assign different costs to these mistakes;
    we might also assign different costs depending on what wrong character was typed
    (two characters adjacent on a keyboard are much more likely to be a legitimate
    error than two that are far apart). We will return briefly to some of these considerations
    later ([Nature as a Fat-Fingered Typist](#%28part._smith-waterman%29)).
  prefs: []
  type: TYPE_NORMAL
- en: Under this metric, the distance between “kitten” and “sitting” is 3 because
    we have to replace “k” with “s”, replace “e” with “i”, and insert “g” (or symmetrically,
    perform the opposite replacements and delete “g”). Here are more examples:<levenshtein-tests>
    ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   check: |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein(empty, empty) is 0 |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein([list:"x"], [list: "x"]) is 0 |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein([list: "x"], [list: "y"]) is 1 |'
  prefs: []
  type: TYPE_TB
- en: '|     # one of about 600 |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "b", "r", "i", "t", "n", "e", "y"], |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "b", "r", "i", "t", "t", "a", "n", "y"]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 3 |'
  prefs: []
  type: TYPE_TB
- en: '|     # http://en.wikipedia.org/wiki/Levenshtein_distance |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "k", "i", "t", "t", "e", "n"], |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "s", "i", "t", "t", "i", "n", "g"]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 3 |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "k", "i", "t", "t", "e", "n"], |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "k", "i", "t", "t", "e", "n"]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 0 |'
  prefs: []
  type: TYPE_TB
- en: '|     # http://en.wikipedia.org/wiki/Levenshtein_distance |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "S", "u", "n", "d", "a", "y"], |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "S", "a", "t", "u", "r", "d", "a", "y"]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 3 |'
  prefs: []
  type: TYPE_TB
- en: '|     # http://www.merriampark.com/ld.htm |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "g", "u", "m", "b", "o"], |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "g", "a", "m", "b", "o", "l"]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 2 |'
  prefs: []
  type: TYPE_TB
- en: '|     # http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html
    |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "a", "c", "g", "t", "a", "c", "g", "t", "a", "c", "g", "t"],
    |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "a", "c", "a", "t", "a", "c", "t", "t", "g", "t", "a", "c",
    "t"]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 4 |'
  prefs: []
  type: TYPE_TB
- en: '|     levenshtein( |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "s", "u", "p", "e", "r", "c", "a", "l", "i", |'
  prefs: []
  type: TYPE_TB
- en: '|         "f", "r", "a", "g", "i", "l", "i", "s", "t" ], |'
  prefs: []
  type: TYPE_TB
- en: '|       [list: "s", "u", "p", "e", "r", "c", "a", "l", "y", |'
  prefs: []
  type: TYPE_TB
- en: '|         "f", "r", "a", "g", "i", "l", "e", "s", "t" ]) |'
  prefs: []
  type: TYPE_TB
- en: '|       is 2 |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: The basic algorithm is in fact very simple:<levenshtein> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   rec levenshtein :: (List<String>, List<String> -> Number) = |'
  prefs: []
  type: TYPE_TB
- en: '|     [<levenshtein-body>](#%28elem._levenshtein-body%29) |'
  prefs: []
  type: TYPE_TB
- en: where, because there are two list inputs, there are four cases, of which two
    are symmetric:<levenshtein-body> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   lam(s, t): |'
  prefs: []
  type: TYPE_TB
- en: '|     [<levenshtein-both-empty>](#%28elem._levenshtein-both-empty%29) |'
  prefs: []
  type: TYPE_TB
- en: '|     [<levenshtein-one-empty>](#%28elem._levenshtein-one-empty%29) |'
  prefs: []
  type: TYPE_TB
- en: '|     [<levenshtein-neither-empty>](#%28elem._levenshtein-neither-empty%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: If both inputs are empty, the answer is simple:<levenshtein-both-empty> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   if is-empty(s) and is-empty(t): 0 |'
  prefs: []
  type: TYPE_TB
- en: When one is empty, then the edit distance corresponds to the length of the other,
    which needs to inserted (or deleted) in its entirety (so we charge a cost of one
    per character):<levenshtein-one-empty> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   else if is-empty(s): t.length() |'
  prefs: []
  type: TYPE_TB
- en: '|   else if is-empty(t): s.length() |'
  prefs: []
  type: TYPE_TB
- en: If neither is empty, then each has a first character. If they are the same,
    then there is no edit cost associated with this character (which we reflect by
    recurring on the rest of the words without adding to the edit cost). If they are
    not the same, however, we consider each of the possible edits:<levenshtein-neither-empty>
    ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   else: |'
  prefs: []
  type: TYPE_TB
- en: '|     if s.first == t.first: |'
  prefs: []
  type: TYPE_TB
- en: '|       levenshtein(s.rest, t.rest) |'
  prefs: []
  type: TYPE_TB
- en: '|     else: |'
  prefs: []
  type: TYPE_TB
- en: '|       min3( |'
  prefs: []
  type: TYPE_TB
- en: '|         1 + levenshtein(s.rest, t), |'
  prefs: []
  type: TYPE_TB
- en: '|         1 + levenshtein(s, t.rest), |'
  prefs: []
  type: TYPE_TB
- en: '|         1 + levenshtein(s.rest, t.rest)) |'
  prefs: []
  type: TYPE_TB
- en: '|     end |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: 'In the first case, we assume s has one too many characters, so we compute the
    cost as if we’re deleting it and finding the lowest cost for the rest of the strings
    (but charging one for this deletion); in the second case, we symmetrically assume
    t has one too many; and in the third case, we assume one character got replaced
    by another, so we charge one but consider the rest of both words (e.g., assume
    “s” was typed for “k” and continue with “itten” and “itting”). This uses the following
    helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This algorithm will indeed pass all the tests we have written above, but with
    a problem: the running time grows exponentially. That is because, each time we
    find a mismatch, we recur on three subproblems. In principle, therefore, the algorithm
    takes time proportional to three to the power of the length of the shorter word.
    In practice, any prefix that matches causes no branching, so it is mismatches
    that incur branching (thus, confirming that the distance of a word with itself
    is zero only takes time linear in the size of the word).'
  prefs: []
  type: TYPE_NORMAL
- en: Observe, however, that many of these subproblems are the same. For instance,
    given “kitten” and “sitting”, the mismatch on the initial character will cause
    the algorithm to compute the distance of “itten” from “itting” but also “itten”
    from “sitting” and “kitten” from “itting”. Those latter two distance computations
    will also involve matching “itten” against “itting”. Thus, again, we want the
    computation tree to turn into a DAG of expressions that are actually evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution, therefore, is naturally to memoize. First, we need a memoizer
    that works over two arguments rather than one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Most of the code is unchanged, except that we store two arguments rather than
    one, and correspondingly look up both.With this, we can redefine levenshtein to
    use memoization:<levenshtein-memo> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   rec levenshtein :: (List<String>, List<String> -> Number) = |'
  prefs: []
  type: TYPE_TB
- en: '|     memoize-2( |'
  prefs: []
  type: TYPE_TB
- en: '|       lam(s, t): |'
  prefs: []
  type: TYPE_TB
- en: '|         if is-empty(s) and is-empty(t): 0 |'
  prefs: []
  type: TYPE_TB
- en: '|         else if is-empty(s): t.length() |'
  prefs: []
  type: TYPE_TB
- en: '|         else if is-empty(t): s.length() |'
  prefs: []
  type: TYPE_TB
- en: '|         else: |'
  prefs: []
  type: TYPE_TB
- en: '|           if s.first == t.first: |'
  prefs: []
  type: TYPE_TB
- en: '|             levenshtein(s.rest, t.rest) |'
  prefs: []
  type: TYPE_TB
- en: '|           else: |'
  prefs: []
  type: TYPE_TB
- en: '|             min3( |'
  prefs: []
  type: TYPE_TB
- en: '|               1 + levenshtein(s.rest, t), |'
  prefs: []
  type: TYPE_TB
- en: '|               1 + levenshtein(s, t.rest), |'
  prefs: []
  type: TYPE_TB
- en: '|               1 + levenshtein(s.rest, t.rest)) |'
  prefs: []
  type: TYPE_TB
- en: '|           end |'
  prefs: []
  type: TYPE_TB
- en: '|         end |'
  prefs: []
  type: TYPE_TB
- en: '|       end) |'
  prefs: []
  type: TYPE_TB
- en: where the argument to memoize-2 is precisely what we saw earlier as [<levenshtein-body>](#%28elem._levenshtein-body%29)
    (and now you know why we defined levenshtein slightly oddly, not using fun).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complexity of this algorithm is still non-trivial. First, let’s introduce
    the term suffix: the suffix of a string is the rest of the string starting from
    any point in the string. (Thus “kitten”, “itten”, “ten”, “n”, and “” are all suffixes
    of “kitten”.) Now, observe that in the worst case, starting with every suffix
    in the first word, we may need to perform a comparison against every suffix in
    the second word. Fortunately, for each of these suffixes we perform a constant
    computation relative to the recursion. Therefore, the overall time complexity
    of computing the distance between strings of length \(m\) and \(n\) is \(O([m,
    n \rightarrow m \cdot n])\). (We will return to space consumption later [[Contrasting
    Memoization and Dynamic Programming](#%28part._memo-vs-dp%29)].)'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Modify the above algorithm to produce an actual (optimal) sequence of edit operations.
    This is sometimes known as the traceback.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 21.3.3Nature as a Fat-Fingered Typist
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have talked about how to address mistakes made by humans. However, humans
    are not the only bad typists: nature is one, too!'
  prefs: []
  type: TYPE_NORMAL
- en: 'When studying living matter we obtain sequences of amino acids and other such
    chemicals that comprise molecules, such as DNA, that hold important and potentially
    determinative information about the organism. These sequences consist of similar
    fragments that we wish to identify because they represent relationships in the
    organism’s behavior or evolution.This section may need to be skipped in [some
    states and countries](http://en.wikipedia.org/wiki/Creation_and_evolution_in_public_education).
    Unfortunately, these sequences are never identical: like all low-level programmers,
    nature slips up and sometimes makes mistakes in copying (called—<wbr>wait for
    it—<wbr>mutations). Therefore, looking for strict equality would rule out too
    many sequences that are almost certainly equivalent. Instead, we must perform
    an alignment step to find these equivalent sequences. As you might have guessed,
    this process is very much a process of computing an edit distance, and using some
    threshold to determine whether the edit is small enough.To be precise, we are
    performing local [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment).
    This algorithm is named, after its creators, Smith-Waterman, and because it is
    essentially identical, has the same complexity as the Levenshtein algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only difference between traditional presentations Levenshtein and Smith-Waterman
    is something we alluded to earlier: why is every edit given a distance of one?
    Instead, in the Smith-Waterman presentation, we assume that we have a function
    that gives us the gap score, i.e., the value to assign every character’s alignment,
    i.e., scores for both matches and edits, with scores driven by biological considerations.
    Of course, as we have already noted, this need is not peculiar to biology; we
    could just as well use a “gap score” to reflect the likelihood of a substitution
    based on keyboard characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.4Dynamic Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have used memoization as our canonical means of saving the values of past
    computations to reuse later. There is another popular technique for doing this
    called dynamic programming. This technique is closely related to memoization;
    indeed, it can be viewed as the dual method for achieving the same end. First
    we will see dynamic programming at work, then discuss how it differs from memoization.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming also proceeds by building up a memory of answers, and looking
    them up instead of recomputing them. As such, it too is a process for turning
    a computation’s shape from a tree to a DAG of actual calls. The key difference
    is that instead of starting with the largest computation and recurring to smaller
    ones, it starts with the smallest computations and builds outward to larger ones.
  prefs: []
  type: TYPE_NORMAL
- en: We will revisit our previous examples in light of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.4.1Catalan Numbers with Dynamic Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To begin with, we need to define a data structure to hold answers. Following
    convention, we will use an array.What happens when we run out of space? We can
    use the doubling technique we studied for [Halloween Analysis](amortized-analysis.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the catalan function simply looks up the answer in this array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'But how do we fill the array? We initialize the one known value, and use the
    formula to compute the rest in incremental order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The resulting program obeys the tests in [<catalan-tests>](#%28elem._catalan-tests%29).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have had to undo the natural recursive definition—<wbr>which
    proceeds from bigger values to smaller ones—<wbr>to instead use a loop that goes
    from smaller values to larger ones. In principle, the program has the danger that
    when we apply catalan to some value, that index of answers will have not yet been
    initialized, resultingin an error. In fact, however, we know that because we fill
    all smaller indices in answers before computing the next larger one, we will never
    actually encounter this error. Note that this requires careful reasoning about
    our program, which we did not need to perform when using memoization because there
    we made precisely the recursive call we needed, which either looked up the value
    or computed it afresh.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.4.2Levenshtein Distance and Dynamic Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s take on rewriting the Levenshtein distance computation:<levenshtein-dp>
    ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   fun levenshtein(s1 :: List<String>, s2 :: List<String>): |'
  prefs: []
  type: TYPE_TB
- en: '|     [<levenshtein-dp/1>](#%28elem._levenshtein-dp%2F1%29) |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: 'We will use a table representing the edit distance for each prefix of each
    word. That is, we will have a two-dimensional table with as many rows as the length
    of s1 and as many columns as the length of s2. At each position, we will record
    the edit distance for the prefixes of s1 and s2 up to the indices represented
    by that position in the table.Note that index arithmetic will be a constant burden:
    if a word is of length \(n\), we have to record the edit distance to its \(n +
    1\) positions, the extra one corresponding to the empty word. This will hold for
    both words:<levenshtein-dp/1> ::='
  prefs: []
  type: TYPE_NORMAL
- en: '|   s1-len = s1.length() |'
  prefs: []
  type: TYPE_TB
- en: '|   s2-len = s2.length() |'
  prefs: []
  type: TYPE_TB
- en: '|   answers = array2d(s1-len + 1, s2-len + 1, none) |'
  prefs: []
  type: TYPE_TB
- en: '|   [<levenshtein-dp/2>](#%28elem._levenshtein-dp%2F2%29) |'
  prefs: []
  type: TYPE_TB
- en: Observe that by creating answers inside levenshtein, we can determine the exact
    size it needs to be based on the inputs, rather than having to over-allocate or
    dynamically grow the array.We have initialized the table with none, so we will
    get an error if we accidentally try to use an uninitialized entry.Which proved
    to be necessary when writing and debugging this code! It will therefore be convenient
    to create helper functions that let us pretend the table contains only numbers:<levenshtein-dp/2>
    ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   fun put(s1-idx :: Number, s2-idx :: Number, n :: Number): |'
  prefs: []
  type: TYPE_TB
- en: '|     answers.set(s1-idx, s2-idx, some(n)) |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: '|   fun lookup(s1-idx :: Number, s2-idx :: Number) -> Number: |'
  prefs: []
  type: TYPE_TB
- en: '|     a = answers.get(s1-idx, s2-idx) |'
  prefs: []
  type: TYPE_TB
- en: '|     cases (Option) a: |'
  prefs: []
  type: TYPE_TB
- en: '|       &#124; none => raise("looking at uninitialized value") |'
  prefs: []
  type: TYPE_TB
- en: '|       &#124; some(v) => v |'
  prefs: []
  type: TYPE_TB
- en: '|     end |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: Now we have to populate the array. First, we initialize the row representing
    the edit distances when s2 is empty, and the column where s1 is empty. At \((0,
    0)\), the edit distance is zero; at every position thereafter, it is the distance
    of that position from zero, because that many characters must be added to one
    or deleted from the other word for the two to coincide:<levenshtein-dp/3> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   for each(s1i from range(0, s1-len + 1)): |'
  prefs: []
  type: TYPE_TB
- en: '|     put(s1i, 0, s1i) |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: '|   for each(s2i from range(0, s2-len + 1)): |'
  prefs: []
  type: TYPE_TB
- en: '|     put(0, s2i, s2i) |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: '|   [<levenshtein-dp/4>](#%28elem._levenshtein-dp%2F4%29) |'
  prefs: []
  type: TYPE_TB
- en: Now we finally get to the heart of the computation. We need to iterate over
    every character in each word. these characters are at indices 0 to s1-len - 1
    and s2-len - 1, which are precisely the ranges of values produced by range(0,
    s1-len) and range(0, s2-len).<levenshtein-dp/4> ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   for each(s1i from range(0, s1-len)): |'
  prefs: []
  type: TYPE_TB
- en: '|     for each(s2i from range(0, s2-len)): |'
  prefs: []
  type: TYPE_TB
- en: '|     [<levenshtein-dp/compute-dist>](#%28elem._levenshtein-dp%2Fcompute-dist%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '|     end |'
  prefs: []
  type: TYPE_TB
- en: '|   end |'
  prefs: []
  type: TYPE_TB
- en: '|   [<levenshtein-dp/get-result>](#%28elem._levenshtein-dp%2Fget-result%29)
    |'
  prefs: []
  type: TYPE_TB
- en: Note that we’re building our way “out” from small cases to large ones, rather
    than starting with the large input and working our way “down”, recursively, to
    small ones.
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is this strictly true?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: No, it isn’t. We did first fill in values for the “borders” of the table. This
    is because doing so in the midst of [<levenshtein-dp/compute-dist>](#%28elem._levenshtein-dp%2Fcompute-dist%29)
    would be much more annoying. By initializing all the known values, we keep the
    core computation cleaner. But it does mean the order in which we fill in the table
    is fairly complex.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s return to computing the distance. For each pair of positions, we
    want the edit distance between the pair of words up to and including those positions.
    This distance is given by checking whether the characters at the pair of positions
    are identical. If they are, then the distance is the same as it was for the previous
    pair of prefixes; otherwise we have to try the three different kinds of edits:<levenshtein-dp/compute-dist>
    ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   dist = |'
  prefs: []
  type: TYPE_TB
- en: '|     if index(s1, s1i) == index(s2, s2i): |'
  prefs: []
  type: TYPE_TB
- en: '|       lookup(s1i, s2i) |'
  prefs: []
  type: TYPE_TB
- en: '|     else: |'
  prefs: []
  type: TYPE_TB
- en: '|       min3( |'
  prefs: []
  type: TYPE_TB
- en: '|         1 + lookup(s1i, s2i + 1), |'
  prefs: []
  type: TYPE_TB
- en: '|         1 + lookup(s1i + 1, s2i), |'
  prefs: []
  type: TYPE_TB
- en: '|         1 + lookup(s1i, s2i)) |'
  prefs: []
  type: TYPE_TB
- en: '|     end |'
  prefs: []
  type: TYPE_TB
- en: '|   put(s1i + 1, s2i + 1, dist) |'
  prefs: []
  type: TYPE_TB
- en: As an aside, this sort of “off-by-one” coordinate arithmetic is traditional
    when using tabular representations, because we write code in terms of elements
    that are not inherently present, and therefore have to create a padded table to
    hold values for the boundary conditions. The alternative would be to allow the
    table to begin its addressing from -1 so that the main computation looks traditional.At
    any rate, when this computation is done, the entire table has been filled with
    values. We still have to read out the answer, with lies at the end of the table:<levenshtein-dp/get-result>
    ::=
  prefs: []
  type: TYPE_NORMAL
- en: '|   lookup(s1-len, s2-len) |'
  prefs: []
  type: TYPE_TB
- en: Even putting aside the helper functions we wrote to satiate our paranoia about
    using undefined values, we end up with:As of this writing, the [current version](http://en.wikipedia.org/w/index.php?title=Levenshtein_distance&oldid=581406185#Iterative_with_full_matrix)
    of the [Wikipedia page](http://en.wikipedia.org/wiki/Levenshtein_distance) on
    the Levenshtein distance features a dynamic programming version that is very similar
    to the code above. By writing in pseudocode, it avoids address arithmetic issues
    (observe how the words are indexed starting from 1 instead of 0, which enables
    the body of the code to look more “normal”), and by initializing all elements
    to zero it permits subtle bugs because an uninitialized table element is indistinguishable
    from a legitimate entry with edit distance of zero. The page also shows the [recursive](http://en.wikipedia.org/w/index.php?title=Levenshtein_distance&oldid=581406185#Recursive)
    solution and alludes to memoization, but does not show it in code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: which is worth contrasting with the memoized version ([<levenshtein-memo>](#%28elem._levenshtein-memo%29)).For
    more examples of canonical dynamic programming problems, see [this page](http://people.csail.mit.edu/bdean/6.046/dp/)
    and think about how each can be expressed as a direct recursion.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3.5Contrasting Memoization and Dynamic Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve seen two very techniques for avoiding recomputation, it’s worth
    contrasting them. The important thing to note is that memoization is a much simpler
    technique: write the natural recursive definition; determine its space complexity;
    decide whether this is problematic enough to warrant a space-time trade-off; and
    if it is, apply memoization. The code remains clean, and subsequent readers and
    maintainers will be grateful for that. In contrast, dynamic programming requires
    a reorganization of the algorithm to work bottom-up, which can often make the
    code harder to follow and full of subtle invariants about boundary conditions
    and computation order.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, the dynamic programming solution can sometimes be more computationally
    efficient. For instance, in the Levenshtein case, observe that at each table element,
    we (at most) only ever use the ones that are from the previous row and column.
    That means we never need to store the entire table; we can retain just the fringe
    of the table, which reduces space to being proportional to the sum, rather than
    product, of the length of the words. In a computational biology setting (when
    using Smith-Waterman), for instance, this saving can be substantial. This optimization
    is essentially impossible for memoization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In more detail, here’s the contrast:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Memoization |  | Dynamic Programming |'
  prefs: []
  type: TYPE_TB
- en: '| Top-down |  | Bottom-up |'
  prefs: []
  type: TYPE_TB
- en: '| Depth-first |  | Breadth-first |'
  prefs: []
  type: TYPE_TB
- en: '| Black-box |  | Requires code reorganization |'
  prefs: []
  type: TYPE_TB
- en: '| All stored calls are necessary |  | May do unnecessary computation |'
  prefs: []
  type: TYPE_TB
- en: '| Cannot easily get rid of unnecessary data |  | Can more easily get rid of
    unnecessary data |'
  prefs: []
  type: TYPE_TB
- en: '| Can never accidentally use an uninitialized answer |  | Can accidentally
    use an uninitialized answer |'
  prefs: []
  type: TYPE_TB
- en: '| Needs to check for the presence of an answer |  | Can be designed to not
    need to check for the presence of an answer |'
  prefs: []
  type: TYPE_TB
- en: As this table should make clear, these are essentialy dual approaches. What
    is perhaps left unstated in most dynamic programming descriptions is that it,
    too, is predicated on the computation always producing the same answer for a given
    input—<wbr>i.e., being a pure function.
  prefs: []
  type: TYPE_NORMAL
- en: From a software design perspective, there are two more considerations.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the performance of a memoized solution can trail that of dynamic programming
    when the memoized solution uses a generic data structure to store the memo table,
    whereas a dynamic programming solution will invariably use a custom data structure
    (since the code needs to be rewritten against it anyway). Therefore, before switching
    to dynamic programming for performance reasons, it makes sense to try to create
    a custom memoizer for the problem: the same knowledge embodied in the dynamic
    programming version can often be encoded in this custom memoizer (e.g., using
    an array instead of list to improve access times). This way, the program can enjoy
    speed comparable to that of dynamic programming while retaining readability and
    maintainability.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, suppose space is an important consideration and the dynamic programming
    version can make use of significantly less space. Then it does make sense to employ
    dynamic programming instead. Does this mean the memoized version is useless?
  prefs: []
  type: TYPE_NORMAL
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What do you think? Do we still have use for the memoized version?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yes, of course we do! It can serve as an oracle [REF] for the dynamic programming
    version, since the two are supposed to produce identical answers anyway—<wbr>and
    the memoized version would be a much more efficient oracle than the purely recursive
    implemenation, and can therefore be used to test the dynamic programming version
    on much larger inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In short, always first produce the memoized version. If you need more performance,
    consider customizing the memoizer’s data structure. If you need to also save space,
    and can arrive at a more space-efficient dynamic programming solution, then keep
    both versions around, using the former to test the latter (the person who inherits
    your code and needs to alter it will thank you!).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We have characterized the fundamental difference between memoization and dynamic
    programming as that between top-down, depth-first and bottom-up, breadth-first
    computation. This should naturally raise the question, what about:'
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: top-down, breadth-first
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: bottom-up, depth-first
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: orders of computation. Do they also have special names that we just happen to
    not know? Are they uninteresting? Or do they not get discussed for a reason?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
