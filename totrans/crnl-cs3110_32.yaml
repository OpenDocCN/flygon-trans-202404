- en: Review of Asymptotic Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: <menu>*   [Asymptotic Analysis](#1)*   [Worst-Case and Average-Case Analysis](#2)*   [Order
    of Growth and Big-O Notation](#3)*   [Comparing Orders of Growth](#4)*   [Binary
    Search Trees](#6)*   [Exercises](#Exercises)</menu>
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When analyzing the running time or space usage of programs, we usually try to
    estimate the time or space as function of the input size. For example, when analyzing
    the worst case running time of a function that sorts a list of numbers, we will
    be concerned with how long it takes as a function of the length of the input list. 
    For example, we say the standard insertion sort takes time *T*(*n*) where *T*(*n*)*=
    c*n*²*+k* for some constants *c and k*.  In contrast, merge sort takes time *T* ′*(*n*)
    *= c*′***n*log*[2](*n*) + *k*′*.***
  prefs: []
  type: TYPE_NORMAL
- en: '**The **asymptotic** behavior of a function *f(n)* (such as *f(n)=c*n* or *f(n)=c*n²*,
    etc.)refers to the growth of *f(n)* as *n* gets large. We typically ignore small
    values of *n*, since we are usually interested in estimating how slow the program
    will be on large inputs. A good rule of thumb is: the slower the asymptotic growth
    rate, the better the algorithm (although this is often not the whole story).'
  prefs: []
  type: TYPE_NORMAL
- en: By this measure, a linear algorithm (*i.e., f(n)=d*n+k*) is always asymptotically
    better than a quadratic one (*e.g., f(n)=c*n²+q*). That is because for any given
    (positive) *c,k,d*, and *q* there is always some *n* at which the magnitude of
    *c*n²+q* overtakes *d*n+k*. For moderate values of *n*, the quadratic algorithm
    could very well take less time than the linear one, for example if *c* is significantly
    smaller than *d* and/or *k* is significantly smaller than *q*. However, the linear
    algorithm will always be better for sufficiently large inputs. Remember to **THINK
    BIG** when working with asymptotic rates of growth.
  prefs: []
  type: TYPE_NORMAL
- en: Worst-Case and Average-Case Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we say that an algorithm runs in time *T(n)*, we mean that *T(n)* is an
    upper bound on the running time that holds for all inputs of size *n*. This is
    called *worst-case analysis*. The algorithm may very well take less time on some
    inputs of size *n*, but it doesn't matter. If an algorithm takes *T(n)=c*n²+k*
    steps on only a single input of each size *n* and only *n* steps on the rest,
    we still say that it is a quadratic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A popular alternative to worst-case analysis is *average-case analysis*. Here
    we do not bound the worst case running time, but try to calculate the expected
    time spent on a randomly chosen input. This kind of analysis is generally harder,
    since it involves probabilistic arguments and often requires assumptions about
    the distribution of inputs that may be difficult to justify. On the other hand,
    it can be more useful because sometimes the worst-case behavior of an algorithm
    is misleadingly bad. A good example of this is the popular quicksort algorithm,
    whose worst-case running time on an input sequence of length *n* is proportional
    to *n*² but whose expected running time is proportional to *n* log *n.*
  prefs: []
  type: TYPE_NORMAL
- en: Order of Growth and Big-O Notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In estimating the running time of `insert_sort` (or any other program) we don't
    know what the constants *c* or *k* are. We know that it is a constant of moderate
    size, but other than that it is not important; we have enough evidence from the
    asymptotic analysis to know that a `merge_sort` (see below) is faster than the
    quadratic `insert_sort`, even though the constants may differ somewhat. (This
    does not always hold; the constants can sometimes make a difference, but in general
    it is a very good rule of thumb.)
  prefs: []
  type: TYPE_NORMAL
- en: We may not even be able to measure the constant *c* directly. For example, we
    may know that a given expression of the language, such as `if`, takes a constant
    number of machine instructions, but we may not know exactly how many. Moreover,
    the same sequence of instructions executed on a Pentium IV will take less time
    than on a Pentium II (although the difference will be roughly a constant factor).
    So these estimates are usually only accurate up to a constant factor anyway. For
    these reasons, we usually ignore constant factors in comparing asymptotic running
    times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer scientists have developed a convenient notation for hiding the constant
    factor. We write *O(n)* (read: ''''order *n*'''') instead of ''''*cn* for some
    constant *c*.'''' Thus an algorithm is said to be *O(n)* or *linear time* if there
    is a fixed constant *c* such that for all sufficiently large *n*, the algorithm
    takes time at most *cn* on inputs of size *n*. An algorithm is said to be *O(n²)*
    or *quadratic time* if there is a fixed constant *c* such that for all sufficiently
    large *n*, the algorithm takes time at most *cn²* on inputs of size *n*. *O(1)*
    means *constant time*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Polynomial time* means *n^(O(1))*, or *n^c* for some constant *c*. Thus any
    constant, linear, quadratic, or cubic (*O(n³)*) time algorithm is a polynomial-time
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: This is called *big-O notation*. It concisely captures the important differences
    in the asymptotic growth rates of functions.
  prefs: []
  type: TYPE_NORMAL
- en: One important advantage of big-O notation is that it makes algorithms much easier
    to analyze, since we can conveniently ignore low-order terms. For example, an
    algorithm that runs in time
  prefs: []
  type: TYPE_NORMAL
- en: '*10n³ + 24n² + 3n log n + 144*'
  prefs: []
  type: TYPE_NORMAL
- en: is still a cubic algorithm, since
  prefs: []
  type: TYPE_NORMAL
- en: '*10n³ + 24n² + 3n log n + 144'
  prefs: []
  type: TYPE_NORMAL
- en: <= 10n³ + 24n³ + 3n³ + 144n³
  prefs: []
  type: TYPE_NORMAL
- en: <= (10 + 24 + 3 + 144)n³
  prefs: []
  type: TYPE_NORMAL
- en: = O(n³)*.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, since we are ignoring constant factors, any two linear algorithms
    will be considered equally good by this measure. There may even be some situations
    in which the constant is so huge in a linear algorithm that even an exponential
    algorithm with a small constant may be preferable in practice. This is a valid
    criticism of asymptotic analysis and big-O notation. However, as a rule of thumb
    it has served us well. Just be aware that it is *only* a rule of thumb--the asymptotically
    optimal algorithm is not necessarily the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Some common orders of growth seen often in complexity analysis are
  prefs: []
  type: TYPE_NORMAL
- en: '| *O(1)* | constant |'
  prefs: []
  type: TYPE_TB
- en: '| *O(log n)* | logarithmic |'
  prefs: []
  type: TYPE_TB
- en: '| *O(n)* | linear |'
  prefs: []
  type: TYPE_TB
- en: '| *O(n log n)* | "n log n" |'
  prefs: []
  type: TYPE_TB
- en: '| *O(n²)* | quadratic |'
  prefs: []
  type: TYPE_TB
- en: '| *O(n³)* | cubic |'
  prefs: []
  type: TYPE_TB
- en: Here *log* means *log[2]* or the logarithm base 2, although the logarithm base
    doesn't really matter since logarithms with different bases differ by a constant
    factor. Note also that *2^(O(n))* and *O(2^n)* are not the same!
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Orders of Growth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**O**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *f* and *g* be functions from positive integers to positive integers. We
    say *f* is *O(g(n))* (read: ''''*f* is order *g*'''') if *g* is an upper bound
    on *f*:  there exist a positive constant *c* and a positive constant *n*[0], such
    that for all *n≥n*[0],'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(n) ≤ cg(n)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Equivalently, *f* is *O(g(n))* if the function *f(n)/g(n)* is bounded above
    by some constant.
  prefs: []
  type: TYPE_NORMAL
- en: '**o**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We say *f* is *o(g(n))* (read: "*f* is little-o of *g*'''') if for all positive
    constants *c*, there exists a positive constant *n*[0], such that for all *n≥n*[0],'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(n) ≤ cg(n)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Equivalently, *f* is *o(g)* if the function *f(n)/g(n)* tends to 0 as *n* tends
    to infinity. That is, f is small compared to g. If *f* is *o(g)* then *f* is also
    *O(g)*
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between the definitions of O and o is that the bound in
    O holds for *some* constant *c*, but the bound in o holds for *all* constants
    *c*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ω**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We say that *f* is Ω(*g*(*n*)) (read: "f is omega of g") if *g* is a *lower*
    bound on *f* for large *n.* Formally, f is Ω(g) if there is a fixed constant *c*
    and a fixed *n*[0] such that for all *n*>*n*[0],'
  prefs: []
  type: TYPE_NORMAL
- en: '*c**g*(*n*) *≤* *f*(*n*)'
  prefs: []
  type: TYPE_NORMAL
- en: For example, any polynomial whose highest exponent is *n^k* is Ω(*n^k*). If
    *f*(*n*) is Ω(*g*(*n*)) then *g(n)* is O(*f*(*n*)). If *f*(*n*) is *o*(*g*(*n*))
    then *f(n)* is *not* Ω(*g*(*n*)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Θ**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We say that *f* is Θ(*g(n)*) (read: "f is theta of g") if *g* is an accurate
    characterization of *f* for large *n:* it can be scaled so it is both an upper
    and a lower bound of *f*. That is, *f* is both O(*g*(*n*)) and Ω(*g*(*n*)). Expanding
    out the definitions of  Ω and *O*, *f* is Θ(*g*(*n*)) if there are fixed constants
    *c*[1] and *c*[2] and a fixed *n*[0] such that for all *n*>*n*[0],'
  prefs: []
  type: TYPE_NORMAL
- en: '*c*[1]*g*(*n*) *≤* *f*(*n*) *≤* *c*[2] *g*(*n*)'
  prefs: []
  type: TYPE_NORMAL
- en: For example, any polynomial whose highest exponent is *n^k* is  Θ(*n^k*). If
    *f* is Θ(g), then it is *O*(*g*) but not *o*(*g*). Sometimes people use *O*(*g*(*n*))
    a bit informally to mean the stronger property Θ(*g*(*n*)); however, the two are
    different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n + log n* is *O(n)* andQ(*n*), because for all *n > 1*, *n* < *n + log n
    < 2n*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n^(1000)* is *o(2^n)*, because *n^(1000)/2^n* tends to 0 as *n* tends to infinity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For any fixed but arbitrarily small real number *c*, *n log n* is *o(n^(1+c))*,
    since *n log n / n^(1+c)* tends to 0\. To see this, take the logarithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*log(n log n / n^(1+c))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: = log(n log n) - log(n^(1+c))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: = log n + log log n - (1+c)log n
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: = log log n - c log n*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and observe that it tends to negative infinity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The meaning of an expression like *O*(*n*²) is really a set of functions: all
    the functions that are *O*(*n*²). When we say that *f(n)* is *O(n*²*)*, we mean
    that *f(n)* is a member of this set. It is also common to write this as *f*(*n*)
    = *O*(*g*(*n*)) although it is not really an equality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now introduce some convenient rules for manipulating expressions involving
    order notation. These rules, which we state without proof, are useful for working
    with orders of growth. They are really statements about sets of functions. For
    example, we can read #2 as saying that the product of any two functions in *O*(*f*(*n*))
    and *O*(*g*(*n*)) is in *O*(*f*(*n*)*g*(*n*)).'
  prefs: []
  type: TYPE_NORMAL
- en: '*cn^m = O(n^k)* for any constant *c* and any *m ≤ k*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*O(f(n)) + O(g(n)) = O(f(n) + g(n))*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*O(f(n))O(g(n)) = O(f(n)g(n))*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*O(cf(n)) = O(f(n))* for any constant *c*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*c* is *O(1)* for any constant *c*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*log[b]n = O(log n)* for any base *b*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All of these rules (except #1) also hold for Θ as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1\. Write an implementation of stacks using lists.  What is the big-O running
    time of each operation?  The signature for stacks is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Write an implementation of queues using lists.  What is the big-O running
    time of each operation?  The signature for queues is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3.  Write some of the functions that occur in the List structure by hand (e.g.,
    rev, @, map, foldl, etc.) and analyze them to determine their big-O running time.**
  prefs: []
  type: TYPE_NORMAL
