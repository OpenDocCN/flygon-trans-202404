- en: '[Map](data_mining_map.htm) > [Data Science](data_mining.htm) > [Predicting
    the Future](predicting_the_future.htm) > [Modeling](modeling.htm) > [Classification](classification.htm)/[Regression](regression.htm)
    > Artificial Neural Network'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An artificial neutral network (**ANN**) is a system that is based on the biological
    neural network, such as the brain. The brain has approximately 100 billion neurons,
    which communicate through electro-chemical signals. The neurons are connected
    through junctions called synapses. Each neuron receives thousands of connections
    with other neurons, constantly receiving incoming signals to reach the cell body.
    If the resulting sum of the signals surpasses a certain threshold, a response
    is sent through the axon. The ANN attempts to recreate the computational mirror
    of the biological neural network, although it is not comparable since the number
    and complexity of neurons and the used in a biological neural network is many
    times more than those in an artificial neutral network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bc37b190a0fddf8c97fa0e19348c650.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An ANN is comprised of a network of artificial neurons (also known as "nodes").
    These nodes are connected to each other, and the strength of their connections
    to one another is assigned a value based on their strength: inhibition (maximum
    being -1.0) or excitation (maximum being +1.0). If the value of the connection
    is high, then it indicates that there is a strong connection. Within each node''s
    design, a transfer function is built in. There are three types of neurons in an
    ANN, **input nodes**, **hidden nodes**, and **output nodes**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38a7b1aad71ea24d0e9a5cfa61833627.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The input nodes take in information, in the form which can be numerically expressed.
    The information is presented as activation values, where each node is given a
    number, the higher the number, the greater the activation. This information is
    then passed throughout the network. Based on the connection strengths (**weights**),
    inhibition or excitation, and transfer functions, the activation value is passed
    from node to node. Each of the nodes sums the activation values it receives; it
    then modifies the value based on its transfer function. The activation flows through
    the network, through hidden layers, until it reaches the output nodes. The output
    nodes then reflect the input in a meaningful way to the outside world. The difference
    between predicted value and actual value (error) will be propagated backward by
    apportioning them to each node's weights according to the amount of this error
    the node is responsible for (e.g., [gradient descent algorithm](gradient_descent.htm)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39405304bf2994968032432a87ed48f5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Transfer (Activation) Functions**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transfer function translates the input signals to output signals. Four types
    of transfer functions are commonly used, Unit step (threshold), sigmoid, piecewise
    linear, and Gaussian.  **Unit step (threshold)**The output is set at one of two
    levels, depending on whether the total input is greater than or less than some
    threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cd5d7873ac35d5f96c404a0b04f79cd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Sigmoid**The sigmoid function consists of 2 functions, *logistic* and *tangential*.
    The values of logistic function range from 0 and 1 and -1 to +1 for tangential
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6db5fcf324f044f7229014ca4504c5e9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Piecewise Linear **The output is proportional to the total weighted output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/071f402bb31b4d22b06170e1f409c5ae.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Gaussian**Gaussian functions are bell-shaped curves that are continuous.
    The node output (high/low) is interpreted in terms of class membership (1/0),
    depending on how close the net input is to a chosen value of average.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27bc9ccc416ab2a23634c8dc00256020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Linear** Like a linear regression, a linear activation function transforms
    the weighted sum inputs of the neuron to an output using a linear function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f697e4f0109680bb1d7c1b68e8b637f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Algorithm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different types of neural networks, but they are generally classified
    into feed-forward and feed-back networks. A **feed-forward network** is a non-recurrent
    network which contains inputs, outputs, and hidden layers; the signals can only
    travel in one direction. Input data is passed onto a layer of processing elements
    where it performs calculations. Each processing element makes its computation
    based upon a weighted sum of its inputs. The new calculated values then become
    the new input values that feed the next layer. This process continues until it
    has gone through all the layers and determines the output. A threshold transfer
    function is sometimes used to quantify the output of a neuron in the output layer.
    Feed-forward networks include [Perceptron](artificial_neural_network_bkp.htm)
    (linear and non-linear) and [Radial Basis Function](artificial_neural_network_rbf.htm)
    networks. Feed-forward networks are often used in data mining. A **feed-back network**
    has feed-back paths meaning they can have signals traveling in both directions
    using loops. All possible connections between neurons are allowed. Since loops
    are present in this type of network, it becomes a non-linear dynamic system which
    changes continuously until it reaches a state of equilibrium. Feed-back networks
    are often used in associative memories and optimization problems where the network
    looks for the best arrangement of interconnected factors.
  prefs: []
  type: TYPE_NORMAL
